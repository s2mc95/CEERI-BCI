{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import scipy\n",
    "from scipy import io\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model #Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, BatchNormalization, Lambda\n",
    "from keras.layers import Conv1D,Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras.constraints import max_norm\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint,LearningRateScheduler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/somsukla/Documents/\n"
     ]
    }
   ],
   "source": [
    "# image_dir='E:\\\\SMC\\\\RSVP_Processed_Emotiv'\n",
    "# print(image_dir)\n",
    "# folders=['Original']\n",
    "\n",
    "\n",
    "image_dir='/home/somsukla/Documents/'\n",
    "print(image_dir)\n",
    "folders=['Original']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of files672\n",
      "1) nonp300sig1548.mat\n",
      "2) nonp300sig923.mat\n",
      "3) nonp300sig1401.mat\n",
      "4) nonp300sig1581.mat\n",
      "5) nonp300sig1400.mat\n",
      "6) nonp300sig1511.mat\n",
      "7) nonp300sig740.mat\n",
      "8) nonp300sig235.mat\n",
      "9) nonp300sig1395.mat\n",
      "10) nonp300sig413.mat\n",
      "11) nonp300sig1554.mat\n",
      "12) nonp300sig1441.mat\n",
      "13) nonp300sig1200.mat\n",
      "14) nonp300sig586.mat\n",
      "15) nonp300sig453.mat\n",
      "16) nonp300sig515.mat\n",
      "17) nonp300sig1182.mat\n",
      "18) nonp300sig399.mat\n",
      "19) nonp300sig478.mat\n",
      "20) nonp300sig75.mat\n",
      "21) nonp300sig1309.mat\n",
      "22) nonp300sig1023.mat\n",
      "23) nonp300sig229.mat\n",
      "24) nonp300sig187.mat\n",
      "25) nonp300sig28.mat\n",
      "26) nonp300sig67.mat\n",
      "27) nonp300sig1399.mat\n",
      "28) nonp300sig1237.mat\n",
      "29) nonp300sig743.mat\n",
      "30) nonp300sig779.mat\n",
      "31) nonp300sig1093.mat\n",
      "32) nonp300sig972.mat\n",
      "33) nonp300sig1358.mat\n",
      "34) nonp300sig1337.mat\n",
      "35) nonp300sig1095.mat\n",
      "36) nonp300sig1406.mat\n",
      "37) nonp300sig994.mat\n",
      "38) nonp300sig1037.mat\n",
      "39) nonp300sig1348.mat\n",
      "40) nonp300sig184.mat\n",
      "41) nonp300sig146.mat\n",
      "42) nonp300sig1336.mat\n",
      "43) nonp300sig878.mat\n",
      "44) nonp300sig1019.mat\n",
      "45) nonp300sig55.mat\n",
      "46) nonp300sig600.mat\n",
      "47) nonp300sig499.mat\n",
      "48) nonp300sig484.mat\n",
      "49) nonp300sig892.mat\n",
      "50) nonp300sig950.mat\n",
      "51) nonp300sig301.mat\n",
      "52) nonp300sig179.mat\n",
      "53) nonp300sig1101.mat\n",
      "54) nonp300sig355.mat\n",
      "55) nonp300sig735.mat\n",
      "56) nonp300sig1347.mat\n",
      "57) nonp300sig1013.mat\n",
      "58) nonp300sig1311.mat\n",
      "59) nonp300sig1383.mat\n",
      "60) nonp300sig1457.mat\n",
      "61) nonp300sig781.mat\n",
      "62) nonp300sig980.mat\n",
      "63) nonp300sig641.mat\n",
      "64) nonp300sig886.mat\n",
      "65) nonp300sig456.mat\n",
      "66) nonp300sig949.mat\n",
      "67) nonp300sig827.mat\n",
      "68) nonp300sig1119.mat\n",
      "69) nonp300sig1567.mat\n",
      "70) nonp300sig701.mat\n",
      "71) nonp300sig415.mat\n",
      "72) nonp300sig444.mat\n",
      "73) nonp300sig919.mat\n",
      "74) nonp300sig1284.mat\n",
      "75) nonp300sig726.mat\n",
      "76) nonp300sig1152.mat\n",
      "77) nonp300sig655.mat\n",
      "78) nonp300sig106.mat\n",
      "79) nonp300sig583.mat\n",
      "80) nonp300sig1090.mat\n",
      "81) nonp300sig94.mat\n",
      "82) nonp300sig252.mat\n",
      "83) nonp300sig328.mat\n",
      "84) nonp300sig593.mat\n",
      "85) nonp300sig279.mat\n",
      "86) nonp300sig191.mat\n",
      "87) nonp300sig1060.mat\n",
      "88) nonp300sig18.mat\n",
      "89) nonp300sig1478.mat\n",
      "90) nonp300sig474.mat\n",
      "91) nonp300sig1174.mat\n",
      "92) nonp300sig580.mat\n",
      "93) nonp300sig1587.mat\n",
      "94) nonp300sig1048.mat\n",
      "95) nonp300sig1187.mat\n",
      "96) nonp300sig338.mat\n",
      "97) nonp300sig1233.mat\n",
      "98) nonp300sig658.mat\n",
      "99) nonp300sig1025.mat\n",
      "100) nonp300sig832.mat\n",
      "101) nonp300sig245.mat\n",
      "102) nonp300sig1166.mat\n",
      "103) nonp300sig1403.mat\n",
      "104) nonp300sig1556.mat\n",
      "105) nonp300sig1261.mat\n",
      "106) nonp300sig855.mat\n",
      "107) nonp300sig913.mat\n",
      "108) nonp300sig34.mat\n",
      "109) nonp300sig286.mat\n",
      "110) nonp300sig1574.mat\n",
      "111) nonp300sig659.mat\n",
      "112) nonp300sig1530.mat\n",
      "113) nonp300sig31.mat\n",
      "114) nonp300sig482.mat\n",
      "115) nonp300sig1482.mat\n",
      "116) nonp300sig1410.mat\n",
      "117) nonp300sig108.mat\n",
      "118) nonp300sig1223.mat\n",
      "119) nonp300sig211.mat\n",
      "120) nonp300sig1194.mat\n",
      "121) nonp300sig898.mat\n",
      "122) nonp300sig1198.mat\n",
      "123) nonp300sig33.mat\n",
      "124) nonp300sig1227.mat\n",
      "125) nonp300sig808.mat\n",
      "126) nonp300sig1255.mat\n",
      "127) nonp300sig560.mat\n",
      "128) nonp300sig1424.mat\n",
      "129) nonp300sig185.mat\n",
      "130) nonp300sig260.mat\n",
      "131) nonp300sig111.mat\n",
      "132) nonp300sig714.mat\n",
      "133) nonp300sig1111.mat\n",
      "134) nonp300sig1066.mat\n",
      "135) nonp300sig247.mat\n",
      "136) nonp300sig1011.mat\n",
      "137) nonp300sig1558.mat\n",
      "138) nonp300sig998.mat\n",
      "139) nonp300sig1239.mat\n",
      "140) nonp300sig305.mat\n",
      "141) nonp300sig299.mat\n",
      "142) nonp300sig452.mat\n",
      "143) nonp300sig627.mat\n",
      "144) nonp300sig722.mat\n",
      "145) nonp300sig1148.mat\n",
      "146) nonp300sig89.mat\n",
      "147) nonp300sig125.mat\n",
      "148) nonp300sig561.mat\n",
      "149) nonp300sig769.mat\n",
      "150) nonp300sig1216.mat\n",
      "151) nonp300sig1352.mat\n",
      "152) nonp300sig1362.mat\n",
      "153) nonp300sig1197.mat\n",
      "154) nonp300sig1451.mat\n",
      "155) nonp300sig466.mat\n",
      "156) nonp300sig272.mat\n",
      "157) nonp300sig1369.mat\n",
      "158) nonp300sig158.mat\n",
      "159) nonp300sig1450.mat\n",
      "160) nonp300sig773.mat\n",
      "161) nonp300sig285.mat\n",
      "162) nonp300sig128.mat\n",
      "163) nonp300sig1494.mat\n",
      "164) nonp300sig790.mat\n",
      "165) nonp300sig709.mat\n",
      "166) nonp300sig214.mat\n",
      "167) nonp300sig241.mat\n",
      "168) nonp300sig11.mat\n",
      "169) nonp300sig205.mat\n",
      "170) nonp300sig476.mat\n",
      "171) nonp300sig366.mat\n",
      "172) nonp300sig325.mat\n",
      "173) nonp300sig775.mat\n",
      "174) nonp300sig309.mat\n",
      "175) nonp300sig645.mat\n",
      "176) nonp300sig681.mat\n",
      "177) nonp300sig1479.mat\n",
      "178) nonp300sig1430.mat\n",
      "179) nonp300sig711.mat\n",
      "180) nonp300sig151.mat\n",
      "181) nonp300sig1016.mat\n",
      "182) nonp300sig826.mat\n",
      "183) nonp300sig1538.mat\n",
      "184) nonp300sig467.mat\n",
      "185) nonp300sig1356.mat\n",
      "186) nonp300sig54.mat\n",
      "187) nonp300sig1502.mat\n",
      "188) nonp300sig339.mat\n",
      "189) nonp300sig730.mat\n",
      "190) nonp300sig733.mat\n",
      "191) nonp300sig500.mat\n",
      "192) nonp300sig363.mat\n",
      "193) nonp300sig720.mat\n",
      "194) nonp300sig1057.mat\n",
      "195) nonp300sig167.mat\n",
      "196) nonp300sig1142.mat\n",
      "197) nonp300sig802.mat\n",
      "198) nonp300sig1277.mat\n",
      "199) nonp300sig666.mat\n",
      "200) nonp300sig1428.mat\n",
      "201) nonp300sig1190.mat\n",
      "202) nonp300sig1118.mat\n",
      "203) nonp300sig1159.mat\n",
      "204) nonp300sig1203.mat\n",
      "205) nonp300sig723.mat\n",
      "206) nonp300sig1140.mat\n",
      "207) nonp300sig983.mat\n",
      "208) nonp300sig771.mat\n",
      "209) nonp300sig1321.mat\n",
      "210) nonp300sig661.mat\n",
      "211) nonp300sig713.mat\n",
      "212) nonp300sig183.mat\n",
      "213) nonp300sig1423.mat\n",
      "214) nonp300sig209.mat\n",
      "215) nonp300sig811.mat\n",
      "216) nonp300sig1179.mat\n",
      "217) nonp300sig483.mat\n",
      "218) nonp300sig1438.mat\n",
      "219) nonp300sig1236.mat\n",
      "220) nonp300sig316.mat\n",
      "221) nonp300sig1522.mat\n",
      "222) nonp300sig961.mat\n",
      "223) nonp300sig384.mat\n",
      "224) nonp300sig513.mat\n",
      "225) nonp300sig394.mat\n",
      "226) nonp300sig1062.mat\n",
      "227) nonp300sig57.mat\n",
      "228) nonp300sig307.mat\n",
      "229) nonp300sig1592.mat\n",
      "230) nonp300sig548.mat\n",
      "231) nonp300sig1206.mat\n",
      "232) nonp300sig835.mat\n",
      "233) nonp300sig851.mat\n",
      "234) nonp300sig489.mat\n",
      "235) nonp300sig978.mat\n",
      "236) nonp300sig1163.mat\n",
      "237) nonp300sig193.mat\n",
      "238) nonp300sig374.mat\n",
      "239) nonp300sig977.mat\n",
      "240) nonp300sig1431.mat\n",
      "241) nonp300sig397.mat\n",
      "242) nonp300sig16.mat\n",
      "243) nonp300sig885.mat\n",
      "244) nonp300sig896.mat\n",
      "245) nonp300sig935.mat\n",
      "246) nonp300sig1226.mat\n",
      "247) nonp300sig1534.mat\n",
      "248) nonp300sig1219.mat\n",
      "249) nonp300sig1531.mat\n",
      "250) nonp300sig1210.mat\n",
      "251) nonp300sig36.mat\n",
      "252) nonp300sig331.mat\n",
      "253) nonp300sig1263.mat\n",
      "254) nonp300sig817.mat\n",
      "255) nonp300sig803.mat\n",
      "256) nonp300sig1125.mat\n",
      "257) nonp300sig632.mat\n",
      "258) nonp300sig844.mat\n",
      "259) nonp300sig1229.mat\n",
      "260) nonp300sig344.mat\n",
      "261) nonp300sig794.mat\n",
      "262) nonp300sig1480.mat\n",
      "263) nonp300sig148.mat\n",
      "264) nonp300sig964.mat\n",
      "265) nonp300sig479.mat\n",
      "266) nonp300sig95.mat\n",
      "267) nonp300sig463.mat\n",
      "268) nonp300sig1220.mat\n",
      "269) nonp300sig1188.mat\n",
      "270) nonp300sig25.mat\n",
      "271) nonp300sig1017.mat\n",
      "272) nonp300sig804.mat\n",
      "273) nonp300sig72.mat\n",
      "274) nonp300sig1243.mat\n",
      "275) nonp300sig154.mat\n",
      "276) nonp300sig1519.mat\n",
      "277) nonp300sig1319.mat\n",
      "278) nonp300sig1469.mat\n",
      "279) nonp300sig74.mat\n",
      "280) nonp300sig929.mat\n",
      "281) nonp300sig461.mat\n",
      "282) nonp300sig1084.mat\n",
      "283) nonp300sig952.mat\n",
      "284) nonp300sig841.mat\n",
      "285) nonp300sig613.mat\n",
      "286) nonp300sig32.mat\n",
      "287) nonp300sig838.mat\n",
      "288) nonp300sig1517.mat\n",
      "289) nonp300sig1513.mat\n",
      "290) nonp300sig574.mat\n",
      "291) nonp300sig333.mat\n",
      "292) nonp300sig14.mat\n",
      "293) nonp300sig987.mat\n",
      "294) nonp300sig990.mat\n",
      "295) nonp300sig1247.mat\n",
      "296) nonp300sig1116.mat\n",
      "297) nonp300sig1176.mat\n",
      "298) nonp300sig1088.mat\n",
      "299) nonp300sig1366.mat\n",
      "300) nonp300sig420.mat\n",
      "301) nonp300sig1411.mat\n",
      "302) nonp300sig962.mat\n",
      "303) nonp300sig725.mat\n",
      "304) nonp300sig884.mat\n",
      "305) nonp300sig340.mat\n",
      "306) nonp300sig527.mat\n",
      "307) nonp300sig1346.mat\n",
      "308) nonp300sig1402.mat\n",
      "309) nonp300sig677.mat\n",
      "310) nonp300sig608.mat\n",
      "311) nonp300sig382.mat\n",
      "312) nonp300sig228.mat\n",
      "313) nonp300sig1318.mat\n",
      "314) nonp300sig1045.mat\n",
      "315) nonp300sig727.mat\n",
      "316) nonp300sig232.mat\n",
      "317) nonp300sig1397.mat\n",
      "318) nonp300sig1367.mat\n",
      "319) nonp300sig831.mat\n",
      "320) nonp300sig517.mat\n",
      "321) nonp300sig488.mat\n",
      "322) nonp300sig1242.mat\n",
      "323) nonp300sig1282.mat\n",
      "324) nonp300sig774.mat\n",
      "325) nonp300sig939.mat\n",
      "326) nonp300sig780.mat\n",
      "327) nonp300sig1153.mat\n",
      "328) nonp300sig1409.mat\n",
      "329) nonp300sig367.mat\n",
      "330) nonp300sig293.mat\n",
      "331) nonp300sig273.mat\n",
      "332) nonp300sig502.mat\n",
      "333) nonp300sig894.mat\n",
      "334) nonp300sig1132.mat\n",
      "335) nonp300sig493.mat\n",
      "336) nonp300sig1295.mat\n",
      "337) nonp300sig531.mat\n",
      "338) nonp300sig332.mat\n",
      "339) nonp300sig1096.mat\n",
      "340) nonp300sig1375.mat\n",
      "341) nonp300sig881.mat\n",
      "342) nonp300sig643.mat\n",
      "343) nonp300sig1523.mat\n",
      "344) nonp300sig858.mat\n",
      "345) nonp300sig1333.mat\n",
      "346) nonp300sig739.mat\n",
      "347) nonp300sig396.mat\n",
      "348) nonp300sig114.mat\n",
      "349) nonp300sig250.mat\n",
      "350) nonp300sig1404.mat\n",
      "351) nonp300sig1512.mat\n",
      "352) nonp300sig1170.mat\n",
      "353) nonp300sig1094.mat\n",
      "354) nonp300sig604.mat\n",
      "355) nonp300sig298.mat\n",
      "356) nonp300sig1364.mat\n",
      "357) nonp300sig1491.mat\n",
      "358) nonp300sig1256.mat\n",
      "359) nonp300sig900.mat\n",
      "360) nonp300sig986.mat\n",
      "361) nonp300sig441.mat\n",
      "362) nonp300sig1317.mat\n",
      "363) nonp300sig430.mat\n",
      "364) nonp300sig372.mat\n",
      "365) nonp300sig908.mat\n",
      "366) nonp300sig1041.mat\n",
      "367) nonp300sig1545.mat\n",
      "368) nonp300sig602.mat\n",
      "369) nonp300sig1505.mat\n",
      "370) nonp300sig66.mat\n",
      "371) nonp300sig758.mat\n",
      "372) nonp300sig766.mat\n",
      "373) nonp300sig750.mat\n",
      "374) nonp300sig1535.mat\n",
      "375) nonp300sig1465.mat\n",
      "376) nonp300sig9.mat\n",
      "377) nonp300sig918.mat\n",
      "378) nonp300sig670.mat\n",
      "379) nonp300sig257.mat\n",
      "380) nonp300sig423.mat\n",
      "381) nonp300sig149.mat\n",
      "382) nonp300sig569.mat\n",
      "383) nonp300sig955.mat\n",
      "384) nonp300sig1508.mat\n",
      "385) nonp300sig1565.mat\n",
      "386) nonp300sig861.mat\n",
      "387) nonp300sig44.mat\n",
      "388) nonp300sig636.mat\n",
      "389) nonp300sig879.mat\n",
      "390) nonp300sig685.mat\n",
      "391) nonp300sig660.mat\n",
      "392) nonp300sig1518.mat\n",
      "393) nonp300sig39.mat\n",
      "394) nonp300sig818.mat\n",
      "395) nonp300sig3.mat\n",
      "396) nonp300sig637.mat\n",
      "397) nonp300sig536.mat\n",
      "398) nonp300sig864.mat\n",
      "399) nonp300sig1186.mat\n",
      "400) nonp300sig1113.mat\n",
      "401) nonp300sig591.mat\n",
      "402) nonp300sig177.mat\n",
      "403) nonp300sig757.mat\n",
      "404) nonp300sig1105.mat\n",
      "405) nonp300sig1144.mat\n",
      "406) nonp300sig1310.mat\n",
      "407) nonp300sig1240.mat\n",
      "408) nonp300sig1199.mat\n",
      "409) nonp300sig922.mat\n",
      "410) nonp300sig971.mat\n",
      "411) nonp300sig616.mat\n",
      "412) nonp300sig1389.mat\n",
      "413) nonp300sig1069.mat\n",
      "414) nonp300sig599.mat\n",
      "415) nonp300sig1128.mat\n",
      "416) nonp300sig116.mat\n",
      "417) nonp300sig705.mat\n",
      "418) nonp300sig491.mat\n",
      "419) nonp300sig550.mat\n",
      "420) nonp300sig140.mat\n",
      "421) nonp300sig496.mat\n",
      "422) nonp300sig327.mat\n",
      "423) nonp300sig433.mat\n",
      "424) nonp300sig1377.mat\n",
      "425) nonp300sig543.mat\n",
      "426) nonp300sig815.mat\n",
      "427) nonp300sig1298.mat\n",
      "428) nonp300sig931.mat\n",
      "429) nonp300sig813.mat\n",
      "430) nonp300sig1071.mat\n",
      "431) nonp300sig419.mat\n",
      "432) nonp300sig225.mat\n",
      "433) nonp300sig570.mat\n",
      "434) nonp300sig957.mat\n",
      "435) nonp300sig303.mat\n",
      "436) nonp300sig796.mat\n",
      "437) nonp300sig1590.mat\n",
      "438) nonp300sig1291.mat\n",
      "439) nonp300sig1418.mat\n",
      "440) nonp300sig1268.mat\n",
      "441) nonp300sig1276.mat\n",
      "442) nonp300sig1435.mat\n",
      "443) nonp300sig122.mat\n",
      "444) nonp300sig1541.mat\n",
      "445) nonp300sig1254.mat\n",
      "446) nonp300sig189.mat\n",
      "447) nonp300sig1473.mat\n",
      "448) nonp300sig1097.mat\n",
      "449) nonp300sig201.mat\n",
      "450) nonp300sig728.mat\n",
      "451) nonp300sig353.mat\n",
      "452) nonp300sig853.mat\n",
      "453) nonp300sig954.mat\n",
      "454) nonp300sig791.mat\n",
      "455) nonp300sig975.mat\n",
      "456) nonp300sig1379.mat\n",
      "457) nonp300sig1124.mat\n",
      "458) nonp300sig742.mat\n",
      "459) nonp300sig1466.mat\n",
      "460) nonp300sig71.mat\n",
      "461) nonp300sig664.mat\n",
      "462) nonp300sig579.mat\n",
      "463) nonp300sig1380.mat\n",
      "464) nonp300sig675.mat\n",
      "465) nonp300sig475.mat\n",
      "466) nonp300sig893.mat\n",
      "467) nonp300sig887.mat\n",
      "468) nonp300sig638.mat\n",
      "469) nonp300sig64.mat\n",
      "470) nonp300sig119.mat\n",
      "471) nonp300sig1136.mat\n",
      "472) nonp300sig251.mat\n",
      "473) nonp300sig680.mat\n",
      "474) nonp300sig1211.mat\n",
      "475) nonp300sig1382.mat\n",
      "476) nonp300sig686.mat\n",
      "477) nonp300sig42.mat\n",
      "478) nonp300sig1010.mat\n",
      "479) nonp300sig904.mat\n",
      "480) nonp300sig1329.mat\n",
      "481) nonp300sig1167.mat\n",
      "482) nonp300sig131.mat\n",
      "483) nonp300sig1150.mat\n",
      "484) nonp300sig1123.mat\n",
      "485) nonp300sig1114.mat\n",
      "486) nonp300sig1217.mat\n",
      "487) nonp300sig1540.mat\n",
      "488) nonp300sig115.mat\n",
      "489) nonp300sig1110.mat\n",
      "490) nonp300sig1079.mat\n",
      "491) nonp300sig497.mat\n",
      "492) nonp300sig1304.mat\n",
      "493) nonp300sig833.mat\n",
      "494) nonp300sig1501.mat\n",
      "495) nonp300sig618.mat\n",
      "496) nonp300sig746.mat\n",
      "497) nonp300sig358.mat\n",
      "498) nonp300sig958.mat\n",
      "499) nonp300sig678.mat\n",
      "500) nonp300sig1269.mat\n",
      "501) nonp300sig501.mat\n",
      "502) nonp300sig1429.mat\n",
      "503) nonp300sig1091.mat\n",
      "504) nonp300sig457.mat\n",
      "505) nonp300sig1576.mat\n",
      "506) nonp300sig993.mat\n",
      "507) nonp300sig402.mat\n",
      "508) nonp300sig933.mat\n",
      "509) nonp300sig943.mat\n",
      "510) nonp300sig924.mat\n",
      "511) nonp300sig352.mat\n",
      "512) nonp300sig653.mat\n",
      "513) nonp300sig258.mat\n",
      "514) nonp300sig1503.mat\n",
      "515) nonp300sig207.mat\n",
      "516) nonp300sig795.mat\n",
      "517) nonp300sig284.mat\n",
      "518) nonp300sig432.mat\n",
      "519) nonp300sig1171.mat\n",
      "520) nonp300sig348.mat\n",
      "521) nonp300sig377.mat\n",
      "522) nonp300sig1177.mat\n",
      "523) nonp300sig1191.mat\n",
      "524) nonp300sig703.mat\n",
      "525) nonp300sig571.mat\n",
      "526) nonp300sig465.mat\n",
      "527) nonp300sig1135.mat\n",
      "528) nonp300sig782.mat\n",
      "529) nonp300sig157.mat\n",
      "530) nonp300sig1257.mat\n",
      "531) nonp300sig414.mat\n",
      "532) nonp300sig1555.mat\n",
      "533) nonp300sig807.mat\n",
      "534) nonp300sig10.mat\n",
      "535) nonp300sig282.mat\n",
      "536) nonp300sig1178.mat\n",
      "537) nonp300sig393.mat\n",
      "538) nonp300sig689.mat\n",
      "539) nonp300sig439.mat\n",
      "540) nonp300sig897.mat\n",
      "541) nonp300sig1008.mat\n",
      "542) nonp300sig22.mat\n",
      "543) nonp300sig113.mat\n",
      "544) nonp300sig1407.mat\n",
      "545) nonp300sig872.mat\n",
      "546) nonp300sig1082.mat\n",
      "547) nonp300sig530.mat\n",
      "548) nonp300sig206.mat\n",
      "549) nonp300sig1262.mat\n",
      "550) nonp300sig789.mat\n",
      "551) nonp300sig90.mat\n",
      "552) nonp300sig19.mat\n",
      "553) nonp300sig697.mat\n",
      "554) nonp300sig528.mat\n",
      "555) nonp300sig956.mat\n",
      "556) nonp300sig117.mat\n",
      "557) nonp300sig1235.mat\n",
      "558) nonp300sig208.mat\n",
      "559) nonp300sig880.mat\n",
      "560) nonp300sig821.mat\n",
      "561) nonp300sig505.mat\n",
      "562) nonp300sig1270.mat\n",
      "563) nonp300sig974.mat\n",
      "564) nonp300sig283.mat\n",
      "565) nonp300sig429.mat\n",
      "566) nonp300sig1330.mat\n",
      "567) nonp300sig1145.mat\n",
      "568) nonp300sig383.mat\n",
      "569) nonp300sig1320.mat\n",
      "570) nonp300sig969.mat\n",
      "571) nonp300sig946.mat\n",
      "572) nonp300sig459.mat\n",
      "573) nonp300sig1509.mat\n",
      "574) nonp300sig12.mat\n",
      "575) nonp300sig222.mat\n",
      "576) nonp300sig1422.mat\n",
      "577) nonp300sig449.mat\n",
      "578) nonp300sig400.mat\n",
      "579) nonp300sig253.mat\n",
      "580) nonp300sig8.mat\n",
      "581) nonp300sig1408.mat\n",
      "582) nonp300sig105.mat\n",
      "583) nonp300sig597.mat\n",
      "584) nonp300sig462.mat\n",
      "585) nonp300sig873.mat\n",
      "586) nonp300sig495.mat\n",
      "587) nonp300sig126.mat\n",
      "588) nonp300sig1520.mat\n",
      "589) nonp300sig854.mat\n",
      "590) nonp300sig910.mat\n",
      "591) nonp300sig1273.mat\n",
      "592) nonp300sig1328.mat\n",
      "593) nonp300sig848.mat\n",
      "594) nonp300sig427.mat\n",
      "595) nonp300sig1181.mat\n",
      "596) nonp300sig640.mat\n",
      "597) nonp300sig218.mat\n",
      "598) nonp300sig1553.mat\n",
      "599) nonp300sig1164.mat\n",
      "600) nonp300sig450.mat\n",
      "601) nonp300sig1313.mat\n",
      "602) nonp300sig97.mat\n",
      "603) nonp300sig20.mat\n",
      "604) nonp300sig704.mat\n",
      "605) nonp300sig1087.mat\n",
      "606) nonp300sig43.mat\n",
      "607) nonp300sig1493.mat\n",
      "608) nonp300sig46.mat\n",
      "609) nonp300sig938.mat\n",
      "610) nonp300sig1000.mat\n",
      "611) nonp300sig648.mat\n",
      "612) nonp300sig1533.mat\n",
      "613) nonp300sig1578.mat\n",
      "614) nonp300sig361.mat\n",
      "615) nonp300sig230.mat\n",
      "616) nonp300sig1201.mat\n",
      "617) nonp300sig1157.mat\n",
      "618) nonp300sig793.mat\n",
      "619) nonp300sig899.mat\n",
      "620) nonp300sig1209.mat\n",
      "621) nonp300sig1461.mat\n",
      "622) nonp300sig672.mat\n",
      "623) nonp300sig778.mat\n",
      "624) nonp300sig1063.mat\n",
      "625) nonp300sig524.mat\n",
      "626) nonp300sig51.mat\n",
      "627) nonp300sig104.mat\n",
      "628) nonp300sig812.mat\n",
      "629) nonp300sig752.mat\n",
      "630) nonp300sig312.mat\n",
      "631) nonp300sig1027.mat\n",
      "632) nonp300sig268.mat\n",
      "633) nonp300sig1416.mat\n",
      "634) nonp300sig587.mat\n",
      "635) nonp300sig1068.mat\n",
      "636) nonp300sig1161.mat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637) nonp300sig926.mat\n",
      "638) nonp300sig836.mat\n",
      "639) nonp300sig1146.mat\n",
      "640) nonp300sig92.mat\n",
      "641) nonp300sig173.mat\n",
      "642) nonp300sig1283.mat\n",
      "643) nonp300sig761.mat\n",
      "644) nonp300sig798.mat\n",
      "645) nonp300sig1098.mat\n",
      "646) nonp300sig748.mat\n",
      "647) nonp300sig1053.mat\n",
      "648) nonp300sig1149.mat\n",
      "649) nonp300sig625.mat\n",
      "650) nonp300sig1331.mat\n",
      "651) nonp300sig525.mat\n",
      "652) nonp300sig236.mat\n",
      "653) nonp300sig1573.mat\n",
      "654) nonp300sig1143.mat\n",
      "655) nonp300sig1417.mat\n",
      "656) nonp300sig973.mat\n",
      "657) nonp300sig615.mat\n",
      "658) nonp300sig871.mat\n",
      "659) nonp300sig1360.mat\n",
      "660) nonp300sig1384.mat\n",
      "661) nonp300sig264.mat\n",
      "662) nonp300sig371.mat\n",
      "663) nonp300sig362.mat\n",
      "664) nonp300sig330.mat\n",
      "665) nonp300sig825.mat\n",
      "666) nonp300sig1245.mat\n",
      "667) nonp300sig38.mat\n",
      "668) nonp300sig995.mat\n",
      "669) nonp300sig1489.mat\n",
      "670) nonp300sig673.mat\n",
      "671) nonp300sig710.mat\n",
      "672) nonp300sig1325.mat\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import glob\n",
    "\n",
    "#Prompting user to enter number of files to select randomly along with directory\n",
    "source='/home/somsukla/Documents/Original/nonP300'\n",
    "dest='/home/somsukla/Documents/Original/nonP300_random'\n",
    "no_of_files=int(input('no of files'))\n",
    "\n",
    "# print(\"%\"*25+\"{ Details Of Transfer }\"+\"%\"*25)\n",
    "# print(\"\\n\\nList of Files Moved to %s :-\"%(dest))\n",
    "\n",
    "#Using for loop to randomly choose multiple files\n",
    "for i in range(no_of_files):\n",
    "    #Variable random_file stores the name of the random file chosen\n",
    "    random_file=random.choice(os.listdir(source))\n",
    "    print('%d) %s'%(i+1,random_file))\n",
    "    source_file='%s/%s'%(source,random_file)\n",
    "    dest_file=dest\n",
    "    #\"shutil.move\" function moves file from one directory to another\n",
    "    shutil.move(source_file,dest_file)\n",
    "\n",
    "# files = glob.glob('/home/somsukla/Documents/Original/nonP300_random/*')\n",
    "# for f in files:\n",
    "#     os.remove(f)\n",
    "\n",
    "# no_of_files=168   #int(input('no of files'))\n",
    "\n",
    "# #Using for loop to randomly choose multiple files\n",
    "# for i in range(no_of_files):\n",
    "#     #Variable random_file stores the name of the random file chosen\n",
    "#     random_file=random.choice(os.listdir(source))\n",
    "#     print('%d) %s'%(i+1,random_file))\n",
    "#     source_file='%s/%s'%(source,random_file)\n",
    "#     dest_file=dest\n",
    "#     #\"shutil.move\" function moves file from one directory to another\n",
    "#     shutil.move(source_file,dest_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# import shutil\n",
    "\n",
    "# #Prompting user to enter number of files to select randomly along with directory\n",
    "# source=\"E:\\\\SMC\\\\RSVP_Processed_Emotiv\\\\Original\\\\nonP300\"\n",
    "# dest=\"E:\\\\SMC\\\\RSVP_Processed_Emotiv\\\\Original\\\\nonp300_random\"\n",
    "# no_of_files=int(input(\"no of files\"))\n",
    "\n",
    "# print(\"%\"*25+\"{ Details Of Transfer }\"+\"%\"*25)\n",
    "# print(\"\\n\\nList of Files Moved to %s :-\"%(dest))\n",
    "\n",
    "# #Using for loop to randomly choose multiple files\n",
    "# for i in range(no_of_files):\n",
    "#     #Variable random_file stores the name of the random file chosen\n",
    "#     random_file=random.choice(os.listdir(source))\n",
    "#     print('%d) %s'%(i+1,random_file))\n",
    "#     source_file='%s\\%s'%(source,random_file)\n",
    "#     dest_file=dest\n",
    "#     #\"shutil.move\" function moves file from one directory to another\n",
    "#     shutil.move(source_file,dest_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "(840, 90, 14)\n",
      "(840, 90, 14)\n"
     ]
    }
   ],
   "source": [
    "nclass=2\n",
    "X = []\n",
    "labels = []\n",
    "labellist = []\n",
    "folderfiles = []\n",
    "files = []\n",
    "cvscores = []\n",
    "counter=0\n",
    "counter1=0\n",
    "scenarios= ['nonP300_random', 'P300'] \n",
    "\n",
    "for subject in folders:\n",
    "        for folder in scenarios:\n",
    "            folderfiles = os.listdir(os.path.join(image_dir, subject, folder))\n",
    "            for matfile in folderfiles:\n",
    "                filename = os.path.join(image_dir, subject, folder, matfile)\n",
    "                cur_file=scipy.io.loadmat(filename) \n",
    "                for ky in [ 'erp_signal' ]:\n",
    "                    if ky in cur_file.keys():\n",
    "                        if ky =='erp_signal':#and counter <1500:\n",
    "                            x_cur=cur_file[ky]\n",
    "                            X.append(np.reshape(np.transpose(x_cur), (90,14)))\n",
    "                            label = filename.split(os.sep)[-2]\n",
    "                            labels.append(label)\n",
    "                            counter +=1\n",
    "                            print(counter)\n",
    "                \n",
    "#                         if ky =='mi_data': # and counter1 < 1500:\n",
    "#                             x_cur=cur_file[ky]\n",
    "#                             X.append(np.reshape(np.transpose(x_cur), (90,14)))\n",
    "#                             label = filename.split(os.sep)[-2]\n",
    "#                             labels.append(label)\n",
    "#                             counter1+=1;\n",
    "#                             print(counter1)\n",
    "                if label not in labellist:\n",
    "                    if len(labellist) >= nclass:\n",
    "                        continue\n",
    "                    labellist.append(label)\n",
    "\n",
    "\n",
    "        for num, label in enumerate(labellist):\n",
    "            for i in range(len(labels)):\n",
    "                if label == labels[i]:\n",
    "                    labels[i] = num\n",
    "\n",
    "X=np.array(X)\n",
    "labels=np.array(labels)\n",
    "print(X.shape)\n",
    "# print(labels)\n",
    "X[np.isnan(X)] = 0\n",
    "x_cur[np.isnan(x_cur)] = 0\n",
    "\n",
    "# X=X[:,:,:,None]\n",
    "print(X.shape)\n",
    "# print(x_cur)\n",
    "# print(X.shape , labels.shape)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 672 training samples and 168 testing samples.\n"
     ]
    }
   ],
   "source": [
    "# Splitting Testing and Training Sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            X ,labels, test_size=0.2, random_state=None) #*0.2,*0.8\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train, nclass)\n",
    "Y_test = np_utils.to_categorical(Y_test, nclass)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(Y_test.shape)\n",
    "print('There are',len(Y_train),'training samples and' ,len(Y_test),'testing samples.')#len(Y_val),'val samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_count = 672\n",
    "testing_data_count = 168\n",
    "n_steps = 90\n",
    "n_inputs = 14\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 128\n",
    "n_hidden3 = 128\n",
    "n_hidden4 = 64\n",
    "n_hidden5 = 64\n",
    "\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "learning_rate = 0.0030\n",
    "lambda_loss_amount = 0.0015\n",
    "training_iters = 30000\n",
    "batch_size = 8\n",
    "display_iter = 100\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X,_weights,_bias):\n",
    "    _X = tf.transpose(_X,[1,0,2])\n",
    "    print(_X.shape)\n",
    "    _X = tf.reshape(_X,[-1,n_inputs])\n",
    "    print(_X.shape)\n",
    "    _X = tf.nn.relu(tf.matmul(_X, weights['hidden']) + biases['hidden'])\n",
    "    _X = tf.split(_X, n_steps, 0) \n",
    "    # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden1, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden2, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_3 = tf.contrib.rnn.BasicLSTMCell(n_hidden3, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_4 = tf.contrib.rnn.BasicLSTMCell(n_hidden4, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_5 = tf.contrib.rnn.BasicLSTMCell(n_hidden5, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "\n",
    "\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2, lstm_cell_3,lstm_cell_4,lstm_cell_5], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # Get last time step's output feature for a \"many-to-one\" style classifier, \n",
    "    # as in the image describing RNNs at the top of this page\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _bias['out']\n",
    "\n",
    "\n",
    "def extract_batch_size(_train,step, batch_size):# Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n",
    "    shape = list(_train.shape)   \n",
    "    shape[0] = batch_size     \n",
    "    batch_s = np.empty(shape) \n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)* batch_size + i) % len(_train)\n",
    "        #print(index)\n",
    "        batch_s[i] = _train[index] \n",
    "        \n",
    "    return batch_s\n",
    "\n",
    "\n",
    "def one_hot(y_, n_classes=n_classes):\n",
    "    # Function to encode neural one-hot output labels from number indexes \n",
    "    # e.g.: \n",
    "    # one_hot(y_=[[5], [0], [3]], n_classes=6):\n",
    "    #     return [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    #y_ = y_.reshape(len(y_))\n",
    "    return np.eye(n_classes)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_inputs, n_hidden5])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden5, n_classes], mean=1.0))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden5])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1023 18:31:40.602274 140204688262912 deprecation.py:323] From <ipython-input-7-6fcd4248d7c7>:11: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W1023 18:31:40.603489 140204688262912 deprecation.py:323] From <ipython-input-7-6fcd4248d7c7>:19: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W1023 18:31:40.604109 140204688262912 deprecation.py:323] From <ipython-input-7-6fcd4248d7c7>:21: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, ?, 14)\n",
      "(?, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1023 18:31:40.810989 140204688262912 deprecation.py:506] From /home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1023 18:31:40.817816 140204688262912 deprecation.py:506] From /home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1023 18:32:02.446737 140204688262912 deprecation.py:506] From /home/somsukla/.virtualenvs/virtual-py3/lib/python3.5/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "\n",
    "l2 = lambda_loss_amount * sum(\n",
    "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    ") # L2 loss prevents this overkill neural network to overfit the data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "# optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost) \n",
    "tf.summary.scalar('loss',cost)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,decay=0.3).minimize(cost) \n",
    "\n",
    "# optimizer = tf.train.AdamOptimizer(0.0030).minimize(cost) # Adam Optimizer\n",
    "#,decay=0.5\n",
    "\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar('accuracy',accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #8:   Batch Loss = 2.952177, Accuracy = 0.875\n",
      "Training iter #16:   Batch Loss = 2.924235, Accuracy = 0.875\n",
      "Training iter #24:   Batch Loss = 3.001496, Accuracy = 0.875\n",
      "Training iter #32:   Batch Loss = 3.041599, Accuracy = 0.875\n",
      "Training iter #40:   Batch Loss = 5.420426, Accuracy = 0.375\n",
      "Training iter #48:   Batch Loss = 4.195803, Accuracy = 0.25\n",
      "Training iter #56:   Batch Loss = 2.483680, Accuracy = 1.0\n",
      "Training iter #64:   Batch Loss = 3.222488, Accuracy = 0.125\n",
      "Training iter #72:   Batch Loss = 2.150843, Accuracy = 1.0\n",
      "Training iter #80:   Batch Loss = 2.436414, Accuracy = 0.875\n",
      "Training iter #88:   Batch Loss = 2.227269, Accuracy = 0.875\n",
      "Training iter #96:   Batch Loss = 1.973006, Accuracy = 0.875\n",
      "Training iter #104:   Batch Loss = 1.443449, Accuracy = 1.0\n",
      "Training iter #112:   Batch Loss = 1.706602, Accuracy = 0.875\n",
      "Training iter #120:   Batch Loss = 2.089483, Accuracy = 0.25\n",
      "Training iter #128:   Batch Loss = 1.207505, Accuracy = 1.0\n",
      "Training iter #136:   Batch Loss = 1.147906, Accuracy = 1.0\n",
      "Training iter #144:   Batch Loss = 3.474135, Accuracy = 0.375\n",
      "Training iter #152:   Batch Loss = 1.848507, Accuracy = 0.75\n",
      "Training iter #160:   Batch Loss = 1.827610, Accuracy = 0.5\n",
      "Training iter #168:   Batch Loss = 1.550425, Accuracy = 0.875\n",
      "Training iter #176:   Batch Loss = 1.059150, Accuracy = 1.0\n",
      "Training iter #184:   Batch Loss = 1.392685, Accuracy = 0.875\n",
      "Training iter #192:   Batch Loss = 1.467006, Accuracy = 0.875\n",
      "Training iter #200:   Batch Loss = 1.639154, Accuracy = 0.75\n",
      "Training iter #208:   Batch Loss = 1.553935, Accuracy = 0.75\n",
      "Training iter #216:   Batch Loss = 1.554457, Accuracy = 0.75\n",
      "Training iter #224:   Batch Loss = 1.301975, Accuracy = 0.875\n",
      "Training iter #232:   Batch Loss = 1.578278, Accuracy = 0.625\n",
      "Training iter #240:   Batch Loss = 2.033345, Accuracy = 0.625\n",
      "Training iter #248:   Batch Loss = 1.584998, Accuracy = 0.25\n",
      "Training iter #256:   Batch Loss = 1.265455, Accuracy = 0.875\n",
      "Training iter #264:   Batch Loss = 1.493061, Accuracy = 0.75\n",
      "Training iter #272:   Batch Loss = 1.236748, Accuracy = 0.875\n",
      "Training iter #280:   Batch Loss = 0.855014, Accuracy = 1.0\n",
      "Training iter #288:   Batch Loss = 0.820836, Accuracy = 1.0\n",
      "Training iter #296:   Batch Loss = 0.794278, Accuracy = 1.0\n",
      "Training iter #304:   Batch Loss = 1.742084, Accuracy = 0.375\n",
      "Training iter #312:   Batch Loss = 0.782840, Accuracy = 1.0\n",
      "Training iter #320:   Batch Loss = 1.350401, Accuracy = 0.75\n",
      "Training iter #328:   Batch Loss = 1.454309, Accuracy = 0.625\n",
      "Training iter #336:   Batch Loss = 1.460228, Accuracy = 0.375\n",
      "Training iter #344:   Batch Loss = 1.546881, Accuracy = 0.25\n",
      "Training iter #352:   Batch Loss = 1.178273, Accuracy = 0.875\n",
      "Training iter #360:   Batch Loss = 1.340502, Accuracy = 0.75\n",
      "Training iter #368:   Batch Loss = 1.307387, Accuracy = 0.75\n",
      "Training iter #376:   Batch Loss = 0.843884, Accuracy = 1.0\n",
      "Training iter #384:   Batch Loss = 1.835407, Accuracy = 0.375\n",
      "Training iter #392:   Batch Loss = 1.109646, Accuracy = 0.875\n",
      "Training iter #400:   Batch Loss = 1.085267, Accuracy = 0.875\n",
      "Training iter #408:   Batch Loss = 1.075770, Accuracy = 0.875\n",
      "Training iter #416:   Batch Loss = 1.249789, Accuracy = 0.75\n",
      "Training iter #424:   Batch Loss = 0.684270, Accuracy = 1.0\n",
      "Training iter #432:   Batch Loss = 0.669512, Accuracy = 1.0\n",
      "Training iter #440:   Batch Loss = 1.374693, Accuracy = 0.375\n",
      "Training iter #448:   Batch Loss = 1.098420, Accuracy = 0.875\n",
      "Training iter #456:   Batch Loss = 1.519130, Accuracy = 0.625\n",
      "Training iter #464:   Batch Loss = 1.393702, Accuracy = 0.625\n",
      "Training iter #472:   Batch Loss = 1.206083, Accuracy = 0.75\n",
      "Training iter #480:   Batch Loss = 1.256514, Accuracy = 0.75\n",
      "Training iter #488:   Batch Loss = 0.632885, Accuracy = 1.0\n",
      "Training iter #496:   Batch Loss = 0.999550, Accuracy = 0.875\n",
      "Training iter #504:   Batch Loss = 4.912548, Accuracy = 0.25\n",
      "Training iter #512:   Batch Loss = 1.275052, Accuracy = 0.625\n",
      "Training iter #520:   Batch Loss = 1.173291, Accuracy = 0.75\n",
      "Training iter #528:   Batch Loss = 1.204806, Accuracy = 0.75\n",
      "Training iter #536:   Batch Loss = 0.976198, Accuracy = 0.875\n",
      "Training iter #544:   Batch Loss = 0.643434, Accuracy = 1.0\n",
      "Training iter #552:   Batch Loss = 1.226677, Accuracy = 0.75\n",
      "Training iter #560:   Batch Loss = 0.961817, Accuracy = 0.875\n",
      "Training iter #568:   Batch Loss = 1.947389, Accuracy = 0.375\n",
      "Training iter #576:   Batch Loss = 0.692817, Accuracy = 1.0\n",
      "Training iter #584:   Batch Loss = 1.297226, Accuracy = 0.625\n",
      "Training iter #592:   Batch Loss = 0.959242, Accuracy = 0.875\n",
      "Training iter #600:   Batch Loss = 0.961616, Accuracy = 0.875\n",
      "Training iter #608:   Batch Loss = 0.962703, Accuracy = 0.875\n",
      "Training iter #616:   Batch Loss = 0.957215, Accuracy = 0.875\n",
      "Training iter #624:   Batch Loss = 0.957339, Accuracy = 0.875\n",
      "Training iter #632:   Batch Loss = 0.545152, Accuracy = 1.0\n",
      "Training iter #640:   Batch Loss = 0.534729, Accuracy = 1.0\n",
      "Training iter #648:   Batch Loss = 1.528859, Accuracy = 0.625\n",
      "Training iter #656:   Batch Loss = 1.281523, Accuracy = 0.625\n",
      "Training iter #664:   Batch Loss = 1.298538, Accuracy = 0.5\n",
      "Training iter #672:   Batch Loss = 1.184166, Accuracy = 0.625\n",
      "Training iter #680:   Batch Loss = 0.935659, Accuracy = 0.875\n",
      "Training iter #688:   Batch Loss = 0.888335, Accuracy = 0.875\n",
      "Training iter #696:   Batch Loss = 0.900108, Accuracy = 0.875\n",
      "Training iter #704:   Batch Loss = 0.886184, Accuracy = 0.875\n",
      "Training iter #712:   Batch Loss = 3.102506, Accuracy = 0.375\n",
      "Training iter #720:   Batch Loss = 1.145845, Accuracy = 0.75\n",
      "Training iter #728:   Batch Loss = 0.605532, Accuracy = 1.0\n",
      "Training iter #736:   Batch Loss = 0.876599, Accuracy = 0.875\n",
      "Training iter #744:   Batch Loss = 0.490758, Accuracy = 1.0\n",
      "Training iter #752:   Batch Loss = 0.857694, Accuracy = 0.875\n",
      "Training iter #760:   Batch Loss = 0.850946, Accuracy = 0.875\n",
      "Training iter #768:   Batch Loss = 0.875569, Accuracy = 0.875\n",
      "Training iter #776:   Batch Loss = 0.515912, Accuracy = 1.0\n",
      "Training iter #784:   Batch Loss = 0.847053, Accuracy = 0.875\n",
      "Training iter #792:   Batch Loss = 1.087211, Accuracy = 0.75\n",
      "Training iter #800:   Batch Loss = 0.469998, Accuracy = 1.0\n",
      "Training iter #808:   Batch Loss = 0.448398, Accuracy = 1.0\n",
      "Training iter #816:   Batch Loss = 1.314121, Accuracy = 0.625\n",
      "Training iter #824:   Batch Loss = 1.027822, Accuracy = 0.75\n",
      "Training iter #832:   Batch Loss = 1.199910, Accuracy = 0.5\n",
      "Training iter #840:   Batch Loss = 0.819060, Accuracy = 0.875\n",
      "Training iter #848:   Batch Loss = 0.433102, Accuracy = 1.0\n",
      "Training iter #856:   Batch Loss = 0.814925, Accuracy = 0.875\n",
      "Training iter #864:   Batch Loss = 0.812928, Accuracy = 0.875\n",
      "Training iter #872:   Batch Loss = 1.178553, Accuracy = 0.25\n",
      "Training iter #880:   Batch Loss = 0.986931, Accuracy = 0.75\n",
      "Training iter #888:   Batch Loss = 0.997159, Accuracy = 0.75\n",
      "Training iter #896:   Batch Loss = 0.838291, Accuracy = 0.875\n",
      "Training iter #904:   Batch Loss = 1.067741, Accuracy = 0.625\n",
      "Training iter #912:   Batch Loss = 1.066213, Accuracy = 0.625\n",
      "Training iter #920:   Batch Loss = 0.971475, Accuracy = 0.75\n",
      "Training iter #928:   Batch Loss = 0.772416, Accuracy = 0.875\n",
      "Training iter #936:   Batch Loss = 0.953164, Accuracy = 0.75\n",
      "Training iter #944:   Batch Loss = 0.777248, Accuracy = 0.875\n",
      "Training iter #952:   Batch Loss = 0.440783, Accuracy = 1.0\n",
      "Training iter #960:   Batch Loss = 0.381797, Accuracy = 1.0\n",
      "Training iter #968:   Batch Loss = 0.376017, Accuracy = 1.0\n",
      "Training iter #976:   Batch Loss = 1.346212, Accuracy = 0.625\n",
      "Training iter #984:   Batch Loss = 0.479115, Accuracy = 1.0\n",
      "Training iter #992:   Batch Loss = 0.946513, Accuracy = 0.75\n",
      "Training iter #1000:   Batch Loss = 1.032389, Accuracy = 0.625\n",
      "Training iter #1008:   Batch Loss = 1.037671, Accuracy = 0.625\n",
      "Training iter #1016:   Batch Loss = 0.944752, Accuracy = 0.75\n",
      "Training iter #1024:   Batch Loss = 0.736474, Accuracy = 0.875\n",
      "Training iter #1032:   Batch Loss = 0.915760, Accuracy = 0.75\n",
      "Training iter #1040:   Batch Loss = 0.911842, Accuracy = 0.75\n",
      "Training iter #1048:   Batch Loss = 0.395945, Accuracy = 1.0\n",
      "Training iter #1056:   Batch Loss = 1.737548, Accuracy = 0.375\n",
      "Training iter #1064:   Batch Loss = 0.726637, Accuracy = 0.875\n",
      "Training iter #1072:   Batch Loss = 0.718915, Accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1080:   Batch Loss = 0.730379, Accuracy = 0.875\n",
      "Training iter #1088:   Batch Loss = 0.900442, Accuracy = 0.75\n",
      "Training iter #1096:   Batch Loss = 0.440416, Accuracy = 1.0\n",
      "Training iter #1104:   Batch Loss = 0.335623, Accuracy = 1.0\n",
      "Training iter #1112:   Batch Loss = 1.447754, Accuracy = 0.625\n",
      "Training iter #1120:   Batch Loss = 0.704822, Accuracy = 0.875\n",
      "Training iter #1128:   Batch Loss = 1.149901, Accuracy = 0.375\n",
      "Training iter #1136:   Batch Loss = 1.159699, Accuracy = 0.625\n",
      "Training iter #1144:   Batch Loss = 0.892416, Accuracy = 0.75\n",
      "Training iter #1152:   Batch Loss = 0.909596, Accuracy = 0.75\n",
      "Training iter #1160:   Batch Loss = 0.347678, Accuracy = 1.0\n",
      "Training iter #1168:   Batch Loss = 0.702221, Accuracy = 0.875\n",
      "Training iter #1176:   Batch Loss = 1.118847, Accuracy = 0.25\n",
      "Training iter #1184:   Batch Loss = 0.973716, Accuracy = 0.625\n",
      "Training iter #1192:   Batch Loss = 0.885386, Accuracy = 0.75\n",
      "Training iter #1200:   Batch Loss = 0.876853, Accuracy = 0.75\n",
      "Training iter #1208:   Batch Loss = 0.686925, Accuracy = 0.875\n",
      "Training iter #1216:   Batch Loss = 0.302775, Accuracy = 1.0\n",
      "Training iter #1224:   Batch Loss = 0.978746, Accuracy = 0.75\n",
      "Training iter #1232:   Batch Loss = 0.685852, Accuracy = 0.875\n",
      "Training iter #1240:   Batch Loss = 1.008088, Accuracy = 0.375\n",
      "Training iter #1248:   Batch Loss = 0.382208, Accuracy = 1.0\n",
      "Training iter #1256:   Batch Loss = 1.116118, Accuracy = 0.625\n",
      "Training iter #1264:   Batch Loss = 0.670709, Accuracy = 0.875\n",
      "Training iter #1272:   Batch Loss = 0.678143, Accuracy = 0.875\n",
      "Training iter #1280:   Batch Loss = 0.687009, Accuracy = 0.875\n",
      "Training iter #1288:   Batch Loss = 0.871634, Accuracy = 0.875\n",
      "Training iter #1296:   Batch Loss = 0.659169, Accuracy = 0.875\n",
      "Training iter #1304:   Batch Loss = 0.310549, Accuracy = 1.0\n",
      "Training iter #1312:   Batch Loss = 0.278091, Accuracy = 1.0\n",
      "Training iter #1320:   Batch Loss = 1.683633, Accuracy = 0.625\n",
      "Training iter #1328:   Batch Loss = 0.956935, Accuracy = 0.625\n",
      "Training iter #1336:   Batch Loss = 0.990773, Accuracy = 0.5\n",
      "Training iter #1344:   Batch Loss = 0.936198, Accuracy = 0.625\n",
      "Training iter #1352:   Batch Loss = 0.672978, Accuracy = 0.875\n",
      "Training iter #1360:   Batch Loss = 0.648582, Accuracy = 0.875\n",
      "Training iter #1368:   Batch Loss = 0.666099, Accuracy = 0.875\n",
      "Training iter #1376:   Batch Loss = 0.652241, Accuracy = 0.875\n",
      "Training iter #1384:   Batch Loss = 0.969503, Accuracy = 0.625\n",
      "Training iter #1392:   Batch Loss = 0.824550, Accuracy = 0.75\n",
      "Training iter #1400:   Batch Loss = 0.379489, Accuracy = 1.0\n",
      "Training iter #1408:   Batch Loss = 0.633612, Accuracy = 0.875\n",
      "Training iter #1416:   Batch Loss = 0.285601, Accuracy = 1.0\n",
      "Training iter #1424:   Batch Loss = 0.654540, Accuracy = 0.875\n",
      "Training iter #1432:   Batch Loss = 0.657158, Accuracy = 0.875\n",
      "Training iter #1440:   Batch Loss = 0.632593, Accuracy = 0.875\n",
      "Training iter #1448:   Batch Loss = 0.310869, Accuracy = 1.0\n",
      "Training iter #1456:   Batch Loss = 0.630804, Accuracy = 0.875\n",
      "Training iter #1464:   Batch Loss = 0.819048, Accuracy = 0.75\n",
      "Training iter #1472:   Batch Loss = 0.304725, Accuracy = 1.0\n",
      "Training iter #1480:   Batch Loss = 0.240662, Accuracy = 1.0\n",
      "Training iter #1488:   Batch Loss = 1.400241, Accuracy = 0.625\n",
      "Training iter #1496:   Batch Loss = 0.827645, Accuracy = 0.75\n",
      "Training iter #1504:   Batch Loss = 1.007493, Accuracy = 0.5\n",
      "Training iter #1512:   Batch Loss = 0.670722, Accuracy = 0.875\n",
      "Training iter #1520:   Batch Loss = 0.281833, Accuracy = 1.0\n",
      "Training iter #1528:   Batch Loss = 0.612202, Accuracy = 0.875\n",
      "Training iter #1536:   Batch Loss = 0.610322, Accuracy = 0.875\n",
      "Training iter #1544:   Batch Loss = 0.841480, Accuracy = 0.75\n",
      "Training iter #1552:   Batch Loss = 0.804374, Accuracy = 0.75\n",
      "Training iter #1560:   Batch Loss = 0.799687, Accuracy = 0.75\n",
      "Training iter #1568:   Batch Loss = 0.633902, Accuracy = 0.875\n",
      "Training iter #1576:   Batch Loss = 0.888128, Accuracy = 0.625\n",
      "Training iter #1584:   Batch Loss = 0.916060, Accuracy = 0.625\n",
      "Training iter #1592:   Batch Loss = 0.791973, Accuracy = 0.75\n",
      "Training iter #1600:   Batch Loss = 0.598230, Accuracy = 0.875\n",
      "Training iter #1608:   Batch Loss = 0.782435, Accuracy = 0.75\n",
      "Training iter #1616:   Batch Loss = 0.596729, Accuracy = 0.875\n",
      "Training iter #1624:   Batch Loss = 0.269675, Accuracy = 1.0\n",
      "Training iter #1632:   Batch Loss = 0.214043, Accuracy = 1.0\n",
      "Training iter #1640:   Batch Loss = 0.210455, Accuracy = 1.0\n",
      "Training iter #1648:   Batch Loss = 1.207246, Accuracy = 0.625\n",
      "Training iter #1656:   Batch Loss = 0.311341, Accuracy = 1.0\n",
      "Training iter #1664:   Batch Loss = 0.794480, Accuracy = 0.75\n",
      "Training iter #1672:   Batch Loss = 0.867592, Accuracy = 0.625\n",
      "Training iter #1680:   Batch Loss = 0.865507, Accuracy = 0.625\n",
      "Training iter #1688:   Batch Loss = 0.764239, Accuracy = 0.75\n",
      "Training iter #1696:   Batch Loss = 0.679680, Accuracy = 0.875\n",
      "Training iter #1704:   Batch Loss = 0.770566, Accuracy = 0.75\n",
      "Training iter #1712:   Batch Loss = 0.778789, Accuracy = 0.75\n",
      "Training iter #1720:   Batch Loss = 0.204177, Accuracy = 1.0\n",
      "Training iter #1728:   Batch Loss = 1.450628, Accuracy = 0.375\n",
      "Training iter #1736:   Batch Loss = 0.590571, Accuracy = 0.875\n",
      "Training iter #1744:   Batch Loss = 0.574747, Accuracy = 0.875\n",
      "Training iter #1752:   Batch Loss = 0.585965, Accuracy = 0.875\n",
      "Training iter #1760:   Batch Loss = 0.767378, Accuracy = 0.75\n",
      "Training iter #1768:   Batch Loss = 0.246248, Accuracy = 1.0\n",
      "Training iter #1776:   Batch Loss = 0.191185, Accuracy = 1.0\n",
      "Training iter #1784:   Batch Loss = 1.151939, Accuracy = 0.625\n",
      "Training iter #1792:   Batch Loss = 0.567153, Accuracy = 0.875\n",
      "Training iter #1800:   Batch Loss = 1.074000, Accuracy = 0.375\n",
      "Training iter #1808:   Batch Loss = 1.009384, Accuracy = 0.375\n",
      "Training iter #1816:   Batch Loss = 0.772029, Accuracy = 0.75\n",
      "Training iter #1824:   Batch Loss = 0.792066, Accuracy = 0.75\n",
      "Training iter #1832:   Batch Loss = 0.313673, Accuracy = 1.0\n",
      "Training iter #1840:   Batch Loss = 0.563802, Accuracy = 0.875\n",
      "Training iter #1848:   Batch Loss = 0.746130, Accuracy = 0.75\n",
      "Training iter #1856:   Batch Loss = 0.844343, Accuracy = 0.625\n",
      "Training iter #1864:   Batch Loss = 1.061821, Accuracy = 0.75\n",
      "Training iter #1872:   Batch Loss = 0.748881, Accuracy = 0.75\n",
      "Training iter #1880:   Batch Loss = 0.564754, Accuracy = 0.875\n",
      "Training iter #1888:   Batch Loss = 0.269086, Accuracy = 1.0\n",
      "Training iter #1896:   Batch Loss = 0.807283, Accuracy = 0.75\n",
      "Training iter #1904:   Batch Loss = 0.554859, Accuracy = 0.875\n",
      "Training iter #1912:   Batch Loss = 0.859558, Accuracy = 0.625\n",
      "Training iter #1920:   Batch Loss = 0.408463, Accuracy = 1.0\n",
      "Training iter #1928:   Batch Loss = 0.888993, Accuracy = 0.625\n",
      "Training iter #1936:   Batch Loss = 0.572406, Accuracy = 0.875\n",
      "Training iter #1944:   Batch Loss = 0.549649, Accuracy = 0.875\n",
      "Training iter #1952:   Batch Loss = 0.554575, Accuracy = 0.875\n",
      "Training iter #1960:   Batch Loss = 0.553367, Accuracy = 0.875\n",
      "Training iter #1968:   Batch Loss = 0.593373, Accuracy = 0.875\n",
      "Training iter #1976:   Batch Loss = 0.212104, Accuracy = 1.0\n",
      "Training iter #1984:   Batch Loss = 0.171165, Accuracy = 1.0\n",
      "Training iter #1992:   Batch Loss = 1.033271, Accuracy = 0.625\n",
      "Training iter #2000:   Batch Loss = 0.955661, Accuracy = 0.375\n",
      "Training iter #2008:   Batch Loss = 0.908476, Accuracy = 0.5\n",
      "Training iter #2016:   Batch Loss = 0.833579, Accuracy = 0.625\n",
      "Training iter #2024:   Batch Loss = 0.585831, Accuracy = 0.875\n",
      "Training iter #2032:   Batch Loss = 0.541888, Accuracy = 0.875\n",
      "Training iter #2040:   Batch Loss = 0.550035, Accuracy = 0.875\n",
      "Training iter #2048:   Batch Loss = 0.561869, Accuracy = 0.875\n",
      "Training iter #2056:   Batch Loss = 0.825190, Accuracy = 0.625\n",
      "Training iter #2064:   Batch Loss = 0.721624, Accuracy = 0.75\n",
      "Training iter #2072:   Batch Loss = 0.231238, Accuracy = 1.0\n",
      "Training iter #2080:   Batch Loss = 0.541287, Accuracy = 0.875\n",
      "Training iter #2088:   Batch Loss = 0.169398, Accuracy = 1.0\n",
      "Training iter #2096:   Batch Loss = 0.534823, Accuracy = 0.875\n",
      "Training iter #2104:   Batch Loss = 0.535485, Accuracy = 0.875\n",
      "Training iter #2112:   Batch Loss = 0.546090, Accuracy = 0.875\n",
      "Training iter #2120:   Batch Loss = 0.172782, Accuracy = 1.0\n",
      "Training iter #2128:   Batch Loss = 0.573126, Accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #2136:   Batch Loss = 1.214935, Accuracy = 0.25\n",
      "Training iter #2144:   Batch Loss = 0.206913, Accuracy = 1.0\n",
      "Training iter #2152:   Batch Loss = 0.155835, Accuracy = 1.0\n",
      "Training iter #2160:   Batch Loss = 1.325408, Accuracy = 0.625\n",
      "Training iter #2168:   Batch Loss = 0.782681, Accuracy = 0.75\n",
      "Training iter #2176:   Batch Loss = 0.954409, Accuracy = 0.5\n",
      "Training iter #2184:   Batch Loss = 0.589143, Accuracy = 0.875\n",
      "Training iter #2192:   Batch Loss = 0.230277, Accuracy = 1.0\n",
      "Training iter #2200:   Batch Loss = 0.528629, Accuracy = 0.875\n",
      "Training iter #2208:   Batch Loss = 0.525921, Accuracy = 0.875\n",
      "Training iter #2216:   Batch Loss = 0.774363, Accuracy = 0.75\n",
      "Training iter #2224:   Batch Loss = 0.720058, Accuracy = 0.75\n",
      "Training iter #2232:   Batch Loss = 0.714671, Accuracy = 0.75\n",
      "Training iter #2240:   Batch Loss = 0.582141, Accuracy = 0.875\n",
      "Training iter #2248:   Batch Loss = 0.806911, Accuracy = 0.625\n",
      "Training iter #2256:   Batch Loss = 0.836020, Accuracy = 0.625\n",
      "Training iter #2264:   Batch Loss = 0.705451, Accuracy = 0.75\n",
      "Training iter #2272:   Batch Loss = 0.530949, Accuracy = 0.875\n",
      "Training iter #2280:   Batch Loss = 0.710681, Accuracy = 0.75\n",
      "Training iter #2288:   Batch Loss = 0.547959, Accuracy = 0.875\n",
      "Training iter #2296:   Batch Loss = 0.185237, Accuracy = 1.0\n",
      "Training iter #2304:   Batch Loss = 0.139988, Accuracy = 1.0\n",
      "Training iter #2312:   Batch Loss = 0.137271, Accuracy = 1.0\n",
      "Training iter #2320:   Batch Loss = 0.980636, Accuracy = 0.625\n",
      "Training iter #2328:   Batch Loss = 0.274927, Accuracy = 1.0\n",
      "Training iter #2336:   Batch Loss = 0.721101, Accuracy = 0.75\n",
      "Training iter #2344:   Batch Loss = 0.796396, Accuracy = 0.625\n",
      "Training iter #2352:   Batch Loss = 0.808987, Accuracy = 0.625\n",
      "Training iter #2360:   Batch Loss = 0.698582, Accuracy = 0.75\n",
      "Training iter #2368:   Batch Loss = 0.519797, Accuracy = 0.875\n",
      "Training iter #2376:   Batch Loss = 0.729226, Accuracy = 0.75\n",
      "Training iter #2384:   Batch Loss = 0.703390, Accuracy = 0.75\n",
      "Training iter #2392:   Batch Loss = 0.219704, Accuracy = 1.0\n",
      "Training iter #2400:   Batch Loss = 1.559799, Accuracy = 0.375\n",
      "Training iter #2408:   Batch Loss = 0.513726, Accuracy = 0.875\n",
      "Training iter #2416:   Batch Loss = 0.515927, Accuracy = 0.875\n",
      "Training iter #2424:   Batch Loss = 0.506846, Accuracy = 0.875\n",
      "Training iter #2432:   Batch Loss = 0.695768, Accuracy = 0.75\n",
      "Training iter #2440:   Batch Loss = 0.270400, Accuracy = 1.0\n",
      "Training iter #2448:   Batch Loss = 0.140319, Accuracy = 1.0\n",
      "Training iter #2456:   Batch Loss = 1.314983, Accuracy = 0.625\n",
      "Training iter #2464:   Batch Loss = 0.505374, Accuracy = 0.875\n",
      "Training iter #2472:   Batch Loss = 0.878046, Accuracy = 0.625\n",
      "Training iter #2480:   Batch Loss = 0.931458, Accuracy = 0.375\n",
      "Training iter #2488:   Batch Loss = 0.713156, Accuracy = 0.75\n",
      "Training iter #2496:   Batch Loss = 0.689298, Accuracy = 0.75\n",
      "Training iter #2504:   Batch Loss = 0.275627, Accuracy = 1.0\n",
      "Training iter #2512:   Batch Loss = 0.512764, Accuracy = 0.875\n",
      "Training iter #2520:   Batch Loss = 0.690650, Accuracy = 0.75\n",
      "Training iter #2528:   Batch Loss = 0.791855, Accuracy = 0.625\n",
      "Training iter #2536:   Batch Loss = 0.684856, Accuracy = 0.75\n",
      "Training iter #2544:   Batch Loss = 0.690078, Accuracy = 0.75\n",
      "Training iter #2552:   Batch Loss = 0.534739, Accuracy = 0.875\n",
      "Training iter #2560:   Batch Loss = 0.148948, Accuracy = 1.0\n",
      "Training iter #2568:   Batch Loss = 0.790944, Accuracy = 0.75\n",
      "Training iter #2576:   Batch Loss = 0.498560, Accuracy = 0.875\n",
      "Training iter #2584:   Batch Loss = 0.795576, Accuracy = 0.625\n",
      "Training iter #2592:   Batch Loss = 0.379566, Accuracy = 1.0\n",
      "Training iter #2600:   Batch Loss = 0.832358, Accuracy = 0.625\n",
      "Training iter #2608:   Batch Loss = 0.522740, Accuracy = 0.875\n",
      "Training iter #2616:   Batch Loss = 0.495598, Accuracy = 0.875\n",
      "Training iter #2624:   Batch Loss = 0.516353, Accuracy = 0.875\n",
      "Training iter #2632:   Batch Loss = 0.520758, Accuracy = 0.875\n",
      "Training iter #2640:   Batch Loss = 0.496016, Accuracy = 0.875\n",
      "Training iter #2648:   Batch Loss = 0.184070, Accuracy = 1.0\n",
      "Training iter #2656:   Batch Loss = 0.116214, Accuracy = 1.0\n",
      "Training iter #2664:   Batch Loss = 1.073855, Accuracy = 0.625\n",
      "Training iter #2672:   Batch Loss = 1.263213, Accuracy = 0.375\n",
      "Training iter #2680:   Batch Loss = 0.871637, Accuracy = 0.5\n",
      "Training iter #2688:   Batch Loss = 0.798824, Accuracy = 0.625\n",
      "Training iter #2696:   Batch Loss = 0.525594, Accuracy = 0.875\n",
      "Training iter #2704:   Batch Loss = 0.490254, Accuracy = 0.875\n",
      "Training iter #2712:   Batch Loss = 0.508318, Accuracy = 0.875\n",
      "Training iter #2720:   Batch Loss = 0.495425, Accuracy = 0.875\n",
      "Training iter #2728:   Batch Loss = 0.773643, Accuracy = 0.625\n",
      "Training iter #2736:   Batch Loss = 0.676783, Accuracy = 0.75\n",
      "Training iter #2744:   Batch Loss = 0.131347, Accuracy = 1.0\n",
      "Training iter #2752:   Batch Loss = 0.511004, Accuracy = 0.875\n",
      "Training iter #2760:   Batch Loss = 0.116563, Accuracy = 1.0\n",
      "Training iter #2768:   Batch Loss = 0.500910, Accuracy = 0.875\n",
      "Training iter #2776:   Batch Loss = 0.489054, Accuracy = 0.875\n",
      "Training iter #2784:   Batch Loss = 0.512155, Accuracy = 0.875\n",
      "Training iter #2792:   Batch Loss = 0.117265, Accuracy = 1.0\n",
      "Training iter #2800:   Batch Loss = 0.497651, Accuracy = 0.875\n",
      "Training iter #2808:   Batch Loss = 1.270442, Accuracy = 0.25\n",
      "Training iter #2816:   Batch Loss = 0.176062, Accuracy = 1.0\n",
      "Training iter #2824:   Batch Loss = 0.110672, Accuracy = 1.0\n",
      "Training iter #2832:   Batch Loss = 1.204042, Accuracy = 0.625\n",
      "Training iter #2840:   Batch Loss = 0.690592, Accuracy = 0.75\n",
      "Training iter #2848:   Batch Loss = 2.477895, Accuracy = 0.5\n",
      "Training iter #2856:   Batch Loss = 0.525643, Accuracy = 0.875\n",
      "Training iter #2864:   Batch Loss = 0.115497, Accuracy = 1.0\n",
      "Training iter #2872:   Batch Loss = 0.493839, Accuracy = 0.875\n",
      "Training iter #2880:   Batch Loss = 0.484852, Accuracy = 0.875\n",
      "Training iter #2888:   Batch Loss = 0.669363, Accuracy = 0.75\n",
      "Training iter #2896:   Batch Loss = 0.667418, Accuracy = 0.75\n",
      "Training iter #2904:   Batch Loss = 0.683852, Accuracy = 0.75\n",
      "Training iter #2912:   Batch Loss = 0.629235, Accuracy = 0.875\n",
      "Training iter #2920:   Batch Loss = 0.765784, Accuracy = 0.625\n",
      "Training iter #2928:   Batch Loss = 0.815448, Accuracy = 0.375\n",
      "Training iter #2936:   Batch Loss = 0.665971, Accuracy = 0.75\n",
      "Training iter #2944:   Batch Loss = 0.482581, Accuracy = 0.875\n",
      "Training iter #2952:   Batch Loss = 0.664848, Accuracy = 0.75\n",
      "Training iter #2960:   Batch Loss = 0.479928, Accuracy = 0.875\n",
      "Training iter #2968:   Batch Loss = 0.132097, Accuracy = 1.0\n",
      "Training iter #2976:   Batch Loss = 0.101731, Accuracy = 1.0\n",
      "Training iter #2984:   Batch Loss = 0.099753, Accuracy = 1.0\n",
      "Training iter #2992:   Batch Loss = 1.415061, Accuracy = 0.625\n",
      "Training iter #3000:   Batch Loss = 0.158153, Accuracy = 1.0\n",
      "Training iter #3008:   Batch Loss = 0.697593, Accuracy = 0.75\n",
      "Training iter #3016:   Batch Loss = 0.767150, Accuracy = 0.625\n",
      "Training iter #3024:   Batch Loss = 0.800678, Accuracy = 0.625\n",
      "Training iter #3032:   Batch Loss = 0.660857, Accuracy = 0.75\n",
      "Training iter #3040:   Batch Loss = 0.482217, Accuracy = 0.875\n",
      "Training iter #3048:   Batch Loss = 0.667630, Accuracy = 0.75\n",
      "Training iter #3056:   Batch Loss = 0.677372, Accuracy = 0.75\n",
      "Training iter #3064:   Batch Loss = 0.180376, Accuracy = 1.0\n",
      "Training iter #3072:   Batch Loss = 1.555211, Accuracy = 0.375\n",
      "Training iter #3080:   Batch Loss = 0.478650, Accuracy = 0.875\n",
      "Training iter #3088:   Batch Loss = 0.477927, Accuracy = 0.875\n",
      "Training iter #3096:   Batch Loss = 0.487642, Accuracy = 0.875\n",
      "Training iter #3104:   Batch Loss = 0.662116, Accuracy = 0.75\n",
      "Training iter #3112:   Batch Loss = 0.227919, Accuracy = 1.0\n",
      "Training iter #3120:   Batch Loss = 0.097487, Accuracy = 1.0\n",
      "Training iter #3128:   Batch Loss = 0.933994, Accuracy = 0.625\n",
      "Training iter #3136:   Batch Loss = 0.473796, Accuracy = 0.875\n",
      "Training iter #3144:   Batch Loss = 1.051336, Accuracy = 0.375\n",
      "Training iter #3152:   Batch Loss = 0.771519, Accuracy = 0.625\n",
      "Training iter #3160:   Batch Loss = 0.753146, Accuracy = 0.75\n",
      "Training iter #3168:   Batch Loss = 0.661463, Accuracy = 0.75\n",
      "Training iter #3176:   Batch Loss = 0.229847, Accuracy = 1.0\n",
      "Training iter #3184:   Batch Loss = 0.501045, Accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #3192:   Batch Loss = 0.687619, Accuracy = 0.75\n",
      "Training iter #3200:   Batch Loss = 0.779112, Accuracy = 0.625\n",
      "Training iter #3208:   Batch Loss = 0.656611, Accuracy = 0.75\n",
      "Training iter #3216:   Batch Loss = 0.654553, Accuracy = 0.75\n",
      "Training iter #3224:   Batch Loss = 0.525718, Accuracy = 0.875\n",
      "Training iter #3232:   Batch Loss = 0.178700, Accuracy = 1.0\n",
      "Training iter #3240:   Batch Loss = 0.720353, Accuracy = 0.75\n",
      "Training iter #3248:   Batch Loss = 0.470108, Accuracy = 0.875\n",
      "Training iter #3256:   Batch Loss = 0.794419, Accuracy = 0.625\n",
      "Training iter #3264:   Batch Loss = 0.314065, Accuracy = 1.0\n",
      "Training iter #3272:   Batch Loss = 0.819092, Accuracy = 0.625\n",
      "Training iter #3280:   Batch Loss = 0.482545, Accuracy = 0.875\n",
      "Training iter #3288:   Batch Loss = 0.521763, Accuracy = 0.875\n",
      "Training iter #3296:   Batch Loss = 0.476747, Accuracy = 0.875\n",
      "Training iter #3304:   Batch Loss = 0.469347, Accuracy = 0.875\n",
      "Training iter #3312:   Batch Loss = 0.484990, Accuracy = 0.875\n",
      "Training iter #3320:   Batch Loss = 0.159735, Accuracy = 1.0\n",
      "Training iter #3328:   Batch Loss = 0.089799, Accuracy = 1.0\n",
      "Training iter #3336:   Batch Loss = 1.245053, Accuracy = 0.625\n",
      "Training iter #3344:   Batch Loss = 0.780480, Accuracy = 0.625\n",
      "Training iter #3352:   Batch Loss = 0.969874, Accuracy = 0.5\n",
      "Training iter #3360:   Batch Loss = 0.752400, Accuracy = 0.625\n",
      "Training iter #3368:   Batch Loss = 0.483098, Accuracy = 0.875\n",
      "Training iter #3376:   Batch Loss = 0.469172, Accuracy = 0.875\n",
      "Training iter #3384:   Batch Loss = 0.480243, Accuracy = 0.875\n",
      "Training iter #3392:   Batch Loss = 0.478125, Accuracy = 0.875\n",
      "Training iter #3400:   Batch Loss = 0.748702, Accuracy = 0.625\n",
      "Training iter #3408:   Batch Loss = 0.651066, Accuracy = 0.75\n",
      "Training iter #3416:   Batch Loss = 0.228498, Accuracy = 1.0\n",
      "Training iter #3424:   Batch Loss = 0.465322, Accuracy = 0.875\n",
      "Training iter #3432:   Batch Loss = 0.120882, Accuracy = 1.0\n",
      "Training iter #3440:   Batch Loss = 0.490251, Accuracy = 0.875\n",
      "Training iter #3448:   Batch Loss = 0.464657, Accuracy = 0.875\n",
      "Training iter #3456:   Batch Loss = 0.518000, Accuracy = 0.875\n",
      "Training iter #3464:   Batch Loss = 0.139789, Accuracy = 1.0\n",
      "Training iter #3472:   Batch Loss = 0.475101, Accuracy = 0.875\n",
      "Training iter #3480:   Batch Loss = 0.647422, Accuracy = 0.75\n",
      "Training iter #3488:   Batch Loss = 0.189671, Accuracy = 1.0\n",
      "Training iter #3496:   Batch Loss = 0.086454, Accuracy = 1.0\n",
      "Training iter #3504:   Batch Loss = 1.165272, Accuracy = 0.625\n",
      "Training iter #3512:   Batch Loss = 0.650419, Accuracy = 0.75\n",
      "Training iter #3520:   Batch Loss = 2.138007, Accuracy = 0.5\n",
      "Training iter #3528:   Batch Loss = 0.464092, Accuracy = 0.875\n",
      "Training iter #3536:   Batch Loss = 0.092497, Accuracy = 1.0\n",
      "Training iter #3544:   Batch Loss = 0.464702, Accuracy = 0.875\n",
      "Training iter #3552:   Batch Loss = 0.462336, Accuracy = 0.875\n",
      "Training iter #3560:   Batch Loss = 0.654893, Accuracy = 0.75\n",
      "Training iter #3568:   Batch Loss = 0.654228, Accuracy = 0.75\n",
      "Training iter #3576:   Batch Loss = 0.654465, Accuracy = 0.75\n",
      "Training iter #3584:   Batch Loss = 0.488945, Accuracy = 0.875\n",
      "Training iter #3592:   Batch Loss = 0.745387, Accuracy = 0.625\n",
      "Training iter #3600:   Batch Loss = 0.763712, Accuracy = 0.625\n",
      "Training iter #3608:   Batch Loss = 0.646194, Accuracy = 0.75\n",
      "Training iter #3616:   Batch Loss = 0.896768, Accuracy = 0.875\n",
      "Training iter #3624:   Batch Loss = 0.645711, Accuracy = 0.75\n",
      "Training iter #3632:   Batch Loss = 0.459621, Accuracy = 0.875\n",
      "Training iter #3640:   Batch Loss = 0.125902, Accuracy = 1.0\n",
      "Training iter #3648:   Batch Loss = 0.082144, Accuracy = 1.0\n",
      "Training iter #3656:   Batch Loss = 0.080489, Accuracy = 1.0\n",
      "Training iter #3664:   Batch Loss = 1.132069, Accuracy = 0.625\n",
      "Training iter #3672:   Batch Loss = 0.176244, Accuracy = 1.0\n",
      "Training iter #3680:   Batch Loss = 0.671113, Accuracy = 0.75\n",
      "Training iter #3688:   Batch Loss = 0.744023, Accuracy = 0.625\n",
      "Training iter #3696:   Batch Loss = 0.779048, Accuracy = 0.625\n",
      "Training iter #3704:   Batch Loss = 0.642030, Accuracy = 0.75\n",
      "Training iter #3712:   Batch Loss = 0.455890, Accuracy = 0.875\n",
      "Training iter #3720:   Batch Loss = 0.652440, Accuracy = 0.75\n",
      "Training iter #3728:   Batch Loss = 0.679382, Accuracy = 0.75\n",
      "Training iter #3736:   Batch Loss = 0.119717, Accuracy = 1.0\n",
      "Training iter #3744:   Batch Loss = 1.486419, Accuracy = 0.375\n",
      "Training iter #3752:   Batch Loss = 0.458843, Accuracy = 0.875\n",
      "Training iter #3760:   Batch Loss = 0.463383, Accuracy = 0.875\n",
      "Training iter #3768:   Batch Loss = 0.463335, Accuracy = 0.875\n",
      "Training iter #3776:   Batch Loss = 0.650563, Accuracy = 0.75\n",
      "Training iter #3784:   Batch Loss = 0.219419, Accuracy = 1.0\n",
      "Training iter #3792:   Batch Loss = 0.080815, Accuracy = 1.0\n",
      "Training iter #3800:   Batch Loss = 0.942047, Accuracy = 0.625\n",
      "Training iter #3808:   Batch Loss = 0.458477, Accuracy = 0.875\n",
      "Training iter #3816:   Batch Loss = 1.029547, Accuracy = 0.375\n",
      "Training iter #3824:   Batch Loss = 0.832315, Accuracy = 0.625\n",
      "Training iter #3832:   Batch Loss = 0.646222, Accuracy = 0.75\n",
      "Training iter #3840:   Batch Loss = 0.649751, Accuracy = 0.75\n",
      "Training iter #3848:   Batch Loss = 0.117715, Accuracy = 1.0\n",
      "Training iter #3856:   Batch Loss = 0.459126, Accuracy = 0.875\n",
      "Training iter #3864:   Batch Loss = 0.851330, Accuracy = 0.25\n",
      "Training iter #3872:   Batch Loss = 0.741722, Accuracy = 0.625\n",
      "Training iter #3880:   Batch Loss = 0.660995, Accuracy = 0.75\n",
      "Training iter #3888:   Batch Loss = 0.641720, Accuracy = 0.75\n",
      "Training iter #3896:   Batch Loss = 0.456139, Accuracy = 0.875\n",
      "Training iter #3904:   Batch Loss = 0.078089, Accuracy = 1.0\n",
      "Training iter #3912:   Batch Loss = 0.704678, Accuracy = 0.75\n",
      "Training iter #3920:   Batch Loss = 0.454364, Accuracy = 0.875\n",
      "Training iter #3928:   Batch Loss = 0.783606, Accuracy = 0.625\n",
      "Training iter #3936:   Batch Loss = 0.294871, Accuracy = 1.0\n",
      "Training iter #3944:   Batch Loss = 0.798412, Accuracy = 0.625\n",
      "Training iter #3952:   Batch Loss = 0.474324, Accuracy = 0.875\n",
      "Training iter #3960:   Batch Loss = 0.452290, Accuracy = 0.875\n",
      "Training iter #3968:   Batch Loss = 0.473288, Accuracy = 0.875\n",
      "Training iter #3976:   Batch Loss = 0.456895, Accuracy = 0.875\n",
      "Training iter #3984:   Batch Loss = 0.477852, Accuracy = 0.875\n",
      "Training iter #3992:   Batch Loss = 0.168398, Accuracy = 1.0\n",
      "Training iter #4000:   Batch Loss = 0.075268, Accuracy = 1.0\n",
      "Training iter #4008:   Batch Loss = 1.117938, Accuracy = 0.625\n",
      "Training iter #4016:   Batch Loss = 1.450466, Accuracy = 0.375\n",
      "Training iter #4024:   Batch Loss = 0.778901, Accuracy = 0.5\n",
      "Training iter #4032:   Batch Loss = 0.802405, Accuracy = 0.625\n",
      "Training iter #4040:   Batch Loss = 0.471986, Accuracy = 0.875\n",
      "Training iter #4048:   Batch Loss = 0.450468, Accuracy = 0.875\n",
      "Training iter #4056:   Batch Loss = 0.461140, Accuracy = 0.875\n",
      "Training iter #4064:   Batch Loss = 0.459708, Accuracy = 0.875\n",
      "Training iter #4072:   Batch Loss = 0.821130, Accuracy = 0.625\n",
      "Training iter #4080:   Batch Loss = 0.633871, Accuracy = 0.75\n",
      "Training iter #4088:   Batch Loss = 0.203081, Accuracy = 1.0\n",
      "Training iter #4096:   Batch Loss = 0.459785, Accuracy = 0.875\n",
      "Training iter #4104:   Batch Loss = 0.142778, Accuracy = 1.0\n",
      "Training iter #4112:   Batch Loss = 0.452158, Accuracy = 0.875\n",
      "Training iter #4120:   Batch Loss = 0.448226, Accuracy = 0.875\n",
      "Training iter #4128:   Batch Loss = 0.447174, Accuracy = 0.875\n",
      "Training iter #4136:   Batch Loss = 0.080667, Accuracy = 1.0\n",
      "Training iter #4144:   Batch Loss = 0.518056, Accuracy = 0.875\n",
      "Training iter #4152:   Batch Loss = 3.088411, Accuracy = 0.25\n",
      "Training iter #4160:   Batch Loss = 0.140489, Accuracy = 1.0\n",
      "Training iter #4168:   Batch Loss = 0.074442, Accuracy = 1.0\n",
      "Training iter #4176:   Batch Loss = 0.949924, Accuracy = 0.625\n",
      "Training iter #4184:   Batch Loss = 0.636736, Accuracy = 0.75\n",
      "Training iter #4192:   Batch Loss = 1.478276, Accuracy = 0.5\n",
      "Training iter #4200:   Batch Loss = 0.456680, Accuracy = 0.875\n",
      "Training iter #4208:   Batch Loss = 0.218098, Accuracy = 1.0\n",
      "Training iter #4216:   Batch Loss = 0.454373, Accuracy = 0.875\n",
      "Training iter #4224:   Batch Loss = 0.448618, Accuracy = 0.875\n",
      "Training iter #4232:   Batch Loss = 0.632498, Accuracy = 0.75\n",
      "Training iter #4240:   Batch Loss = 0.630479, Accuracy = 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #4248:   Batch Loss = 0.645047, Accuracy = 0.75\n",
      "Training iter #4256:   Batch Loss = 0.465570, Accuracy = 0.875\n",
      "Training iter #4264:   Batch Loss = 0.765873, Accuracy = 0.625\n",
      "Training iter #4272:   Batch Loss = 0.830711, Accuracy = 0.625\n",
      "Training iter #4280:   Batch Loss = 0.633544, Accuracy = 0.75\n",
      "Training iter #4288:   Batch Loss = 0.448568, Accuracy = 0.875\n",
      "Training iter #4296:   Batch Loss = 0.632002, Accuracy = 0.75\n",
      "Training iter #4304:   Batch Loss = 0.449086, Accuracy = 0.875\n",
      "Training iter #4312:   Batch Loss = 0.070689, Accuracy = 1.0\n",
      "Training iter #4320:   Batch Loss = 0.068554, Accuracy = 1.0\n",
      "Training iter #4328:   Batch Loss = 0.066652, Accuracy = 1.0\n",
      "Training iter #4336:   Batch Loss = 0.925738, Accuracy = 0.625\n",
      "Training iter #4344:   Batch Loss = 0.205091, Accuracy = 1.0\n",
      "Training iter #4352:   Batch Loss = 0.650748, Accuracy = 0.75\n",
      "Training iter #4360:   Batch Loss = 0.727396, Accuracy = 0.625\n",
      "Training iter #4368:   Batch Loss = 0.774171, Accuracy = 0.625\n",
      "Training iter #4376:   Batch Loss = 0.627685, Accuracy = 0.75\n",
      "Training iter #4384:   Batch Loss = 0.441080, Accuracy = 0.875\n",
      "Training iter #4392:   Batch Loss = 0.628581, Accuracy = 0.75\n",
      "Training iter #4400:   Batch Loss = 0.626018, Accuracy = 0.75\n",
      "Training iter #4408:   Batch Loss = 0.224090, Accuracy = 1.0\n",
      "Training iter #4416:   Batch Loss = 1.312772, Accuracy = 0.375\n",
      "Training iter #4424:   Batch Loss = 0.464899, Accuracy = 0.875\n",
      "Training iter #4432:   Batch Loss = 0.458152, Accuracy = 0.875\n",
      "Training iter #4440:   Batch Loss = 0.442319, Accuracy = 0.875\n",
      "Training iter #4448:   Batch Loss = 0.628278, Accuracy = 0.75\n",
      "Training iter #4456:   Batch Loss = 0.216634, Accuracy = 1.0\n",
      "Training iter #4464:   Batch Loss = 0.065320, Accuracy = 1.0\n",
      "Training iter #4472:   Batch Loss = 1.030170, Accuracy = 0.625\n",
      "Training iter #4480:   Batch Loss = 0.443788, Accuracy = 0.875\n",
      "Training iter #4488:   Batch Loss = 0.934062, Accuracy = 0.375\n",
      "Training iter #4496:   Batch Loss = 0.868989, Accuracy = 0.375\n",
      "Training iter #4504:   Batch Loss = 0.646947, Accuracy = 0.75\n",
      "Training iter #4512:   Batch Loss = 0.627724, Accuracy = 0.75\n",
      "Training iter #4520:   Batch Loss = 0.230958, Accuracy = 1.0\n",
      "Training iter #4528:   Batch Loss = 0.445269, Accuracy = 0.875\n",
      "Training iter #4536:   Batch Loss = 0.626224, Accuracy = 0.75\n",
      "Training iter #4544:   Batch Loss = 0.723977, Accuracy = 0.625\n",
      "Training iter #4552:   Batch Loss = 0.624333, Accuracy = 0.75\n",
      "Training iter #4560:   Batch Loss = 0.633943, Accuracy = 0.75\n",
      "Training iter #4568:   Batch Loss = 0.451126, Accuracy = 0.875\n",
      "Training iter #4576:   Batch Loss = 0.065157, Accuracy = 1.0\n",
      "Training iter #4584:   Batch Loss = 0.681943, Accuracy = 0.75\n",
      "Training iter #4592:   Batch Loss = 0.442510, Accuracy = 0.875\n",
      "Training iter #4600:   Batch Loss = 0.995845, Accuracy = 0.375\n",
      "Training iter #4608:   Batch Loss = 0.176138, Accuracy = 1.0\n",
      "Training iter #4616:   Batch Loss = 0.810369, Accuracy = 0.625\n",
      "Training iter #4624:   Batch Loss = 0.452842, Accuracy = 0.875\n",
      "Training iter #4632:   Batch Loss = 0.442412, Accuracy = 0.875\n",
      "Training iter #4640:   Batch Loss = 0.454100, Accuracy = 0.875\n",
      "Training iter #4648:   Batch Loss = 0.445761, Accuracy = 0.875\n",
      "Training iter #4656:   Batch Loss = 0.451555, Accuracy = 0.875\n",
      "Training iter #4664:   Batch Loss = 0.079734, Accuracy = 1.0\n",
      "Training iter #4672:   Batch Loss = 0.063053, Accuracy = 1.0\n",
      "Training iter #4680:   Batch Loss = 1.103966, Accuracy = 0.625\n",
      "Training iter #4688:   Batch Loss = 0.991663, Accuracy = 0.375\n",
      "Training iter #4696:   Batch Loss = 0.779825, Accuracy = 0.5\n",
      "Training iter #4704:   Batch Loss = 0.771685, Accuracy = 0.625\n",
      "Training iter #4712:   Batch Loss = 0.452329, Accuracy = 0.875\n",
      "Training iter #4720:   Batch Loss = 0.437950, Accuracy = 0.875\n",
      "Training iter #4728:   Batch Loss = 0.444426, Accuracy = 0.875\n",
      "Training iter #4736:   Batch Loss = 0.440043, Accuracy = 0.875\n",
      "Training iter #4744:   Batch Loss = 1.976459, Accuracy = 0.375\n",
      "Training iter #4752:   Batch Loss = 0.642989, Accuracy = 0.75\n",
      "Training iter #4760:   Batch Loss = 0.301623, Accuracy = 1.0\n",
      "Training iter #4768:   Batch Loss = 0.438637, Accuracy = 0.875\n",
      "Training iter #4776:   Batch Loss = 0.061996, Accuracy = 1.0\n",
      "Training iter #4784:   Batch Loss = 0.438320, Accuracy = 0.875\n",
      "Training iter #4792:   Batch Loss = 0.436860, Accuracy = 0.875\n",
      "Training iter #4800:   Batch Loss = 0.436434, Accuracy = 0.875\n",
      "Training iter #4808:   Batch Loss = 0.122453, Accuracy = 1.0\n",
      "Training iter #4816:   Batch Loss = 0.446274, Accuracy = 0.875\n",
      "Training iter #4824:   Batch Loss = 0.623320, Accuracy = 0.75\n",
      "Training iter #4832:   Batch Loss = 0.206429, Accuracy = 1.0\n",
      "Training iter #4840:   Batch Loss = 0.062230, Accuracy = 1.0\n",
      "Training iter #4848:   Batch Loss = 0.942330, Accuracy = 0.625\n",
      "Training iter #4856:   Batch Loss = 0.629170, Accuracy = 0.75\n",
      "Training iter #4864:   Batch Loss = 1.129537, Accuracy = 0.5\n",
      "Training iter #4872:   Batch Loss = 0.448222, Accuracy = 0.875\n",
      "Training iter #4880:   Batch Loss = 0.076238, Accuracy = 1.0\n",
      "Training iter #4888:   Batch Loss = 0.442452, Accuracy = 0.875\n",
      "Training iter #4896:   Batch Loss = 0.440294, Accuracy = 0.875\n",
      "Training iter #4904:   Batch Loss = 0.763446, Accuracy = 0.25\n",
      "Training iter #4912:   Batch Loss = 0.624760, Accuracy = 0.75\n",
      "Training iter #4920:   Batch Loss = 0.628116, Accuracy = 0.75\n",
      "Training iter #4928:   Batch Loss = 0.441006, Accuracy = 0.875\n",
      "Training iter #4936:   Batch Loss = 0.718754, Accuracy = 0.625\n",
      "Training iter #4944:   Batch Loss = 0.786452, Accuracy = 0.625\n",
      "Training iter #4952:   Batch Loss = 0.621148, Accuracy = 0.75\n",
      "Training iter #4960:   Batch Loss = 0.435498, Accuracy = 0.875\n",
      "Training iter #4968:   Batch Loss = 0.622469, Accuracy = 0.75\n",
      "Training iter #4976:   Batch Loss = 0.435434, Accuracy = 0.875\n",
      "Training iter #4984:   Batch Loss = 0.084823, Accuracy = 1.0\n",
      "Training iter #4992:   Batch Loss = 0.056463, Accuracy = 1.0\n",
      "Training iter #5000:   Batch Loss = 0.055529, Accuracy = 1.0\n",
      "Training iter #5008:   Batch Loss = 1.405753, Accuracy = 0.625\n",
      "Training iter #5016:   Batch Loss = 0.111834, Accuracy = 1.0\n",
      "Training iter #5024:   Batch Loss = 0.682434, Accuracy = 0.75\n",
      "Training iter #5032:   Batch Loss = 0.734508, Accuracy = 0.625\n",
      "Training iter #5040:   Batch Loss = 0.718337, Accuracy = 0.625\n",
      "Training iter #5048:   Batch Loss = 0.617428, Accuracy = 0.75\n",
      "Training iter #5056:   Batch Loss = 0.433191, Accuracy = 0.875\n",
      "Training iter #5064:   Batch Loss = 0.623199, Accuracy = 0.75\n",
      "Training iter #5072:   Batch Loss = 0.622068, Accuracy = 0.75\n",
      "Training iter #5080:   Batch Loss = 0.143862, Accuracy = 1.0\n",
      "Training iter #5088:   Batch Loss = 1.503331, Accuracy = 0.375\n",
      "Training iter #5096:   Batch Loss = 0.436708, Accuracy = 0.875\n",
      "Training iter #5104:   Batch Loss = 0.430015, Accuracy = 0.875\n",
      "Training iter #5112:   Batch Loss = 0.429326, Accuracy = 0.875\n",
      "Training iter #5120:   Batch Loss = 0.614784, Accuracy = 0.75\n",
      "Training iter #5128:   Batch Loss = 0.199177, Accuracy = 1.0\n",
      "Training iter #5136:   Batch Loss = 0.054761, Accuracy = 1.0\n",
      "Training iter #5144:   Batch Loss = 1.079132, Accuracy = 0.625\n",
      "Training iter #5152:   Batch Loss = 0.438978, Accuracy = 0.875\n",
      "Training iter #5160:   Batch Loss = 1.073346, Accuracy = 0.375\n",
      "Training iter #5168:   Batch Loss = 1.092243, Accuracy = 0.625\n",
      "Training iter #5176:   Batch Loss = 0.628596, Accuracy = 0.75\n",
      "Training iter #5184:   Batch Loss = 0.616887, Accuracy = 0.75\n",
      "Training iter #5192:   Batch Loss = 0.205722, Accuracy = 1.0\n",
      "Training iter #5200:   Batch Loss = 0.436988, Accuracy = 0.875\n",
      "Training iter #5208:   Batch Loss = 0.619624, Accuracy = 0.75\n",
      "Training iter #5216:   Batch Loss = 0.714539, Accuracy = 0.625\n",
      "Training iter #5224:   Batch Loss = 0.616865, Accuracy = 0.75\n",
      "Training iter #5232:   Batch Loss = 0.614931, Accuracy = 0.75\n",
      "Training iter #5240:   Batch Loss = 0.437031, Accuracy = 0.875\n",
      "Training iter #5248:   Batch Loss = 0.132143, Accuracy = 1.0\n",
      "Training iter #5256:   Batch Loss = 0.710777, Accuracy = 0.75\n",
      "Training iter #5264:   Batch Loss = 0.428261, Accuracy = 0.875\n",
      "Training iter #5272:   Batch Loss = 0.756141, Accuracy = 0.625\n",
      "Training iter #5280:   Batch Loss = 0.278341, Accuracy = 1.0\n",
      "Training iter #5288:   Batch Loss = 0.717834, Accuracy = 0.625\n",
      "Training iter #5296:   Batch Loss = 0.533025, Accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #5304:   Batch Loss = 0.443744, Accuracy = 0.875\n",
      "Training iter #5312:   Batch Loss = 0.430618, Accuracy = 0.875\n",
      "Training iter #5320:   Batch Loss = 0.456346, Accuracy = 0.875\n",
      "Training iter #5328:   Batch Loss = 0.432496, Accuracy = 0.875\n",
      "Training iter #5336:   Batch Loss = 0.115071, Accuracy = 1.0\n",
      "Training iter #5344:   Batch Loss = 0.052685, Accuracy = 1.0\n",
      "Training iter #5352:   Batch Loss = 1.028966, Accuracy = 0.625\n",
      "Training iter #5360:   Batch Loss = 0.809479, Accuracy = 0.625\n",
      "Training iter #5368:   Batch Loss = 0.893642, Accuracy = 0.5\n",
      "Training iter #5376:   Batch Loss = 0.716144, Accuracy = 0.625\n",
      "Training iter #5384:   Batch Loss = 0.448548, Accuracy = 0.875\n",
      "Training iter #5392:   Batch Loss = 0.429967, Accuracy = 0.875\n",
      "Training iter #5400:   Batch Loss = 0.442814, Accuracy = 0.875\n",
      "Training iter #5408:   Batch Loss = 0.436629, Accuracy = 0.875\n",
      "Training iter #5416:   Batch Loss = 0.795226, Accuracy = 0.625\n",
      "Training iter #5424:   Batch Loss = 0.610929, Accuracy = 0.75\n",
      "Training iter #5432:   Batch Loss = 0.186672, Accuracy = 1.0\n",
      "Training iter #5440:   Batch Loss = 0.425954, Accuracy = 0.875\n",
      "Training iter #5448:   Batch Loss = 0.053286, Accuracy = 1.0\n",
      "Training iter #5456:   Batch Loss = 0.428811, Accuracy = 0.875\n",
      "Training iter #5464:   Batch Loss = 0.426655, Accuracy = 0.875\n",
      "Training iter #5472:   Batch Loss = 0.427935, Accuracy = 0.875\n",
      "Training iter #5480:   Batch Loss = 0.129184, Accuracy = 1.0\n",
      "Training iter #5488:   Batch Loss = 0.432571, Accuracy = 0.875\n",
      "Training iter #5496:   Batch Loss = 0.611093, Accuracy = 0.75\n",
      "Training iter #5504:   Batch Loss = 0.146814, Accuracy = 1.0\n",
      "Training iter #5512:   Batch Loss = 0.050453, Accuracy = 1.0\n",
      "Training iter #5520:   Batch Loss = 0.925561, Accuracy = 0.625\n",
      "Training iter #5528:   Batch Loss = 0.623587, Accuracy = 0.75\n",
      "Training iter #5536:   Batch Loss = 0.754185, Accuracy = 0.5\n",
      "Training iter #5544:   Batch Loss = 0.619360, Accuracy = 0.875\n",
      "Training iter #5552:   Batch Loss = 0.096526, Accuracy = 1.0\n",
      "Training iter #5560:   Batch Loss = 0.429526, Accuracy = 0.875\n",
      "Training iter #5568:   Batch Loss = 0.429842, Accuracy = 0.875\n",
      "Training iter #5576:   Batch Loss = 0.626151, Accuracy = 0.75\n",
      "Training iter #5584:   Batch Loss = 0.621878, Accuracy = 0.75\n",
      "Training iter #5592:   Batch Loss = 0.619145, Accuracy = 0.75\n",
      "Training iter #5600:   Batch Loss = 0.435506, Accuracy = 0.875\n",
      "Training iter #5608:   Batch Loss = 0.714211, Accuracy = 0.625\n",
      "Training iter #5616:   Batch Loss = 0.708525, Accuracy = 0.625\n",
      "Training iter #5624:   Batch Loss = 0.610331, Accuracy = 0.75\n",
      "Training iter #5632:   Batch Loss = 0.429166, Accuracy = 0.875\n",
      "Training iter #5640:   Batch Loss = 0.616316, Accuracy = 0.75\n",
      "Training iter #5648:   Batch Loss = 0.429456, Accuracy = 0.875\n",
      "Training iter #5656:   Batch Loss = 0.121978, Accuracy = 1.0\n",
      "Training iter #5664:   Batch Loss = 0.047369, Accuracy = 1.0\n",
      "Training iter #5672:   Batch Loss = 0.046751, Accuracy = 1.0\n",
      "Training iter #5680:   Batch Loss = 1.363552, Accuracy = 0.625\n",
      "Training iter #5688:   Batch Loss = 0.113833, Accuracy = 1.0\n",
      "Training iter #5696:   Batch Loss = 0.656436, Accuracy = 0.75\n",
      "Training iter #5704:   Batch Loss = 0.717787, Accuracy = 0.625\n",
      "Training iter #5712:   Batch Loss = 0.724043, Accuracy = 0.625\n",
      "Training iter #5720:   Batch Loss = 0.609877, Accuracy = 0.75\n",
      "Training iter #5728:   Batch Loss = 0.427802, Accuracy = 0.875\n",
      "Training iter #5736:   Batch Loss = 0.613977, Accuracy = 0.75\n",
      "Training iter #5744:   Batch Loss = 0.614741, Accuracy = 0.75\n",
      "Training iter #5752:   Batch Loss = 0.134205, Accuracy = 1.0\n",
      "Training iter #5760:   Batch Loss = 1.458389, Accuracy = 0.375\n",
      "Training iter #5768:   Batch Loss = 0.429997, Accuracy = 0.875\n",
      "Training iter #5776:   Batch Loss = 0.421672, Accuracy = 0.875\n",
      "Training iter #5784:   Batch Loss = 0.421137, Accuracy = 0.875\n",
      "Training iter #5792:   Batch Loss = 0.606257, Accuracy = 0.75\n",
      "Training iter #5800:   Batch Loss = 0.167846, Accuracy = 1.0\n",
      "Training iter #5808:   Batch Loss = 0.045859, Accuracy = 1.0\n",
      "Training iter #5816:   Batch Loss = 1.157517, Accuracy = 0.625\n",
      "Training iter #5824:   Batch Loss = 0.422011, Accuracy = 0.875\n",
      "Training iter #5832:   Batch Loss = 0.740732, Accuracy = 0.375\n",
      "Training iter #5840:   Batch Loss = 0.789344, Accuracy = 0.625\n",
      "Training iter #5848:   Batch Loss = 0.660353, Accuracy = 0.75\n",
      "Training iter #5856:   Batch Loss = 0.612510, Accuracy = 0.75\n",
      "Training iter #5864:   Batch Loss = 0.171172, Accuracy = 1.0\n",
      "Training iter #5872:   Batch Loss = 0.435725, Accuracy = 0.875\n",
      "Training iter #5880:   Batch Loss = 0.610775, Accuracy = 0.75\n",
      "Training iter #5888:   Batch Loss = 0.708338, Accuracy = 0.625\n",
      "Training iter #5896:   Batch Loss = 0.607147, Accuracy = 0.75\n",
      "Training iter #5904:   Batch Loss = 0.606439, Accuracy = 0.75\n",
      "Training iter #5912:   Batch Loss = 0.437935, Accuracy = 0.875\n",
      "Training iter #5920:   Batch Loss = 0.047001, Accuracy = 1.0\n",
      "Training iter #5928:   Batch Loss = 0.842595, Accuracy = 0.75\n",
      "Training iter #5936:   Batch Loss = 0.424812, Accuracy = 0.875\n",
      "Training iter #5944:   Batch Loss = 0.743470, Accuracy = 0.625\n",
      "Training iter #5952:   Batch Loss = 0.271358, Accuracy = 1.0\n",
      "Training iter #5960:   Batch Loss = 0.761841, Accuracy = 0.625\n",
      "Training iter #5968:   Batch Loss = 0.442728, Accuracy = 0.875\n",
      "Training iter #5976:   Batch Loss = 0.419830, Accuracy = 0.875\n",
      "Training iter #5984:   Batch Loss = 0.444721, Accuracy = 0.875\n",
      "Training iter #5992:   Batch Loss = 0.422259, Accuracy = 0.875\n",
      "Training iter #6000:   Batch Loss = 0.428624, Accuracy = 0.875\n",
      "Training iter #6008:   Batch Loss = 0.118827, Accuracy = 1.0\n",
      "Training iter #6016:   Batch Loss = 0.043455, Accuracy = 1.0\n",
      "Training iter #6024:   Batch Loss = 1.244465, Accuracy = 0.625\n",
      "Training iter #6032:   Batch Loss = 0.840480, Accuracy = 0.625\n",
      "Training iter #6040:   Batch Loss = 0.798380, Accuracy = 0.5\n",
      "Training iter #6048:   Batch Loss = 0.715916, Accuracy = 0.625\n",
      "Training iter #6056:   Batch Loss = 0.465803, Accuracy = 0.875\n",
      "Training iter #6064:   Batch Loss = 0.423275, Accuracy = 0.875\n",
      "Training iter #6072:   Batch Loss = 0.430897, Accuracy = 0.875\n",
      "Training iter #6080:   Batch Loss = 0.424275, Accuracy = 0.875\n",
      "Training iter #6088:   Batch Loss = 0.710239, Accuracy = 0.625\n",
      "Training iter #6096:   Batch Loss = 0.604068, Accuracy = 0.75\n",
      "Training iter #6104:   Batch Loss = 0.162437, Accuracy = 1.0\n",
      "Training iter #6112:   Batch Loss = 0.417789, Accuracy = 0.875\n",
      "Training iter #6120:   Batch Loss = 0.077031, Accuracy = 1.0\n",
      "Training iter #6128:   Batch Loss = 0.440785, Accuracy = 0.875\n",
      "Training iter #6136:   Batch Loss = 0.418452, Accuracy = 0.875\n",
      "Training iter #6144:   Batch Loss = 0.421688, Accuracy = 0.875\n",
      "Training iter #6152:   Batch Loss = 0.044008, Accuracy = 1.0\n",
      "Training iter #6160:   Batch Loss = 0.460697, Accuracy = 0.875\n",
      "Training iter #6168:   Batch Loss = 0.603831, Accuracy = 0.75\n",
      "Training iter #6176:   Batch Loss = 0.148509, Accuracy = 1.0\n",
      "Training iter #6184:   Batch Loss = 0.042432, Accuracy = 1.0\n",
      "Training iter #6192:   Batch Loss = 0.972867, Accuracy = 0.625\n",
      "Training iter #6200:   Batch Loss = 0.654642, Accuracy = 0.75\n",
      "Training iter #6208:   Batch Loss = 0.763972, Accuracy = 0.5\n",
      "Training iter #6216:   Batch Loss = 0.460084, Accuracy = 0.875\n",
      "Training iter #6224:   Batch Loss = 0.048341, Accuracy = 1.0\n",
      "Training iter #6232:   Batch Loss = 0.426229, Accuracy = 0.875\n",
      "Training iter #6240:   Batch Loss = 0.460205, Accuracy = 0.875\n",
      "Training iter #6248:   Batch Loss = 0.623250, Accuracy = 0.75\n",
      "Training iter #6256:   Batch Loss = 0.618146, Accuracy = 0.75\n",
      "Training iter #6264:   Batch Loss = 0.610784, Accuracy = 0.75\n",
      "Training iter #6272:   Batch Loss = 0.451672, Accuracy = 0.875\n",
      "Training iter #6280:   Batch Loss = 0.701012, Accuracy = 0.625\n",
      "Training iter #6288:   Batch Loss = 0.707731, Accuracy = 0.625\n",
      "Training iter #6296:   Batch Loss = 0.604295, Accuracy = 0.75\n",
      "Training iter #6304:   Batch Loss = 0.472717, Accuracy = 0.875\n",
      "Training iter #6312:   Batch Loss = 0.603779, Accuracy = 0.75\n",
      "Training iter #6320:   Batch Loss = 0.439982, Accuracy = 0.875\n",
      "Training iter #6328:   Batch Loss = 0.111932, Accuracy = 1.0\n",
      "Training iter #6336:   Batch Loss = 0.040683, Accuracy = 1.0\n",
      "Training iter #6344:   Batch Loss = 0.039697, Accuracy = 1.0\n",
      "Training iter #6352:   Batch Loss = 1.106627, Accuracy = 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #6360:   Batch Loss = 0.145700, Accuracy = 1.0\n",
      "Training iter #6368:   Batch Loss = 0.615788, Accuracy = 0.75\n",
      "Training iter #6376:   Batch Loss = 0.700417, Accuracy = 0.625\n",
      "Training iter #6384:   Batch Loss = 0.712589, Accuracy = 0.625\n",
      "Training iter #6392:   Batch Loss = 0.618423, Accuracy = 0.75\n",
      "Training iter #6400:   Batch Loss = 0.422619, Accuracy = 0.875\n",
      "Training iter #6408:   Batch Loss = 0.600615, Accuracy = 0.75\n",
      "Training iter #6416:   Batch Loss = 0.599831, Accuracy = 0.75\n",
      "Training iter #6424:   Batch Loss = 0.111040, Accuracy = 1.0\n",
      "Training iter #6432:   Batch Loss = 1.511169, Accuracy = 0.375\n",
      "Training iter #6440:   Batch Loss = 0.428236, Accuracy = 0.875\n",
      "Training iter #6448:   Batch Loss = 0.421875, Accuracy = 0.875\n",
      "Training iter #6456:   Batch Loss = 0.417062, Accuracy = 0.875\n",
      "Training iter #6464:   Batch Loss = 0.605110, Accuracy = 0.75\n",
      "Training iter #6472:   Batch Loss = 0.114520, Accuracy = 1.0\n",
      "Training iter #6480:   Batch Loss = 0.038215, Accuracy = 1.0\n",
      "Training iter #6488:   Batch Loss = 0.974236, Accuracy = 0.625\n",
      "Training iter #6496:   Batch Loss = 0.416973, Accuracy = 0.875\n",
      "Training iter #6504:   Batch Loss = 0.913759, Accuracy = 0.375\n",
      "Training iter #6512:   Batch Loss = 1.513611, Accuracy = 0.625\n",
      "Training iter #6520:   Batch Loss = 0.640484, Accuracy = 0.75\n",
      "Training iter #6528:   Batch Loss = 0.603800, Accuracy = 0.75\n",
      "Training iter #6536:   Batch Loss = 0.187327, Accuracy = 1.0\n",
      "Training iter #6544:   Batch Loss = 0.420219, Accuracy = 0.875\n",
      "Training iter #6552:   Batch Loss = 0.606792, Accuracy = 0.75\n",
      "Training iter #6560:   Batch Loss = 0.700231, Accuracy = 0.625\n",
      "Training iter #6568:   Batch Loss = 0.599801, Accuracy = 0.75\n",
      "Training iter #6576:   Batch Loss = 0.609481, Accuracy = 0.75\n",
      "Training iter #6584:   Batch Loss = 0.424354, Accuracy = 0.875\n",
      "Training iter #6592:   Batch Loss = 0.038522, Accuracy = 1.0\n",
      "Training iter #6600:   Batch Loss = 0.770935, Accuracy = 0.75\n",
      "Training iter #6608:   Batch Loss = 0.416277, Accuracy = 0.875\n",
      "Training iter #6616:   Batch Loss = 0.714402, Accuracy = 0.625\n",
      "Training iter #6624:   Batch Loss = 0.259423, Accuracy = 1.0\n",
      "Training iter #6632:   Batch Loss = 0.757122, Accuracy = 0.625\n",
      "Training iter #6640:   Batch Loss = 0.434274, Accuracy = 0.875\n",
      "Training iter #6648:   Batch Loss = 0.412849, Accuracy = 0.875\n",
      "Training iter #6656:   Batch Loss = 0.465085, Accuracy = 0.875\n",
      "Training iter #6664:   Batch Loss = 0.412958, Accuracy = 0.875\n",
      "Training iter #6672:   Batch Loss = 0.428874, Accuracy = 0.875\n",
      "Training iter #6680:   Batch Loss = 0.113980, Accuracy = 1.0\n",
      "Training iter #6688:   Batch Loss = 0.036645, Accuracy = 1.0\n",
      "Training iter #6696:   Batch Loss = 1.079844, Accuracy = 0.625\n",
      "Training iter #6704:   Batch Loss = 0.766751, Accuracy = 0.625\n",
      "Training iter #6712:   Batch Loss = 0.756692, Accuracy = 0.5\n",
      "Training iter #6720:   Batch Loss = 0.696735, Accuracy = 0.625\n",
      "Training iter #6728:   Batch Loss = 0.438662, Accuracy = 0.875\n",
      "Training iter #6736:   Batch Loss = 0.451597, Accuracy = 0.875\n",
      "Training iter #6744:   Batch Loss = 0.423996, Accuracy = 0.875\n",
      "Training iter #6752:   Batch Loss = 0.437089, Accuracy = 0.875\n",
      "Training iter #6760:   Batch Loss = 0.727561, Accuracy = 0.625\n",
      "Training iter #6768:   Batch Loss = 0.597374, Accuracy = 0.75\n",
      "Training iter #6776:   Batch Loss = 0.137019, Accuracy = 1.0\n",
      "Training iter #6784:   Batch Loss = 0.412518, Accuracy = 0.875\n",
      "Training iter #6792:   Batch Loss = 0.083391, Accuracy = 1.0\n",
      "Training iter #6800:   Batch Loss = 0.415627, Accuracy = 0.875\n",
      "Training iter #6808:   Batch Loss = 0.411392, Accuracy = 0.875\n",
      "Training iter #6816:   Batch Loss = 0.412092, Accuracy = 0.875\n",
      "Training iter #6824:   Batch Loss = 0.036278, Accuracy = 1.0\n",
      "Training iter #6832:   Batch Loss = 0.456710, Accuracy = 0.875\n",
      "Training iter #6840:   Batch Loss = 0.614155, Accuracy = 0.75\n",
      "Training iter #6848:   Batch Loss = 0.146989, Accuracy = 1.0\n",
      "Training iter #6856:   Batch Loss = 0.035408, Accuracy = 1.0\n",
      "Training iter #6864:   Batch Loss = 1.099508, Accuracy = 0.625\n",
      "Training iter #6872:   Batch Loss = 0.597935, Accuracy = 0.75\n",
      "Training iter #6880:   Batch Loss = 0.734793, Accuracy = 0.5\n",
      "Training iter #6888:   Batch Loss = 0.416006, Accuracy = 0.875\n",
      "Training iter #6896:   Batch Loss = 0.036761, Accuracy = 1.0\n",
      "Training iter #6904:   Batch Loss = 0.416714, Accuracy = 0.875\n",
      "Training iter #6912:   Batch Loss = 0.411705, Accuracy = 0.875\n",
      "Training iter #6920:   Batch Loss = 0.602474, Accuracy = 0.75\n",
      "Training iter #6928:   Batch Loss = 0.598911, Accuracy = 0.75\n",
      "Training iter #6936:   Batch Loss = 0.616217, Accuracy = 0.75\n",
      "Training iter #6944:   Batch Loss = 0.444150, Accuracy = 0.875\n",
      "Training iter #6952:   Batch Loss = 0.695618, Accuracy = 0.625\n",
      "Training iter #6960:   Batch Loss = 0.695397, Accuracy = 0.625\n",
      "Training iter #6968:   Batch Loss = 0.596656, Accuracy = 0.75\n",
      "Training iter #6976:   Batch Loss = 0.465972, Accuracy = 0.875\n",
      "Training iter #6984:   Batch Loss = 0.597778, Accuracy = 0.75\n",
      "Training iter #6992:   Batch Loss = 0.436979, Accuracy = 0.875\n",
      "Training iter #7000:   Batch Loss = 0.101017, Accuracy = 1.0\n",
      "Training iter #7008:   Batch Loss = 0.034288, Accuracy = 1.0\n",
      "Training iter #7016:   Batch Loss = 0.033755, Accuracy = 1.0\n",
      "Training iter #7024:   Batch Loss = 1.068030, Accuracy = 0.625\n",
      "Training iter #7032:   Batch Loss = 0.141017, Accuracy = 1.0\n",
      "Training iter #7040:   Batch Loss = 0.607302, Accuracy = 0.75\n",
      "Training iter #7048:   Batch Loss = 0.694772, Accuracy = 0.625\n",
      "Training iter #7056:   Batch Loss = 0.698982, Accuracy = 0.625\n",
      "Training iter #7064:   Batch Loss = 0.597308, Accuracy = 0.75\n",
      "Training iter #7072:   Batch Loss = 0.509296, Accuracy = 0.875\n",
      "Training iter #7080:   Batch Loss = 0.596667, Accuracy = 0.75\n",
      "Training iter #7088:   Batch Loss = 0.610714, Accuracy = 0.75\n",
      "Training iter #7096:   Batch Loss = 0.041840, Accuracy = 1.0\n",
      "Training iter #7104:   Batch Loss = 1.666890, Accuracy = 0.375\n",
      "Training iter #7112:   Batch Loss = 0.428291, Accuracy = 0.875\n",
      "Training iter #7120:   Batch Loss = 0.409245, Accuracy = 0.875\n",
      "Training iter #7128:   Batch Loss = 0.408991, Accuracy = 0.875\n",
      "Training iter #7136:   Batch Loss = 0.603416, Accuracy = 0.75\n",
      "Training iter #7144:   Batch Loss = 0.076884, Accuracy = 1.0\n",
      "Training iter #7152:   Batch Loss = 0.033807, Accuracy = 1.0\n",
      "Training iter #7160:   Batch Loss = 0.975451, Accuracy = 0.625\n",
      "Training iter #7168:   Batch Loss = 0.418259, Accuracy = 0.875\n",
      "Training iter #7176:   Batch Loss = 0.889219, Accuracy = 0.375\n",
      "Training iter #7184:   Batch Loss = 0.733295, Accuracy = 0.375\n",
      "Training iter #7192:   Batch Loss = 0.675667, Accuracy = 0.75\n",
      "Training iter #7200:   Batch Loss = 0.604819, Accuracy = 0.75\n",
      "Training iter #7208:   Batch Loss = 0.203009, Accuracy = 1.0\n",
      "Training iter #7216:   Batch Loss = 0.420125, Accuracy = 0.875\n",
      "Training iter #7224:   Batch Loss = 0.594390, Accuracy = 0.75\n",
      "Training iter #7232:   Batch Loss = 0.699004, Accuracy = 0.625\n",
      "Training iter #7240:   Batch Loss = 0.609616, Accuracy = 0.75\n",
      "Training iter #7248:   Batch Loss = 0.598768, Accuracy = 0.75\n",
      "Training iter #7256:   Batch Loss = 0.412104, Accuracy = 0.875\n",
      "Training iter #7264:   Batch Loss = 0.032669, Accuracy = 1.0\n",
      "Training iter #7272:   Batch Loss = 0.782407, Accuracy = 0.75\n",
      "Training iter #7280:   Batch Loss = 0.410997, Accuracy = 0.875\n",
      "Training iter #7288:   Batch Loss = 0.707530, Accuracy = 0.625\n",
      "Training iter #7296:   Batch Loss = 0.247524, Accuracy = 1.0\n",
      "Training iter #7304:   Batch Loss = 0.751175, Accuracy = 0.625\n",
      "Training iter #7312:   Batch Loss = 0.428860, Accuracy = 0.875\n",
      "Training iter #7320:   Batch Loss = 0.408373, Accuracy = 0.875\n",
      "Training iter #7328:   Batch Loss = 0.449655, Accuracy = 0.875\n",
      "Training iter #7336:   Batch Loss = 0.410528, Accuracy = 0.875\n",
      "Training iter #7344:   Batch Loss = 0.435824, Accuracy = 0.875\n",
      "Training iter #7352:   Batch Loss = 0.101606, Accuracy = 1.0\n",
      "Training iter #7360:   Batch Loss = 0.032086, Accuracy = 1.0\n",
      "Training iter #7368:   Batch Loss = 1.043738, Accuracy = 0.625\n",
      "Training iter #7376:   Batch Loss = 0.777175, Accuracy = 0.625\n",
      "Training iter #7384:   Batch Loss = 0.754884, Accuracy = 0.5\n",
      "Training iter #7392:   Batch Loss = 0.692617, Accuracy = 0.625\n",
      "Training iter #7400:   Batch Loss = 0.427867, Accuracy = 0.875\n",
      "Training iter #7408:   Batch Loss = 0.452902, Accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #7416:   Batch Loss = 0.407706, Accuracy = 0.875\n",
      "Training iter #7424:   Batch Loss = 0.423191, Accuracy = 0.875\n",
      "Training iter #7432:   Batch Loss = 0.773484, Accuracy = 0.625\n",
      "Training iter #7440:   Batch Loss = 0.592955, Accuracy = 0.75\n",
      "Training iter #7448:   Batch Loss = 0.124093, Accuracy = 1.0\n",
      "Training iter #7456:   Batch Loss = 0.407913, Accuracy = 0.875\n",
      "Training iter #7464:   Batch Loss = 0.083910, Accuracy = 1.0\n",
      "Training iter #7472:   Batch Loss = 0.407683, Accuracy = 0.875\n",
      "Training iter #7480:   Batch Loss = 0.407257, Accuracy = 0.875\n",
      "Training iter #7488:   Batch Loss = 0.406845, Accuracy = 0.875\n",
      "Training iter #7496:   Batch Loss = 0.035056, Accuracy = 1.0\n",
      "Training iter #7504:   Batch Loss = 0.433736, Accuracy = 0.875\n",
      "Training iter #7512:   Batch Loss = 0.592535, Accuracy = 0.75\n",
      "Training iter #7520:   Batch Loss = 0.131277, Accuracy = 1.0\n",
      "Training iter #7528:   Batch Loss = 0.030761, Accuracy = 1.0\n",
      "Training iter #7536:   Batch Loss = 0.969903, Accuracy = 0.625\n",
      "Training iter #7544:   Batch Loss = 0.597643, Accuracy = 0.75\n",
      "Training iter #7552:   Batch Loss = 0.849149, Accuracy = 0.5\n",
      "Training iter #7560:   Batch Loss = 0.419899, Accuracy = 0.875\n",
      "Training iter #7568:   Batch Loss = 0.031694, Accuracy = 1.0\n",
      "Training iter #7576:   Batch Loss = 0.415057, Accuracy = 0.875\n",
      "Training iter #7584:   Batch Loss = 0.413684, Accuracy = 0.875\n",
      "Training iter #7592:   Batch Loss = 0.609659, Accuracy = 0.75\n",
      "Training iter #7600:   Batch Loss = 0.616706, Accuracy = 0.75\n",
      "Training iter #7608:   Batch Loss = 0.599660, Accuracy = 0.75\n",
      "Training iter #7616:   Batch Loss = 0.524402, Accuracy = 0.875\n",
      "Training iter #7624:   Batch Loss = 0.696549, Accuracy = 0.625\n",
      "Training iter #7632:   Batch Loss = 0.692776, Accuracy = 0.625\n",
      "Training iter #7640:   Batch Loss = 0.591885, Accuracy = 0.75\n",
      "Training iter #7648:   Batch Loss = 0.435404, Accuracy = 0.875\n",
      "Training iter #7656:   Batch Loss = 0.591714, Accuracy = 0.75\n",
      "Training iter #7664:   Batch Loss = 0.442464, Accuracy = 0.875\n",
      "Training iter #7672:   Batch Loss = 0.103015, Accuracy = 1.0\n",
      "Training iter #7680:   Batch Loss = 0.030360, Accuracy = 1.0\n",
      "Training iter #7688:   Batch Loss = 0.029855, Accuracy = 1.0\n",
      "Training iter #7696:   Batch Loss = 1.000886, Accuracy = 0.625\n",
      "Training iter #7704:   Batch Loss = 0.147499, Accuracy = 1.0\n",
      "Training iter #7712:   Batch Loss = 0.599525, Accuracy = 0.75\n",
      "Training iter #7720:   Batch Loss = 0.690458, Accuracy = 0.625\n",
      "Training iter #7728:   Batch Loss = 0.691660, Accuracy = 0.625\n",
      "Training iter #7736:   Batch Loss = 0.591078, Accuracy = 0.75\n",
      "Training iter #7744:   Batch Loss = 0.584330, Accuracy = 0.875\n",
      "Training iter #7752:   Batch Loss = 0.591885, Accuracy = 0.75\n",
      "Training iter #7760:   Batch Loss = 0.599834, Accuracy = 0.75\n",
      "Training iter #7768:   Batch Loss = 0.031209, Accuracy = 1.0\n",
      "Training iter #7776:   Batch Loss = 1.420105, Accuracy = 0.375\n",
      "Training iter #7784:   Batch Loss = 0.422658, Accuracy = 0.875\n",
      "Training iter #7792:   Batch Loss = 0.405568, Accuracy = 0.875\n",
      "Training iter #7800:   Batch Loss = 0.405275, Accuracy = 0.875\n",
      "Training iter #7808:   Batch Loss = 0.616112, Accuracy = 0.75\n",
      "Training iter #7816:   Batch Loss = 0.051733, Accuracy = 1.0\n",
      "Training iter #7824:   Batch Loss = 0.029881, Accuracy = 1.0\n",
      "Training iter #7832:   Batch Loss = 1.014891, Accuracy = 0.625\n",
      "Training iter #7840:   Batch Loss = 0.421921, Accuracy = 0.875\n",
      "Training iter #7848:   Batch Loss = 0.882073, Accuracy = 0.375\n",
      "Training iter #7856:   Batch Loss = 0.733587, Accuracy = 0.375\n",
      "Training iter #7864:   Batch Loss = 0.664458, Accuracy = 0.75\n",
      "Training iter #7872:   Batch Loss = 0.601416, Accuracy = 0.75\n",
      "Training iter #7880:   Batch Loss = 0.201858, Accuracy = 1.0\n",
      "Training iter #7888:   Batch Loss = 0.417369, Accuracy = 0.875\n",
      "Training iter #7896:   Batch Loss = 0.591127, Accuracy = 0.75\n",
      "Training iter #7904:   Batch Loss = 0.692994, Accuracy = 0.625\n",
      "Training iter #7912:   Batch Loss = 0.603664, Accuracy = 0.75\n",
      "Training iter #7920:   Batch Loss = 0.595448, Accuracy = 0.75\n",
      "Training iter #7928:   Batch Loss = 0.409225, Accuracy = 0.875\n",
      "Training iter #7936:   Batch Loss = 0.028844, Accuracy = 1.0\n",
      "Training iter #7944:   Batch Loss = 0.821927, Accuracy = 0.75\n",
      "Training iter #7952:   Batch Loss = 0.412195, Accuracy = 0.875\n",
      "Training iter #7960:   Batch Loss = 0.701816, Accuracy = 0.625\n",
      "Training iter #7968:   Batch Loss = 0.245005, Accuracy = 1.0\n",
      "Training iter #7976:   Batch Loss = 0.748136, Accuracy = 0.625\n",
      "Training iter #7984:   Batch Loss = 0.425642, Accuracy = 0.875\n",
      "Training iter #7992:   Batch Loss = 0.404649, Accuracy = 0.875\n",
      "Training iter #8000:   Batch Loss = 0.452968, Accuracy = 0.875\n",
      "Training iter #8008:   Batch Loss = 0.405072, Accuracy = 0.875\n",
      "Training iter #8016:   Batch Loss = 0.429775, Accuracy = 0.875\n",
      "Training iter #8024:   Batch Loss = 0.096418, Accuracy = 1.0\n",
      "Training iter #8032:   Batch Loss = 0.028180, Accuracy = 1.0\n",
      "Training iter #8040:   Batch Loss = 1.002807, Accuracy = 0.625\n",
      "Training iter #8048:   Batch Loss = 0.746036, Accuracy = 0.625\n",
      "Training iter #8056:   Batch Loss = 1.397647, Accuracy = 0.5\n",
      "Training iter #8064:   Batch Loss = 0.690279, Accuracy = 0.625\n",
      "Training iter #8072:   Batch Loss = 0.427259, Accuracy = 0.875\n",
      "Training iter #8080:   Batch Loss = 0.443065, Accuracy = 0.875\n",
      "Training iter #8088:   Batch Loss = 0.420065, Accuracy = 0.875\n",
      "Training iter #8096:   Batch Loss = 0.429507, Accuracy = 0.875\n",
      "Training iter #8104:   Batch Loss = 0.722743, Accuracy = 0.625\n",
      "Training iter #8112:   Batch Loss = 0.589838, Accuracy = 0.75\n",
      "Training iter #8120:   Batch Loss = 0.123370, Accuracy = 1.0\n",
      "Training iter #8128:   Batch Loss = 0.404600, Accuracy = 0.875\n",
      "Training iter #8136:   Batch Loss = 0.079555, Accuracy = 1.0\n",
      "Training iter #8144:   Batch Loss = 0.404043, Accuracy = 0.875\n",
      "Training iter #8152:   Batch Loss = 0.403366, Accuracy = 0.875\n",
      "Training iter #8160:   Batch Loss = 0.403155, Accuracy = 0.875\n",
      "Training iter #8168:   Batch Loss = 0.030097, Accuracy = 1.0\n",
      "Training iter #8176:   Batch Loss = 0.441012, Accuracy = 0.875\n",
      "Training iter #8184:   Batch Loss = 0.590001, Accuracy = 0.75\n",
      "Training iter #8192:   Batch Loss = 0.141846, Accuracy = 1.0\n",
      "Training iter #8200:   Batch Loss = 0.027137, Accuracy = 1.0\n",
      "Training iter #8208:   Batch Loss = 1.066441, Accuracy = 0.625\n",
      "Training iter #8216:   Batch Loss = 0.592386, Accuracy = 0.75\n",
      "Training iter #8224:   Batch Loss = 0.744580, Accuracy = 0.5\n",
      "Training iter #8232:   Batch Loss = 0.506497, Accuracy = 0.875\n",
      "Training iter #8240:   Batch Loss = 0.038321, Accuracy = 1.0\n",
      "Training iter #8248:   Batch Loss = 0.458935, Accuracy = 0.875\n",
      "Training iter #8256:   Batch Loss = 0.403372, Accuracy = 0.875\n",
      "Training iter #8264:   Batch Loss = 0.598348, Accuracy = 0.75\n",
      "Training iter #8272:   Batch Loss = 0.590313, Accuracy = 0.75\n",
      "Training iter #8280:   Batch Loss = 0.607062, Accuracy = 0.75\n",
      "Training iter #8288:   Batch Loss = 0.450492, Accuracy = 0.875\n",
      "Training iter #8296:   Batch Loss = 0.688009, Accuracy = 0.625\n",
      "Training iter #8304:   Batch Loss = 0.687224, Accuracy = 0.625\n",
      "Training iter #8312:   Batch Loss = 0.587956, Accuracy = 0.75\n",
      "Training iter #8320:   Batch Loss = 0.414097, Accuracy = 0.875\n",
      "Training iter #8328:   Batch Loss = 0.590394, Accuracy = 0.75\n",
      "Training iter #8336:   Batch Loss = 0.478652, Accuracy = 0.875\n",
      "Training iter #8344:   Batch Loss = 0.088494, Accuracy = 1.0\n",
      "Training iter #8352:   Batch Loss = 0.026619, Accuracy = 1.0\n",
      "Training iter #8360:   Batch Loss = 0.026194, Accuracy = 1.0\n",
      "Training iter #8368:   Batch Loss = 0.981407, Accuracy = 0.625\n",
      "Training iter #8376:   Batch Loss = 0.145794, Accuracy = 1.0\n",
      "Training iter #8384:   Batch Loss = 0.596699, Accuracy = 0.75\n",
      "Training iter #8392:   Batch Loss = 0.689111, Accuracy = 0.625\n",
      "Training iter #8400:   Batch Loss = 0.686493, Accuracy = 0.625\n",
      "Training iter #8408:   Batch Loss = 0.587067, Accuracy = 0.75\n",
      "Training iter #8416:   Batch Loss = 0.419233, Accuracy = 0.875\n",
      "Training iter #8424:   Batch Loss = 0.587279, Accuracy = 0.75\n",
      "Training iter #8432:   Batch Loss = 0.612540, Accuracy = 0.75\n",
      "Training iter #8440:   Batch Loss = 0.062903, Accuracy = 1.0\n",
      "Training iter #8448:   Batch Loss = 1.662726, Accuracy = 0.375\n",
      "Training iter #8456:   Batch Loss = 0.416982, Accuracy = 0.875\n",
      "Training iter #8464:   Batch Loss = 0.401863, Accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #8472:   Batch Loss = 0.401656, Accuracy = 0.875\n",
      "Training iter #8480:   Batch Loss = 0.603206, Accuracy = 0.75\n",
      "Training iter #8488:   Batch Loss = 0.078930, Accuracy = 1.0\n",
      "Training iter #8496:   Batch Loss = 0.025929, Accuracy = 1.0\n",
      "Training iter #8504:   Batch Loss = 0.961220, Accuracy = 0.625\n",
      "Training iter #8512:   Batch Loss = 0.408046, Accuracy = 0.875\n",
      "Training iter #8520:   Batch Loss = 0.893070, Accuracy = 0.375\n",
      "Training iter #8528:   Batch Loss = 0.706169, Accuracy = 0.625\n",
      "Training iter #8536:   Batch Loss = 0.767209, Accuracy = 0.25\n",
      "Training iter #8544:   Batch Loss = 0.600738, Accuracy = 0.75\n",
      "Training iter #8552:   Batch Loss = 0.028118, Accuracy = 1.0\n",
      "Training iter #8560:   Batch Loss = 0.410289, Accuracy = 0.875\n",
      "Training iter #8568:   Batch Loss = 0.591301, Accuracy = 0.75\n",
      "Training iter #8576:   Batch Loss = 0.688881, Accuracy = 0.625\n",
      "Training iter #8584:   Batch Loss = 0.592921, Accuracy = 0.75\n",
      "Training iter #8592:   Batch Loss = 0.597846, Accuracy = 0.75\n",
      "Training iter #8600:   Batch Loss = 0.410986, Accuracy = 0.875\n",
      "Training iter #8608:   Batch Loss = 0.026295, Accuracy = 1.0\n",
      "Training iter #8616:   Batch Loss = 0.810609, Accuracy = 0.75\n",
      "Training iter #8624:   Batch Loss = 0.409148, Accuracy = 0.875\n",
      "Training iter #8632:   Batch Loss = 0.701880, Accuracy = 0.625\n",
      "Training iter #8640:   Batch Loss = 0.247921, Accuracy = 1.0\n",
      "Training iter #8648:   Batch Loss = 0.741019, Accuracy = 0.625\n",
      "Training iter #8656:   Batch Loss = 0.423987, Accuracy = 0.875\n",
      "Training iter #8664:   Batch Loss = 0.401717, Accuracy = 0.875\n",
      "Training iter #8672:   Batch Loss = 0.459918, Accuracy = 0.875\n",
      "Training iter #8680:   Batch Loss = 0.401836, Accuracy = 0.875\n",
      "Training iter #8688:   Batch Loss = 0.422806, Accuracy = 0.875\n",
      "Training iter #8696:   Batch Loss = 0.097564, Accuracy = 1.0\n",
      "Training iter #8704:   Batch Loss = 0.025400, Accuracy = 1.0\n",
      "Training iter #8712:   Batch Loss = 0.975869, Accuracy = 0.625\n",
      "Training iter #8720:   Batch Loss = 0.749203, Accuracy = 0.625\n",
      "Training iter #8728:   Batch Loss = 0.741989, Accuracy = 0.5\n",
      "Training iter #8736:   Batch Loss = 0.688053, Accuracy = 0.625\n",
      "Training iter #8744:   Batch Loss = 0.435096, Accuracy = 0.875\n",
      "Training iter #8752:   Batch Loss = 0.434450, Accuracy = 0.875\n",
      "Training iter #8760:   Batch Loss = 0.427089, Accuracy = 0.875\n",
      "Training iter #8768:   Batch Loss = 0.426690, Accuracy = 0.875\n",
      "Training iter #8776:   Batch Loss = 0.732455, Accuracy = 0.625\n",
      "Training iter #8784:   Batch Loss = 0.586252, Accuracy = 0.75\n",
      "Training iter #8792:   Batch Loss = 0.126190, Accuracy = 1.0\n",
      "Training iter #8800:   Batch Loss = 0.401592, Accuracy = 0.875\n",
      "Training iter #8808:   Batch Loss = 0.078532, Accuracy = 1.0\n",
      "Training iter #8816:   Batch Loss = 0.401934, Accuracy = 0.875\n",
      "Training iter #8824:   Batch Loss = 0.400790, Accuracy = 0.875\n",
      "Training iter #8832:   Batch Loss = 0.400689, Accuracy = 0.875\n",
      "Training iter #8840:   Batch Loss = 0.026422, Accuracy = 1.0\n",
      "Training iter #8848:   Batch Loss = 0.436295, Accuracy = 0.875\n",
      "Training iter #8856:   Batch Loss = 0.586772, Accuracy = 0.75\n",
      "Training iter #8864:   Batch Loss = 0.143055, Accuracy = 1.0\n",
      "Training iter #8872:   Batch Loss = 0.024832, Accuracy = 1.0\n",
      "Training iter #8880:   Batch Loss = 1.102401, Accuracy = 0.625\n",
      "Training iter #8888:   Batch Loss = 0.588869, Accuracy = 0.75\n",
      "Training iter #8896:   Batch Loss = 0.751387, Accuracy = 0.5\n",
      "Training iter #8904:   Batch Loss = 0.499876, Accuracy = 0.875\n",
      "Training iter #8912:   Batch Loss = 0.040096, Accuracy = 1.0\n",
      "Training iter #8920:   Batch Loss = 0.494450, Accuracy = 0.875\n",
      "Training iter #8928:   Batch Loss = 0.401705, Accuracy = 0.875\n",
      "Training iter #8936:   Batch Loss = 0.588788, Accuracy = 0.75\n",
      "Training iter #8944:   Batch Loss = 0.585652, Accuracy = 0.75\n",
      "Training iter #8952:   Batch Loss = 0.589921, Accuracy = 0.75\n",
      "Training iter #8960:   Batch Loss = 0.519528, Accuracy = 0.875\n",
      "Training iter #8968:   Batch Loss = 0.691971, Accuracy = 0.625\n",
      "Training iter #8976:   Batch Loss = 0.684355, Accuracy = 0.625\n",
      "Training iter #8984:   Batch Loss = 0.591080, Accuracy = 0.75\n",
      "Training iter #8992:   Batch Loss = 0.407743, Accuracy = 0.875\n",
      "Training iter #9000:   Batch Loss = 0.585589, Accuracy = 0.75\n",
      "Training iter #9008:   Batch Loss = 0.401901, Accuracy = 0.875\n",
      "Training iter #9016:   Batch Loss = 0.025003, Accuracy = 1.0\n",
      "Training iter #9024:   Batch Loss = 0.024456, Accuracy = 1.0\n",
      "Training iter #9032:   Batch Loss = 0.023887, Accuracy = 1.0\n",
      "Training iter #9040:   Batch Loss = 0.943543, Accuracy = 0.625\n",
      "Training iter #9048:   Batch Loss = 0.159949, Accuracy = 1.0\n",
      "Training iter #9056:   Batch Loss = 0.589832, Accuracy = 0.75\n",
      "Training iter #9064:   Batch Loss = 0.687761, Accuracy = 0.625\n",
      "Training iter #9072:   Batch Loss = 0.683391, Accuracy = 0.625\n",
      "Training iter #9080:   Batch Loss = 0.588158, Accuracy = 0.75\n",
      "Training iter #9088:   Batch Loss = 0.405384, Accuracy = 0.875\n",
      "Training iter #9096:   Batch Loss = 0.585187, Accuracy = 0.75\n",
      "Training iter #9104:   Batch Loss = 0.584357, Accuracy = 0.75\n",
      "Training iter #9112:   Batch Loss = 0.074154, Accuracy = 1.0\n",
      "Training iter #9120:   Batch Loss = 1.553708, Accuracy = 0.375\n",
      "Training iter #9128:   Batch Loss = 0.434060, Accuracy = 0.875\n",
      "Training iter #9136:   Batch Loss = 0.399193, Accuracy = 0.875\n",
      "Training iter #9144:   Batch Loss = 0.399007, Accuracy = 0.875\n",
      "Training iter #9152:   Batch Loss = 0.593487, Accuracy = 0.75\n",
      "Training iter #9160:   Batch Loss = 0.091485, Accuracy = 1.0\n",
      "Training iter #9168:   Batch Loss = 0.023505, Accuracy = 1.0\n",
      "Training iter #9176:   Batch Loss = 0.921328, Accuracy = 0.625\n",
      "Training iter #9184:   Batch Loss = 0.409584, Accuracy = 0.875\n",
      "Training iter #9192:   Batch Loss = 0.888732, Accuracy = 0.375\n",
      "Training iter #9200:   Batch Loss = 0.734859, Accuracy = 0.375\n",
      "Training iter #9208:   Batch Loss = 0.654604, Accuracy = 0.75\n",
      "Training iter #9216:   Batch Loss = 0.592088, Accuracy = 0.75\n",
      "Training iter #9224:   Batch Loss = 0.190823, Accuracy = 1.0\n",
      "Training iter #9232:   Batch Loss = 0.411859, Accuracy = 0.875\n",
      "Training iter #9240:   Batch Loss = 0.584632, Accuracy = 0.75\n",
      "Training iter #9248:   Batch Loss = 0.685728, Accuracy = 0.625\n",
      "Training iter #9256:   Batch Loss = 0.593754, Accuracy = 0.75\n",
      "Training iter #9264:   Batch Loss = 0.590950, Accuracy = 0.75\n",
      "Training iter #9272:   Batch Loss = 0.404931, Accuracy = 0.875\n",
      "Training iter #9280:   Batch Loss = 0.023288, Accuracy = 1.0\n",
      "Training iter #9288:   Batch Loss = 0.844424, Accuracy = 0.75\n",
      "Training iter #9296:   Batch Loss = 0.409309, Accuracy = 0.875\n",
      "Training iter #9304:   Batch Loss = 0.695053, Accuracy = 0.625\n",
      "Training iter #9312:   Batch Loss = 0.239791, Accuracy = 1.0\n",
      "Training iter #9320:   Batch Loss = 0.738344, Accuracy = 0.625\n",
      "Training iter #9328:   Batch Loss = 0.421576, Accuracy = 0.875\n",
      "Training iter #9336:   Batch Loss = 0.399186, Accuracy = 0.875\n",
      "Training iter #9344:   Batch Loss = 0.459236, Accuracy = 0.875\n",
      "Training iter #9352:   Batch Loss = 0.399375, Accuracy = 0.875\n",
      "Training iter #9360:   Batch Loss = 0.427715, Accuracy = 0.875\n",
      "Training iter #9368:   Batch Loss = 0.092223, Accuracy = 1.0\n",
      "Training iter #9376:   Batch Loss = 0.022929, Accuracy = 1.0\n",
      "Training iter #9384:   Batch Loss = 0.978719, Accuracy = 0.625\n",
      "Training iter #9392:   Batch Loss = 0.727499, Accuracy = 0.625\n",
      "Training iter #9400:   Batch Loss = 0.731440, Accuracy = 0.5\n",
      "Training iter #9408:   Batch Loss = 0.684547, Accuracy = 0.625\n",
      "Training iter #9416:   Batch Loss = 0.426680, Accuracy = 0.875\n",
      "Training iter #9424:   Batch Loss = 0.441777, Accuracy = 0.875\n",
      "Training iter #9432:   Batch Loss = 0.411267, Accuracy = 0.875\n",
      "Training iter #9440:   Batch Loss = 0.424577, Accuracy = 0.875\n",
      "Training iter #9448:   Batch Loss = 0.708261, Accuracy = 0.625\n",
      "Training iter #9456:   Batch Loss = 0.584660, Accuracy = 0.75\n",
      "Training iter #9464:   Batch Loss = 0.105009, Accuracy = 1.0\n",
      "Training iter #9472:   Batch Loss = 0.400539, Accuracy = 0.875\n",
      "Training iter #9480:   Batch Loss = 0.066579, Accuracy = 1.0\n",
      "Training iter #9488:   Batch Loss = 0.398996, Accuracy = 0.875\n",
      "Training iter #9496:   Batch Loss = 0.398429, Accuracy = 0.875\n",
      "Training iter #9504:   Batch Loss = 0.398150, Accuracy = 0.875\n",
      "Training iter #9512:   Batch Loss = 0.024967, Accuracy = 1.0\n",
      "Training iter #9520:   Batch Loss = 0.418998, Accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #9528:   Batch Loss = 0.583587, Accuracy = 0.75\n",
      "Training iter #9536:   Batch Loss = 0.117707, Accuracy = 1.0\n",
      "Training iter #9544:   Batch Loss = 0.022217, Accuracy = 1.0\n",
      "Training iter #9552:   Batch Loss = 0.928828, Accuracy = 0.625\n",
      "Training iter #9560:   Batch Loss = 0.584578, Accuracy = 0.75\n",
      "Training iter #9568:   Batch Loss = 0.721020, Accuracy = 0.5\n",
      "Training iter #9576:   Batch Loss = 0.656795, Accuracy = 0.875\n",
      "Training iter #9584:   Batch Loss = 0.173554, Accuracy = 1.0\n",
      "Training iter #9592:   Batch Loss = 0.410476, Accuracy = 0.875\n",
      "Training iter #9600:   Batch Loss = 0.400016, Accuracy = 0.875\n",
      "Training iter #9608:   Batch Loss = 0.589219, Accuracy = 0.75\n",
      "Training iter #9616:   Batch Loss = 0.584629, Accuracy = 0.75\n",
      "Training iter #9624:   Batch Loss = 0.608953, Accuracy = 0.75\n",
      "Training iter #9632:   Batch Loss = 0.461608, Accuracy = 0.875\n",
      "Training iter #9640:   Batch Loss = 0.683730, Accuracy = 0.625\n",
      "Training iter #9648:   Batch Loss = 0.684234, Accuracy = 0.625\n",
      "Training iter #9656:   Batch Loss = 0.583975, Accuracy = 0.75\n",
      "Training iter #9664:   Batch Loss = 0.506382, Accuracy = 0.875\n",
      "Training iter #9672:   Batch Loss = 0.583933, Accuracy = 0.75\n",
      "Training iter #9680:   Batch Loss = 0.414064, Accuracy = 0.875\n",
      "Training iter #9688:   Batch Loss = 0.084954, Accuracy = 1.0\n",
      "Training iter #9696:   Batch Loss = 0.022177, Accuracy = 1.0\n",
      "Training iter #9704:   Batch Loss = 0.021875, Accuracy = 1.0\n",
      "Training iter #9712:   Batch Loss = 1.003209, Accuracy = 0.625\n",
      "Training iter #9720:   Batch Loss = 0.163083, Accuracy = 1.0\n",
      "Training iter #9728:   Batch Loss = 0.588937, Accuracy = 0.75\n",
      "Training iter #9736:   Batch Loss = 0.686492, Accuracy = 0.625\n",
      "Training iter #9744:   Batch Loss = 0.681859, Accuracy = 0.625\n",
      "Training iter #9752:   Batch Loss = 0.586539, Accuracy = 0.75\n",
      "Training iter #9760:   Batch Loss = 0.401883, Accuracy = 0.875\n",
      "Training iter #9768:   Batch Loss = 0.582913, Accuracy = 0.75\n",
      "Training iter #9776:   Batch Loss = 0.582671, Accuracy = 0.75\n",
      "Training iter #9784:   Batch Loss = 0.069173, Accuracy = 1.0\n",
      "Training iter #9792:   Batch Loss = 1.599362, Accuracy = 0.375\n",
      "Training iter #9800:   Batch Loss = 0.436880, Accuracy = 0.875\n",
      "Training iter #9808:   Batch Loss = 0.397651, Accuracy = 0.875\n",
      "Training iter #9816:   Batch Loss = 0.398270, Accuracy = 0.875\n",
      "Training iter #9824:   Batch Loss = 0.582980, Accuracy = 0.75\n",
      "Training iter #9832:   Batch Loss = 0.124917, Accuracy = 1.0\n",
      "Training iter #9840:   Batch Loss = 0.022243, Accuracy = 1.0\n",
      "Training iter #9848:   Batch Loss = 0.874139, Accuracy = 0.625\n",
      "Training iter #9856:   Batch Loss = 0.406992, Accuracy = 0.875\n",
      "Training iter #9864:   Batch Loss = 0.891033, Accuracy = 0.375\n",
      "Training iter #9872:   Batch Loss = 0.737057, Accuracy = 0.375\n",
      "Training iter #9880:   Batch Loss = 0.650139, Accuracy = 0.75\n",
      "Training iter #9888:   Batch Loss = 0.589436, Accuracy = 0.75\n",
      "Training iter #9896:   Batch Loss = 0.193693, Accuracy = 1.0\n",
      "Training iter #9904:   Batch Loss = 0.411713, Accuracy = 0.875\n",
      "Training iter #9912:   Batch Loss = 0.582960, Accuracy = 0.75\n",
      "Training iter #9920:   Batch Loss = 0.682826, Accuracy = 0.625\n",
      "Training iter #9928:   Batch Loss = 0.587190, Accuracy = 0.75\n",
      "Training iter #9936:   Batch Loss = 0.595636, Accuracy = 0.75\n",
      "Training iter #9944:   Batch Loss = 0.406884, Accuracy = 0.875\n",
      "Training iter #9952:   Batch Loss = 0.021540, Accuracy = 1.0\n",
      "Training iter #9960:   Batch Loss = 0.813992, Accuracy = 0.75\n",
      "Training iter #9968:   Batch Loss = 0.406857, Accuracy = 0.875\n",
      "Training iter #9976:   Batch Loss = 0.694236, Accuracy = 0.625\n",
      "Training iter #9984:   Batch Loss = 0.237374, Accuracy = 1.0\n",
      "Training iter #9992:   Batch Loss = 0.734502, Accuracy = 0.625\n",
      "Training iter #10000:   Batch Loss = 0.419695, Accuracy = 0.875\n",
      "Training iter #10008:   Batch Loss = 0.397660, Accuracy = 0.875\n",
      "Training iter #10016:   Batch Loss = 0.449108, Accuracy = 0.875\n",
      "Training iter #10024:   Batch Loss = 0.397447, Accuracy = 0.875\n",
      "Training iter #10032:   Batch Loss = 0.417782, Accuracy = 0.875\n",
      "Training iter #10040:   Batch Loss = 0.089014, Accuracy = 1.0\n",
      "Training iter #10048:   Batch Loss = 0.021369, Accuracy = 1.0\n",
      "Training iter #10056:   Batch Loss = 0.976118, Accuracy = 0.625\n",
      "Training iter #10064:   Batch Loss = 0.714097, Accuracy = 0.625\n",
      "Training iter #10072:   Batch Loss = 0.726666, Accuracy = 0.5\n",
      "Training iter #10080:   Batch Loss = 0.681747, Accuracy = 0.625\n",
      "Training iter #10088:   Batch Loss = 0.415272, Accuracy = 0.875\n",
      "Training iter #10096:   Batch Loss = 0.441642, Accuracy = 0.875\n",
      "Training iter #10104:   Batch Loss = 0.397846, Accuracy = 0.875\n",
      "Training iter #10112:   Batch Loss = 0.497670, Accuracy = 0.875\n",
      "Training iter #10120:   Batch Loss = 0.687008, Accuracy = 0.625\n",
      "Training iter #10128:   Batch Loss = 0.582427, Accuracy = 0.75\n",
      "Training iter #10136:   Batch Loss = 0.077846, Accuracy = 1.0\n",
      "Training iter #10144:   Batch Loss = 0.411165, Accuracy = 0.875\n",
      "Training iter #10152:   Batch Loss = 0.048542, Accuracy = 1.0\n",
      "Training iter #10160:   Batch Loss = 0.403600, Accuracy = 0.875\n",
      "Training iter #10168:   Batch Loss = 0.410310, Accuracy = 0.875\n",
      "Training iter #10176:   Batch Loss = 0.410964, Accuracy = 0.875\n",
      "Training iter #10184:   Batch Loss = 0.080056, Accuracy = 1.0\n",
      "Training iter #10192:   Batch Loss = 0.404124, Accuracy = 0.875\n",
      "Training iter #10200:   Batch Loss = 0.588619, Accuracy = 0.75\n",
      "Training iter #10208:   Batch Loss = 0.059628, Accuracy = 1.0\n",
      "Training iter #10216:   Batch Loss = 0.020894, Accuracy = 1.0\n",
      "Training iter #10224:   Batch Loss = 0.977823, Accuracy = 0.625\n",
      "Training iter #10232:   Batch Loss = 0.582391, Accuracy = 0.75\n",
      "Training iter #10240:   Batch Loss = 0.725343, Accuracy = 0.5\n",
      "Training iter #10248:   Batch Loss = 0.501863, Accuracy = 0.875\n",
      "Training iter #10256:   Batch Loss = 0.021725, Accuracy = 1.0\n",
      "Training iter #10264:   Batch Loss = 0.401613, Accuracy = 0.875\n",
      "Training iter #10272:   Batch Loss = 0.397387, Accuracy = 0.875\n",
      "Training iter #10280:   Batch Loss = 0.594289, Accuracy = 0.75\n",
      "Training iter #10288:   Batch Loss = 0.582824, Accuracy = 0.75\n",
      "Training iter #10296:   Batch Loss = 0.615081, Accuracy = 0.75\n",
      "Training iter #10304:   Batch Loss = 0.535096, Accuracy = 0.875\n",
      "Training iter #10312:   Batch Loss = 0.685126, Accuracy = 0.625\n",
      "Training iter #10320:   Batch Loss = 0.681510, Accuracy = 0.625\n",
      "Training iter #10328:   Batch Loss = 0.588669, Accuracy = 0.75\n",
      "Training iter #10336:   Batch Loss = 0.399288, Accuracy = 0.875\n",
      "Training iter #10344:   Batch Loss = 0.583750, Accuracy = 0.75\n",
      "Training iter #10352:   Batch Loss = 0.397169, Accuracy = 0.875\n",
      "Training iter #10360:   Batch Loss = 0.021931, Accuracy = 1.0\n",
      "Training iter #10368:   Batch Loss = 0.021314, Accuracy = 1.0\n",
      "Training iter #10376:   Batch Loss = 0.020720, Accuracy = 1.0\n",
      "Training iter #10384:   Batch Loss = 0.906073, Accuracy = 0.625\n",
      "Training iter #10392:   Batch Loss = 0.170099, Accuracy = 1.0\n",
      "Training iter #10400:   Batch Loss = 0.584224, Accuracy = 0.75\n",
      "Training iter #10408:   Batch Loss = 0.683067, Accuracy = 0.625\n",
      "Training iter #10416:   Batch Loss = 0.680373, Accuracy = 0.625\n",
      "Training iter #10424:   Batch Loss = 0.590716, Accuracy = 0.75\n",
      "Training iter #10432:   Batch Loss = 0.402641, Accuracy = 0.875\n",
      "Training iter #10440:   Batch Loss = 0.581816, Accuracy = 0.75\n",
      "Training iter #10448:   Batch Loss = 0.581415, Accuracy = 0.75\n",
      "Training iter #10456:   Batch Loss = 0.059044, Accuracy = 1.0\n",
      "Training iter #10464:   Batch Loss = 1.533254, Accuracy = 0.375\n",
      "Training iter #10472:   Batch Loss = 0.445828, Accuracy = 0.875\n",
      "Training iter #10480:   Batch Loss = 0.396228, Accuracy = 0.875\n",
      "Training iter #10488:   Batch Loss = 0.401708, Accuracy = 0.875\n",
      "Training iter #10496:   Batch Loss = 0.591429, Accuracy = 0.75\n",
      "Training iter #10504:   Batch Loss = 0.216204, Accuracy = 1.0\n",
      "Training iter #10512:   Batch Loss = 0.020461, Accuracy = 1.0\n",
      "Training iter #10520:   Batch Loss = 0.967103, Accuracy = 0.625\n",
      "Training iter #10528:   Batch Loss = 0.419628, Accuracy = 0.875\n",
      "Training iter #10536:   Batch Loss = 0.864024, Accuracy = 0.375\n",
      "Training iter #10544:   Batch Loss = 0.729610, Accuracy = 0.375\n",
      "Training iter #10552:   Batch Loss = 0.652389, Accuracy = 0.75\n",
      "Training iter #10560:   Batch Loss = 0.593641, Accuracy = 0.75\n",
      "Training iter #10568:   Batch Loss = 0.194291, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #10576:   Batch Loss = 0.410902, Accuracy = 0.875\n",
      "Training iter #10584:   Batch Loss = 0.581568, Accuracy = 0.75\n",
      "Training iter #10592:   Batch Loss = 0.681935, Accuracy = 0.625\n",
      "Training iter #10600:   Batch Loss = 0.587175, Accuracy = 0.75\n",
      "Training iter #10608:   Batch Loss = 0.594873, Accuracy = 0.75\n",
      "Training iter #10616:   Batch Loss = 0.406932, Accuracy = 0.875\n",
      "Training iter #10624:   Batch Loss = 0.020339, Accuracy = 1.0\n",
      "Training iter #10632:   Batch Loss = 0.816232, Accuracy = 0.75\n",
      "Training iter #10640:   Batch Loss = 0.405271, Accuracy = 0.875\n",
      "Training iter #10648:   Batch Loss = 0.691970, Accuracy = 0.625\n",
      "Training iter #10656:   Batch Loss = 0.232495, Accuracy = 1.0\n",
      "Training iter #10664:   Batch Loss = 0.738592, Accuracy = 0.625\n",
      "Training iter #10672:   Batch Loss = 0.417868, Accuracy = 0.875\n",
      "Training iter #10680:   Batch Loss = 0.396301, Accuracy = 0.875\n",
      "Training iter #10688:   Batch Loss = 0.445806, Accuracy = 0.875\n",
      "Training iter #10696:   Batch Loss = 0.396904, Accuracy = 0.875\n",
      "Training iter #10704:   Batch Loss = 0.432753, Accuracy = 0.875\n",
      "Training iter #10712:   Batch Loss = 0.080959, Accuracy = 1.0\n",
      "Training iter #10720:   Batch Loss = 0.020015, Accuracy = 1.0\n",
      "Training iter #10728:   Batch Loss = 0.962094, Accuracy = 0.625\n",
      "Training iter #10736:   Batch Loss = 0.710877, Accuracy = 0.625\n",
      "Training iter #10744:   Batch Loss = 0.722437, Accuracy = 0.5\n",
      "Training iter #10752:   Batch Loss = 0.680360, Accuracy = 0.625\n",
      "Training iter #10760:   Batch Loss = 0.410127, Accuracy = 0.875\n",
      "Training iter #10768:   Batch Loss = 0.439404, Accuracy = 0.875\n",
      "Training iter #10776:   Batch Loss = 0.397704, Accuracy = 0.875\n",
      "Training iter #10784:   Batch Loss = 0.473963, Accuracy = 0.875\n",
      "Training iter #10792:   Batch Loss = 0.682098, Accuracy = 0.625\n",
      "Training iter #10800:   Batch Loss = 0.580912, Accuracy = 0.75\n",
      "Training iter #10808:   Batch Loss = 0.076349, Accuracy = 1.0\n",
      "Training iter #10816:   Batch Loss = 0.411668, Accuracy = 0.875\n",
      "Training iter #10824:   Batch Loss = 0.051151, Accuracy = 1.0\n",
      "Training iter #10832:   Batch Loss = 0.402488, Accuracy = 0.875\n",
      "Training iter #10840:   Batch Loss = 0.411586, Accuracy = 0.875\n",
      "Training iter #10848:   Batch Loss = 0.406331, Accuracy = 0.875\n",
      "Training iter #10856:   Batch Loss = 0.071548, Accuracy = 1.0\n",
      "Training iter #10864:   Batch Loss = 0.406300, Accuracy = 0.875\n",
      "Training iter #10872:   Batch Loss = 0.588998, Accuracy = 0.75\n",
      "Training iter #10880:   Batch Loss = 0.046802, Accuracy = 1.0\n",
      "Training iter #10888:   Batch Loss = 0.019455, Accuracy = 1.0\n",
      "Training iter #10896:   Batch Loss = 0.994516, Accuracy = 0.625\n",
      "Training iter #10904:   Batch Loss = 0.581032, Accuracy = 0.75\n",
      "Training iter #10912:   Batch Loss = 0.726634, Accuracy = 0.5\n",
      "Training iter #10920:   Batch Loss = 0.503498, Accuracy = 0.875\n",
      "Training iter #10928:   Batch Loss = 0.019862, Accuracy = 1.0\n",
      "Training iter #10936:   Batch Loss = 0.400040, Accuracy = 0.875\n",
      "Training iter #10944:   Batch Loss = 0.396367, Accuracy = 0.875\n",
      "Training iter #10952:   Batch Loss = 0.587281, Accuracy = 0.75\n",
      "Training iter #10960:   Batch Loss = 0.580624, Accuracy = 0.75\n",
      "Training iter #10968:   Batch Loss = 0.581191, Accuracy = 0.75\n",
      "Training iter #10976:   Batch Loss = 0.399792, Accuracy = 0.875\n",
      "Training iter #10984:   Batch Loss = 0.712366, Accuracy = 0.625\n",
      "Training iter #10992:   Batch Loss = 0.681158, Accuracy = 0.625\n",
      "Training iter #11000:   Batch Loss = 0.581164, Accuracy = 0.75\n",
      "Training iter #11008:   Batch Loss = 0.449045, Accuracy = 0.875\n",
      "Training iter #11016:   Batch Loss = 0.580840, Accuracy = 0.75\n",
      "Training iter #11024:   Batch Loss = 0.422510, Accuracy = 0.875\n",
      "Training iter #11032:   Batch Loss = 0.092774, Accuracy = 1.0\n",
      "Training iter #11040:   Batch Loss = 0.019305, Accuracy = 1.0\n",
      "Training iter #11048:   Batch Loss = 0.019031, Accuracy = 1.0\n",
      "Training iter #11056:   Batch Loss = 0.926662, Accuracy = 0.625\n",
      "Training iter #11064:   Batch Loss = 0.169113, Accuracy = 1.0\n",
      "Training iter #11072:   Batch Loss = 0.582723, Accuracy = 0.75\n",
      "Training iter #11080:   Batch Loss = 0.682399, Accuracy = 0.625\n",
      "Training iter #11088:   Batch Loss = 0.679175, Accuracy = 0.625\n",
      "Training iter #11096:   Batch Loss = 0.591751, Accuracy = 0.75\n",
      "Training iter #11104:   Batch Loss = 0.403446, Accuracy = 0.875\n",
      "Training iter #11112:   Batch Loss = 0.580382, Accuracy = 0.75\n",
      "Training iter #11120:   Batch Loss = 0.580017, Accuracy = 0.75\n",
      "Training iter #11128:   Batch Loss = 0.062268, Accuracy = 1.0\n",
      "Training iter #11136:   Batch Loss = 1.535775, Accuracy = 0.375\n",
      "Training iter #11144:   Batch Loss = 0.440081, Accuracy = 0.875\n",
      "Training iter #11152:   Batch Loss = 0.395222, Accuracy = 0.875\n",
      "Training iter #11160:   Batch Loss = 0.404353, Accuracy = 0.875\n",
      "Training iter #11168:   Batch Loss = 0.586709, Accuracy = 0.75\n",
      "Training iter #11176:   Batch Loss = 0.206387, Accuracy = 1.0\n",
      "Training iter #11184:   Batch Loss = 0.019084, Accuracy = 1.0\n",
      "Training iter #11192:   Batch Loss = 0.940834, Accuracy = 0.625\n",
      "Training iter #11200:   Batch Loss = 0.415807, Accuracy = 0.875\n",
      "Training iter #11208:   Batch Loss = 0.860097, Accuracy = 0.375\n",
      "Training iter #11216:   Batch Loss = 0.726787, Accuracy = 0.375\n",
      "Training iter #11224:   Batch Loss = 0.651429, Accuracy = 0.75\n",
      "Training iter #11232:   Batch Loss = 0.596634, Accuracy = 0.75\n",
      "Training iter #11240:   Batch Loss = 0.186001, Accuracy = 1.0\n",
      "Training iter #11248:   Batch Loss = 0.405915, Accuracy = 0.875\n",
      "Training iter #11256:   Batch Loss = 0.583458, Accuracy = 0.75\n",
      "Training iter #11264:   Batch Loss = 0.680403, Accuracy = 0.625\n",
      "Training iter #11272:   Batch Loss = 0.586897, Accuracy = 0.75\n",
      "Training iter #11280:   Batch Loss = 0.590550, Accuracy = 0.75\n",
      "Training iter #11288:   Batch Loss = 0.402317, Accuracy = 0.875\n",
      "Training iter #11296:   Batch Loss = 0.019600, Accuracy = 1.0\n",
      "Training iter #11304:   Batch Loss = 0.872465, Accuracy = 0.75\n",
      "Training iter #11312:   Batch Loss = 0.411641, Accuracy = 0.875\n",
      "Training iter #11320:   Batch Loss = 0.686257, Accuracy = 0.625\n",
      "Training iter #11328:   Batch Loss = 0.230385, Accuracy = 1.0\n",
      "Training iter #11336:   Batch Loss = 0.732539, Accuracy = 0.625\n",
      "Training iter #11344:   Batch Loss = 0.417789, Accuracy = 0.875\n",
      "Training iter #11352:   Batch Loss = 0.395266, Accuracy = 0.875\n",
      "Training iter #11360:   Batch Loss = 0.445959, Accuracy = 0.875\n",
      "Training iter #11368:   Batch Loss = 0.395048, Accuracy = 0.875\n",
      "Training iter #11376:   Batch Loss = 0.423390, Accuracy = 0.875\n",
      "Training iter #11384:   Batch Loss = 0.086242, Accuracy = 1.0\n",
      "Training iter #11392:   Batch Loss = 0.018963, Accuracy = 1.0\n",
      "Training iter #11400:   Batch Loss = 0.963103, Accuracy = 0.625\n",
      "Training iter #11408:   Batch Loss = 0.701977, Accuracy = 0.625\n",
      "Training iter #11416:   Batch Loss = 0.713662, Accuracy = 0.5\n",
      "Training iter #11424:   Batch Loss = 0.680637, Accuracy = 0.625\n",
      "Training iter #11432:   Batch Loss = 0.407181, Accuracy = 0.875\n",
      "Training iter #11440:   Batch Loss = 0.440013, Accuracy = 0.875\n",
      "Training iter #11448:   Batch Loss = 0.395517, Accuracy = 0.875\n",
      "Training iter #11456:   Batch Loss = 0.454306, Accuracy = 0.875\n",
      "Training iter #11464:   Batch Loss = 0.698779, Accuracy = 0.625\n",
      "Training iter #11472:   Batch Loss = 0.580660, Accuracy = 0.75\n",
      "Training iter #11480:   Batch Loss = 0.077599, Accuracy = 1.0\n",
      "Training iter #11488:   Batch Loss = 0.403226, Accuracy = 0.875\n",
      "Training iter #11496:   Batch Loss = 0.037009, Accuracy = 1.0\n",
      "Training iter #11504:   Batch Loss = 0.398545, Accuracy = 0.875\n",
      "Training iter #11512:   Batch Loss = 0.395812, Accuracy = 0.875\n",
      "Training iter #11520:   Batch Loss = 0.462136, Accuracy = 0.875\n",
      "Training iter #11528:   Batch Loss = 0.091449, Accuracy = 1.0\n",
      "Training iter #11536:   Batch Loss = 0.395022, Accuracy = 0.875\n",
      "Training iter #11544:   Batch Loss = 0.580428, Accuracy = 0.75\n",
      "Training iter #11552:   Batch Loss = 0.085005, Accuracy = 1.0\n",
      "Training iter #11560:   Batch Loss = 0.018694, Accuracy = 1.0\n",
      "Training iter #11568:   Batch Loss = 0.943170, Accuracy = 0.625\n",
      "Training iter #11576:   Batch Loss = 0.579929, Accuracy = 0.75\n",
      "Training iter #11584:   Batch Loss = 0.723481, Accuracy = 0.5\n",
      "Training iter #11592:   Batch Loss = 0.498413, Accuracy = 0.875\n",
      "Training iter #11600:   Batch Loss = 0.019240, Accuracy = 1.0\n",
      "Training iter #11608:   Batch Loss = 0.396357, Accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #11616:   Batch Loss = 0.395491, Accuracy = 0.875\n",
      "Training iter #11624:   Batch Loss = 0.585393, Accuracy = 0.75\n",
      "Training iter #11632:   Batch Loss = 0.579533, Accuracy = 0.75\n",
      "Training iter #11640:   Batch Loss = 0.579413, Accuracy = 0.75\n",
      "Training iter #11648:   Batch Loss = 0.399462, Accuracy = 0.875\n",
      "Training iter #11656:   Batch Loss = 0.687585, Accuracy = 0.625\n",
      "Training iter #11664:   Batch Loss = 0.684563, Accuracy = 0.625\n",
      "Training iter #11672:   Batch Loss = 0.647634, Accuracy = 0.75\n",
      "Training iter #11680:   Batch Loss = 0.402485, Accuracy = 0.875\n",
      "Training iter #11688:   Batch Loss = 0.580461, Accuracy = 0.75\n",
      "Training iter #11696:   Batch Loss = 0.395995, Accuracy = 0.875\n",
      "Training iter #11704:   Batch Loss = 0.034707, Accuracy = 1.0\n",
      "Training iter #11712:   Batch Loss = 0.018586, Accuracy = 1.0\n",
      "Training iter #11720:   Batch Loss = 0.018136, Accuracy = 1.0\n",
      "Training iter #11728:   Batch Loss = 1.038346, Accuracy = 0.625\n",
      "Training iter #11736:   Batch Loss = 0.176055, Accuracy = 1.0\n",
      "Training iter #11744:   Batch Loss = 0.581732, Accuracy = 0.75\n",
      "Training iter #11752:   Batch Loss = 0.679954, Accuracy = 0.625\n",
      "Training iter #11760:   Batch Loss = 0.677863, Accuracy = 0.625\n",
      "Training iter #11768:   Batch Loss = 0.582998, Accuracy = 0.75\n",
      "Training iter #11776:   Batch Loss = 0.397948, Accuracy = 0.875\n",
      "Training iter #11784:   Batch Loss = 0.578859, Accuracy = 0.75\n",
      "Training iter #11792:   Batch Loss = 0.578680, Accuracy = 0.75\n",
      "Training iter #11800:   Batch Loss = 0.065716, Accuracy = 1.0\n",
      "Training iter #11808:   Batch Loss = 1.564251, Accuracy = 0.375\n",
      "Training iter #11816:   Batch Loss = 0.427346, Accuracy = 0.875\n",
      "Training iter #11824:   Batch Loss = 0.393687, Accuracy = 0.875\n",
      "Training iter #11832:   Batch Loss = 0.396330, Accuracy = 0.875\n",
      "Training iter #11840:   Batch Loss = 0.578634, Accuracy = 0.75\n",
      "Training iter #11848:   Batch Loss = 0.038247, Accuracy = 1.0\n",
      "Training iter #11856:   Batch Loss = 0.017672, Accuracy = 1.0\n",
      "Training iter #11864:   Batch Loss = 0.992545, Accuracy = 0.625\n",
      "Training iter #11872:   Batch Loss = 0.414384, Accuracy = 0.875\n",
      "Training iter #11880:   Batch Loss = 0.862590, Accuracy = 0.375\n",
      "Training iter #11888:   Batch Loss = 0.726835, Accuracy = 0.375\n",
      "Training iter #11896:   Batch Loss = 0.649783, Accuracy = 0.75\n",
      "Training iter #11904:   Batch Loss = 0.593496, Accuracy = 0.75\n",
      "Training iter #11912:   Batch Loss = 0.193610, Accuracy = 1.0\n",
      "Training iter #11920:   Batch Loss = 0.407161, Accuracy = 0.875\n",
      "Training iter #11928:   Batch Loss = 0.580110, Accuracy = 0.75\n",
      "Training iter #11936:   Batch Loss = 0.679054, Accuracy = 0.625\n",
      "Training iter #11944:   Batch Loss = 0.583002, Accuracy = 0.75\n",
      "Training iter #11952:   Batch Loss = 0.595452, Accuracy = 0.75\n",
      "Training iter #11960:   Batch Loss = 0.402787, Accuracy = 0.875\n",
      "Training iter #11968:   Batch Loss = 0.017717, Accuracy = 1.0\n",
      "Training iter #11976:   Batch Loss = 0.831784, Accuracy = 0.75\n",
      "Training iter #11984:   Batch Loss = 0.404668, Accuracy = 0.875\n",
      "Training iter #11992:   Batch Loss = 0.687237, Accuracy = 0.625\n",
      "Training iter #12000:   Batch Loss = 0.228379, Accuracy = 1.0\n",
      "Training iter #12008:   Batch Loss = 0.734500, Accuracy = 0.625\n",
      "Training iter #12016:   Batch Loss = 0.415817, Accuracy = 0.875\n",
      "Training iter #12024:   Batch Loss = 0.393709, Accuracy = 0.875\n",
      "Training iter #12032:   Batch Loss = 0.454424, Accuracy = 0.875\n",
      "Training iter #12040:   Batch Loss = 0.393914, Accuracy = 0.875\n",
      "Training iter #12048:   Batch Loss = 0.422103, Accuracy = 0.875\n",
      "Training iter #12056:   Batch Loss = 0.083956, Accuracy = 1.0\n",
      "Training iter #12064:   Batch Loss = 0.017887, Accuracy = 1.0\n",
      "Training iter #12072:   Batch Loss = 0.975032, Accuracy = 0.625\n",
      "Training iter #12080:   Batch Loss = 0.699283, Accuracy = 0.625\n",
      "Training iter #12088:   Batch Loss = 0.717937, Accuracy = 0.5\n",
      "Training iter #12096:   Batch Loss = 0.678253, Accuracy = 0.625\n",
      "Training iter #12104:   Batch Loss = 0.397891, Accuracy = 0.875\n",
      "Training iter #12112:   Batch Loss = 0.445866, Accuracy = 0.875\n",
      "Training iter #12120:   Batch Loss = 0.396300, Accuracy = 0.875\n",
      "Training iter #12128:   Batch Loss = 0.467462, Accuracy = 0.875\n",
      "Training iter #12136:   Batch Loss = 0.678389, Accuracy = 0.625\n",
      "Training iter #12144:   Batch Loss = 0.578860, Accuracy = 0.75\n",
      "Training iter #12152:   Batch Loss = 0.087348, Accuracy = 1.0\n",
      "Training iter #12160:   Batch Loss = 0.398998, Accuracy = 0.875\n",
      "Training iter #12168:   Batch Loss = 0.028993, Accuracy = 1.0\n",
      "Training iter #12176:   Batch Loss = 0.393996, Accuracy = 0.875\n",
      "Training iter #12184:   Batch Loss = 0.393764, Accuracy = 0.875\n",
      "Training iter #12192:   Batch Loss = 0.492021, Accuracy = 0.875\n",
      "Training iter #12200:   Batch Loss = 0.093373, Accuracy = 1.0\n",
      "Training iter #12208:   Batch Loss = 0.393863, Accuracy = 0.875\n",
      "Training iter #12216:   Batch Loss = 0.580082, Accuracy = 0.75\n",
      "Training iter #12224:   Batch Loss = 0.079170, Accuracy = 1.0\n",
      "Training iter #12232:   Batch Loss = 0.017704, Accuracy = 1.0\n",
      "Training iter #12240:   Batch Loss = 0.955470, Accuracy = 0.625\n",
      "Training iter #12248:   Batch Loss = 0.578696, Accuracy = 0.75\n",
      "Training iter #12256:   Batch Loss = 0.725775, Accuracy = 0.5\n",
      "Training iter #12264:   Batch Loss = 0.498746, Accuracy = 0.875\n",
      "Training iter #12272:   Batch Loss = 0.018161, Accuracy = 1.0\n",
      "Training iter #12280:   Batch Loss = 0.394991, Accuracy = 0.875\n",
      "Training iter #12288:   Batch Loss = 0.394449, Accuracy = 0.875\n",
      "Training iter #12296:   Batch Loss = 0.583139, Accuracy = 0.75\n",
      "Training iter #12304:   Batch Loss = 0.578292, Accuracy = 0.75\n",
      "Training iter #12312:   Batch Loss = 0.582320, Accuracy = 0.75\n",
      "Training iter #12320:   Batch Loss = 0.405616, Accuracy = 0.875\n",
      "Training iter #12328:   Batch Loss = 0.711102, Accuracy = 0.625\n",
      "Training iter #12336:   Batch Loss = 0.679946, Accuracy = 0.625\n",
      "Training iter #12344:   Batch Loss = 0.578422, Accuracy = 0.75\n",
      "Training iter #12352:   Batch Loss = 0.457534, Accuracy = 0.875\n",
      "Training iter #12360:   Batch Loss = 0.578544, Accuracy = 0.75\n",
      "Training iter #12368:   Batch Loss = 0.427688, Accuracy = 0.875\n",
      "Training iter #12376:   Batch Loss = 0.087061, Accuracy = 1.0\n",
      "Training iter #12384:   Batch Loss = 0.017427, Accuracy = 1.0\n",
      "Training iter #12392:   Batch Loss = 0.017048, Accuracy = 1.0\n",
      "Training iter #12400:   Batch Loss = 0.970626, Accuracy = 0.625\n",
      "Training iter #12408:   Batch Loss = 0.178241, Accuracy = 1.0\n",
      "Training iter #12416:   Batch Loss = 0.579180, Accuracy = 0.75\n",
      "Training iter #12424:   Batch Loss = 0.678582, Accuracy = 0.625\n",
      "Training iter #12432:   Batch Loss = 0.676981, Accuracy = 0.625\n",
      "Training iter #12440:   Batch Loss = 0.593055, Accuracy = 0.75\n",
      "Training iter #12448:   Batch Loss = 0.402339, Accuracy = 0.875\n",
      "Training iter #12456:   Batch Loss = 0.578459, Accuracy = 0.75\n",
      "Training iter #12464:   Batch Loss = 0.577861, Accuracy = 0.75\n",
      "Training iter #12472:   Batch Loss = 0.053145, Accuracy = 1.0\n",
      "Training iter #12480:   Batch Loss = 1.503818, Accuracy = 0.375\n",
      "Training iter #12488:   Batch Loss = 0.440957, Accuracy = 0.875\n",
      "Training iter #12496:   Batch Loss = 0.393755, Accuracy = 0.875\n",
      "Training iter #12504:   Batch Loss = 0.436504, Accuracy = 0.875\n",
      "Training iter #12512:   Batch Loss = 0.578251, Accuracy = 0.75\n",
      "Training iter #12520:   Batch Loss = 0.078711, Accuracy = 1.0\n",
      "Training iter #12528:   Batch Loss = 0.016892, Accuracy = 1.0\n",
      "Training iter #12536:   Batch Loss = 0.924979, Accuracy = 0.625\n",
      "Training iter #12544:   Batch Loss = 0.409097, Accuracy = 0.875\n",
      "Training iter #12552:   Batch Loss = 0.852161, Accuracy = 0.375\n",
      "Training iter #12560:   Batch Loss = 0.707762, Accuracy = 0.625\n",
      "Training iter #12568:   Batch Loss = 0.684098, Accuracy = 0.75\n",
      "Training iter #12576:   Batch Loss = 0.591179, Accuracy = 0.75\n",
      "Training iter #12584:   Batch Loss = 0.176158, Accuracy = 1.0\n",
      "Training iter #12592:   Batch Loss = 0.401864, Accuracy = 0.875\n",
      "Training iter #12600:   Batch Loss = 0.582002, Accuracy = 0.75\n",
      "Training iter #12608:   Batch Loss = 0.678731, Accuracy = 0.625\n",
      "Training iter #12616:   Batch Loss = 0.588284, Accuracy = 0.75\n",
      "Training iter #12624:   Batch Loss = 0.585681, Accuracy = 0.75\n",
      "Training iter #12632:   Batch Loss = 0.398131, Accuracy = 0.875\n",
      "Training iter #12640:   Batch Loss = 0.017125, Accuracy = 1.0\n",
      "Training iter #12648:   Batch Loss = 0.806064, Accuracy = 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #12656:   Batch Loss = 0.401047, Accuracy = 0.875\n",
      "Training iter #12664:   Batch Loss = 0.688389, Accuracy = 0.625\n",
      "Training iter #12672:   Batch Loss = 0.226235, Accuracy = 1.0\n",
      "Training iter #12680:   Batch Loss = 0.735755, Accuracy = 0.625\n",
      "Training iter #12688:   Batch Loss = 0.412230, Accuracy = 0.875\n",
      "Training iter #12696:   Batch Loss = 0.392351, Accuracy = 0.875\n",
      "Training iter #12704:   Batch Loss = 0.394327, Accuracy = 0.875\n",
      "Training iter #12712:   Batch Loss = 0.403667, Accuracy = 0.875\n",
      "Training iter #12720:   Batch Loss = 0.404298, Accuracy = 0.875\n",
      "Training iter #12728:   Batch Loss = 0.078098, Accuracy = 1.0\n",
      "Training iter #12736:   Batch Loss = 0.016790, Accuracy = 1.0\n",
      "Training iter #12744:   Batch Loss = 1.043612, Accuracy = 0.625\n",
      "Training iter #12752:   Batch Loss = 0.698196, Accuracy = 0.625\n",
      "Training iter #12760:   Batch Loss = 0.719048, Accuracy = 0.5\n",
      "Training iter #12768:   Batch Loss = 0.677457, Accuracy = 0.625\n",
      "Training iter #12776:   Batch Loss = 0.420764, Accuracy = 0.875\n",
      "Training iter #12784:   Batch Loss = 0.443596, Accuracy = 0.875\n",
      "Training iter #12792:   Batch Loss = 0.395933, Accuracy = 0.875\n",
      "Training iter #12800:   Batch Loss = 0.432868, Accuracy = 0.875\n",
      "Training iter #12808:   Batch Loss = 0.691941, Accuracy = 0.625\n",
      "Training iter #12816:   Batch Loss = 0.578276, Accuracy = 0.75\n",
      "Training iter #12824:   Batch Loss = 0.073298, Accuracy = 1.0\n",
      "Training iter #12832:   Batch Loss = 0.400195, Accuracy = 0.875\n",
      "Training iter #12840:   Batch Loss = 0.035349, Accuracy = 1.0\n",
      "Training iter #12848:   Batch Loss = 0.395909, Accuracy = 0.875\n",
      "Training iter #12856:   Batch Loss = 0.392725, Accuracy = 0.875\n",
      "Training iter #12864:   Batch Loss = 0.442734, Accuracy = 0.875\n",
      "Training iter #12872:   Batch Loss = 0.083365, Accuracy = 1.0\n",
      "Training iter #12880:   Batch Loss = 0.394399, Accuracy = 0.875\n",
      "Training iter #12888:   Batch Loss = 0.578864, Accuracy = 0.75\n",
      "Training iter #12896:   Batch Loss = 0.080429, Accuracy = 1.0\n",
      "Training iter #12904:   Batch Loss = 0.016824, Accuracy = 1.0\n",
      "Training iter #12912:   Batch Loss = 0.961450, Accuracy = 0.625\n",
      "Training iter #12920:   Batch Loss = 0.577909, Accuracy = 0.75\n",
      "Training iter #12928:   Batch Loss = 0.724898, Accuracy = 0.5\n",
      "Training iter #12936:   Batch Loss = 0.497625, Accuracy = 0.875\n",
      "Training iter #12944:   Batch Loss = 0.017873, Accuracy = 1.0\n",
      "Training iter #12952:   Batch Loss = 0.393555, Accuracy = 0.875\n",
      "Training iter #12960:   Batch Loss = 0.393721, Accuracy = 0.875\n",
      "Training iter #12968:   Batch Loss = 0.581206, Accuracy = 0.75\n",
      "Training iter #12976:   Batch Loss = 0.577421, Accuracy = 0.75\n",
      "Training iter #12984:   Batch Loss = 0.580596, Accuracy = 0.75\n",
      "Training iter #12992:   Batch Loss = 0.404226, Accuracy = 0.875\n",
      "Training iter #13000:   Batch Loss = 0.710188, Accuracy = 0.625\n",
      "Training iter #13008:   Batch Loss = 0.678207, Accuracy = 0.625\n",
      "Training iter #13016:   Batch Loss = 0.577984, Accuracy = 0.75\n",
      "Training iter #13024:   Batch Loss = 0.439240, Accuracy = 0.875\n",
      "Training iter #13032:   Batch Loss = 0.577864, Accuracy = 0.75\n",
      "Training iter #13040:   Batch Loss = 0.418851, Accuracy = 0.875\n",
      "Training iter #13048:   Batch Loss = 0.083396, Accuracy = 1.0\n",
      "Training iter #13056:   Batch Loss = 0.016552, Accuracy = 1.0\n",
      "Training iter #13064:   Batch Loss = 0.016211, Accuracy = 1.0\n",
      "Training iter #13072:   Batch Loss = 0.972035, Accuracy = 0.625\n",
      "Training iter #13080:   Batch Loss = 0.178934, Accuracy = 1.0\n",
      "Training iter #13088:   Batch Loss = 0.578207, Accuracy = 0.75\n",
      "Training iter #13096:   Batch Loss = 0.677697, Accuracy = 0.625\n",
      "Training iter #13104:   Batch Loss = 0.676134, Accuracy = 0.625\n",
      "Training iter #13112:   Batch Loss = 0.592575, Accuracy = 0.75\n",
      "Training iter #13120:   Batch Loss = 0.400581, Accuracy = 0.875\n",
      "Training iter #13128:   Batch Loss = 0.577785, Accuracy = 0.75\n",
      "Training iter #13136:   Batch Loss = 0.577064, Accuracy = 0.75\n",
      "Training iter #13144:   Batch Loss = 0.045511, Accuracy = 1.0\n",
      "Training iter #13152:   Batch Loss = 1.487643, Accuracy = 0.375\n",
      "Training iter #13160:   Batch Loss = 0.449406, Accuracy = 0.875\n",
      "Training iter #13168:   Batch Loss = 0.393072, Accuracy = 0.875\n",
      "Training iter #13176:   Batch Loss = 0.438794, Accuracy = 0.875\n",
      "Training iter #13184:   Batch Loss = 0.577422, Accuracy = 0.75\n",
      "Training iter #13192:   Batch Loss = 0.072779, Accuracy = 1.0\n",
      "Training iter #13200:   Batch Loss = 0.016163, Accuracy = 1.0\n",
      "Training iter #13208:   Batch Loss = 0.930211, Accuracy = 0.625\n",
      "Training iter #13216:   Batch Loss = 0.410623, Accuracy = 0.875\n",
      "Training iter #13224:   Batch Loss = 0.848246, Accuracy = 0.375\n",
      "Training iter #13232:   Batch Loss = 0.711851, Accuracy = 0.375\n",
      "Training iter #13240:   Batch Loss = 0.655593, Accuracy = 0.75\n",
      "Training iter #13248:   Batch Loss = 0.594452, Accuracy = 0.75\n",
      "Training iter #13256:   Batch Loss = 0.182459, Accuracy = 1.0\n",
      "Training iter #13264:   Batch Loss = 0.401168, Accuracy = 0.875\n",
      "Training iter #13272:   Batch Loss = 0.586010, Accuracy = 0.75\n",
      "Training iter #13280:   Batch Loss = 0.678396, Accuracy = 0.625\n",
      "Training iter #13288:   Batch Loss = 0.584725, Accuracy = 0.75\n",
      "Training iter #13296:   Batch Loss = 0.589517, Accuracy = 0.75\n",
      "Training iter #13304:   Batch Loss = 0.398627, Accuracy = 0.875\n",
      "Training iter #13312:   Batch Loss = 0.016242, Accuracy = 1.0\n",
      "Training iter #13320:   Batch Loss = 0.805525, Accuracy = 0.75\n",
      "Training iter #13328:   Batch Loss = 0.400333, Accuracy = 0.875\n",
      "Training iter #13336:   Batch Loss = 0.685197, Accuracy = 0.625\n",
      "Training iter #13344:   Batch Loss = 0.222814, Accuracy = 1.0\n",
      "Training iter #13352:   Batch Loss = 0.734946, Accuracy = 0.625\n",
      "Training iter #13360:   Batch Loss = 0.412939, Accuracy = 0.875\n",
      "Training iter #13368:   Batch Loss = 0.391693, Accuracy = 0.875\n",
      "Training iter #13376:   Batch Loss = 0.438554, Accuracy = 0.875\n",
      "Training iter #13384:   Batch Loss = 0.393887, Accuracy = 0.875\n",
      "Training iter #13392:   Batch Loss = 0.437677, Accuracy = 0.875\n",
      "Training iter #13400:   Batch Loss = 0.072336, Accuracy = 1.0\n",
      "Training iter #13408:   Batch Loss = 0.016242, Accuracy = 1.0\n",
      "Training iter #13416:   Batch Loss = 0.979589, Accuracy = 0.625\n",
      "Training iter #13424:   Batch Loss = 0.691348, Accuracy = 0.625\n",
      "Training iter #13432:   Batch Loss = 0.714258, Accuracy = 0.5\n",
      "Training iter #13440:   Batch Loss = 0.676507, Accuracy = 0.625\n",
      "Training iter #13448:   Batch Loss = 0.397365, Accuracy = 0.875\n",
      "Training iter #13456:   Batch Loss = 0.440416, Accuracy = 0.875\n",
      "Training iter #13464:   Batch Loss = 0.393764, Accuracy = 0.875\n",
      "Training iter #13472:   Batch Loss = 0.484538, Accuracy = 0.875\n",
      "Training iter #13480:   Batch Loss = 0.676891, Accuracy = 0.625\n",
      "Training iter #13488:   Batch Loss = 0.577301, Accuracy = 0.75\n",
      "Training iter #13496:   Batch Loss = 0.082324, Accuracy = 1.0\n",
      "Training iter #13504:   Batch Loss = 0.398049, Accuracy = 0.875\n",
      "Training iter #13512:   Batch Loss = 0.027575, Accuracy = 1.0\n",
      "Training iter #13520:   Batch Loss = 0.391649, Accuracy = 0.875\n",
      "Training iter #13528:   Batch Loss = 0.392020, Accuracy = 0.875\n",
      "Training iter #13536:   Batch Loss = 0.527946, Accuracy = 0.875\n",
      "Training iter #13544:   Batch Loss = 0.090853, Accuracy = 1.0\n",
      "Training iter #13552:   Batch Loss = 0.392449, Accuracy = 0.875\n",
      "Training iter #13560:   Batch Loss = 0.578818, Accuracy = 0.75\n",
      "Training iter #13568:   Batch Loss = 0.070962, Accuracy = 1.0\n",
      "Training iter #13576:   Batch Loss = 0.016324, Accuracy = 1.0\n",
      "Training iter #13584:   Batch Loss = 0.965708, Accuracy = 0.625\n",
      "Training iter #13592:   Batch Loss = 0.577012, Accuracy = 0.75\n",
      "Training iter #13600:   Batch Loss = 0.724590, Accuracy = 0.5\n",
      "Training iter #13608:   Batch Loss = 0.497079, Accuracy = 0.875\n",
      "Training iter #13616:   Batch Loss = 0.017230, Accuracy = 1.0\n",
      "Training iter #13624:   Batch Loss = 0.392118, Accuracy = 0.875\n",
      "Training iter #13632:   Batch Loss = 0.391856, Accuracy = 0.875\n",
      "Training iter #13640:   Batch Loss = 0.584408, Accuracy = 0.75\n",
      "Training iter #13648:   Batch Loss = 0.576634, Accuracy = 0.75\n",
      "Training iter #13656:   Batch Loss = 0.587172, Accuracy = 0.75\n",
      "Training iter #13664:   Batch Loss = 0.405109, Accuracy = 0.875\n",
      "Training iter #13672:   Batch Loss = 0.710515, Accuracy = 0.625\n",
      "Training iter #13680:   Batch Loss = 0.679512, Accuracy = 0.625\n",
      "Training iter #13688:   Batch Loss = 0.576755, Accuracy = 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #13696:   Batch Loss = 0.465606, Accuracy = 0.875\n",
      "Training iter #13704:   Batch Loss = 0.576694, Accuracy = 0.75\n",
      "Training iter #13712:   Batch Loss = 0.429969, Accuracy = 0.875\n",
      "Training iter #13720:   Batch Loss = 0.086484, Accuracy = 1.0\n",
      "Training iter #13728:   Batch Loss = 0.016157, Accuracy = 1.0\n",
      "Training iter #13736:   Batch Loss = 0.015674, Accuracy = 1.0\n",
      "Training iter #13744:   Batch Loss = 0.977061, Accuracy = 0.625\n",
      "Training iter #13752:   Batch Loss = 0.181897, Accuracy = 1.0\n",
      "Training iter #13760:   Batch Loss = 0.577470, Accuracy = 0.75\n",
      "Training iter #13768:   Batch Loss = 0.677269, Accuracy = 0.625\n",
      "Training iter #13776:   Batch Loss = 0.675392, Accuracy = 0.625\n",
      "Training iter #13784:   Batch Loss = 0.590027, Accuracy = 0.75\n",
      "Training iter #13792:   Batch Loss = 0.400608, Accuracy = 0.875\n",
      "Training iter #13800:   Batch Loss = 0.576656, Accuracy = 0.75\n",
      "Training iter #13808:   Batch Loss = 0.576292, Accuracy = 0.75\n",
      "Training iter #13816:   Batch Loss = 0.052995, Accuracy = 1.0\n",
      "Training iter #13824:   Batch Loss = 1.515435, Accuracy = 0.375\n",
      "Training iter #13832:   Batch Loss = 0.436188, Accuracy = 0.875\n",
      "Training iter #13840:   Batch Loss = 0.391814, Accuracy = 0.875\n",
      "Training iter #13848:   Batch Loss = 0.419607, Accuracy = 0.875\n",
      "Training iter #13856:   Batch Loss = 0.577644, Accuracy = 0.75\n",
      "Training iter #13864:   Batch Loss = 0.115653, Accuracy = 1.0\n",
      "Training iter #13872:   Batch Loss = 0.015554, Accuracy = 1.0\n",
      "Training iter #13880:   Batch Loss = 0.892743, Accuracy = 0.625\n",
      "Training iter #13888:   Batch Loss = 0.404597, Accuracy = 0.875\n",
      "Training iter #13896:   Batch Loss = 0.848811, Accuracy = 0.375\n",
      "Training iter #13904:   Batch Loss = 0.706007, Accuracy = 0.625\n",
      "Training iter #13912:   Batch Loss = 0.707705, Accuracy = 0.75\n",
      "Training iter #13920:   Batch Loss = 0.581691, Accuracy = 0.75\n",
      "Training iter #13928:   Batch Loss = 0.138284, Accuracy = 1.0\n",
      "Training iter #13936:   Batch Loss = 0.392270, Accuracy = 0.875\n",
      "Training iter #13944:   Batch Loss = 0.589070, Accuracy = 0.75\n",
      "Training iter #13952:   Batch Loss = 0.676918, Accuracy = 0.625\n",
      "Training iter #13960:   Batch Loss = 0.580650, Accuracy = 0.75\n",
      "Training iter #13968:   Batch Loss = 0.595402, Accuracy = 0.75\n",
      "Training iter #13976:   Batch Loss = 0.396299, Accuracy = 0.875\n",
      "Training iter #13984:   Batch Loss = 0.015683, Accuracy = 1.0\n",
      "Training iter #13992:   Batch Loss = 0.811464, Accuracy = 0.75\n",
      "Training iter #14000:   Batch Loss = 0.404449, Accuracy = 0.875\n",
      "Training iter #14008:   Batch Loss = 0.683202, Accuracy = 0.625\n",
      "Training iter #14016:   Batch Loss = 0.223366, Accuracy = 1.0\n",
      "Training iter #14024:   Batch Loss = 0.733154, Accuracy = 0.625\n",
      "Training iter #14032:   Batch Loss = 0.412896, Accuracy = 0.875\n",
      "Training iter #14040:   Batch Loss = 0.391105, Accuracy = 0.875\n",
      "Training iter #14048:   Batch Loss = 0.446864, Accuracy = 0.875\n",
      "Training iter #14056:   Batch Loss = 0.392646, Accuracy = 0.875\n",
      "Training iter #14064:   Batch Loss = 0.438744, Accuracy = 0.875\n",
      "Training iter #14072:   Batch Loss = 0.077493, Accuracy = 1.0\n",
      "Training iter #14080:   Batch Loss = 0.016063, Accuracy = 1.0\n",
      "Training iter #14088:   Batch Loss = 1.015371, Accuracy = 0.625\n",
      "Training iter #14096:   Batch Loss = 0.694213, Accuracy = 0.625\n",
      "Training iter #14104:   Batch Loss = 0.715800, Accuracy = 0.5\n",
      "Training iter #14112:   Batch Loss = 0.675921, Accuracy = 0.625\n",
      "Training iter #14120:   Batch Loss = 0.395390, Accuracy = 0.875\n",
      "Training iter #14128:   Batch Loss = 0.443119, Accuracy = 0.875\n",
      "Training iter #14136:   Batch Loss = 0.391832, Accuracy = 0.875\n",
      "Training iter #14144:   Batch Loss = 0.528307, Accuracy = 0.875\n",
      "Training iter #14152:   Batch Loss = 0.678231, Accuracy = 0.625\n",
      "Training iter #14160:   Batch Loss = 0.576147, Accuracy = 0.75\n",
      "Training iter #14168:   Batch Loss = 0.056830, Accuracy = 1.0\n",
      "Training iter #14176:   Batch Loss = 0.405194, Accuracy = 0.875\n",
      "Training iter #14184:   Batch Loss = 0.041390, Accuracy = 1.0\n",
      "Training iter #14192:   Batch Loss = 0.393086, Accuracy = 0.875\n",
      "Training iter #14200:   Batch Loss = 0.404978, Accuracy = 0.875\n",
      "Training iter #14208:   Batch Loss = 0.405961, Accuracy = 0.875\n",
      "Training iter #14216:   Batch Loss = 0.069023, Accuracy = 1.0\n",
      "Training iter #14224:   Batch Loss = 0.397594, Accuracy = 0.875\n",
      "Training iter #14232:   Batch Loss = 0.592737, Accuracy = 0.75\n",
      "Training iter #14240:   Batch Loss = 0.029534, Accuracy = 1.0\n",
      "Training iter #14248:   Batch Loss = 0.015181, Accuracy = 1.0\n",
      "Training iter #14256:   Batch Loss = 0.957700, Accuracy = 0.625\n",
      "Training iter #14264:   Batch Loss = 0.576651, Accuracy = 0.75\n",
      "Training iter #14272:   Batch Loss = 0.719474, Accuracy = 0.5\n",
      "Training iter #14280:   Batch Loss = 0.495082, Accuracy = 0.875\n",
      "Training iter #14288:   Batch Loss = 0.015565, Accuracy = 1.0\n",
      "Training iter #14296:   Batch Loss = 0.391274, Accuracy = 0.875\n",
      "Training iter #14304:   Batch Loss = 0.396738, Accuracy = 0.875\n",
      "Training iter #14312:   Batch Loss = 0.576437, Accuracy = 0.75\n",
      "Training iter #14320:   Batch Loss = 0.576134, Accuracy = 0.75\n",
      "Training iter #14328:   Batch Loss = 0.575972, Accuracy = 0.75\n",
      "Training iter #14336:   Batch Loss = 0.471091, Accuracy = 0.875\n",
      "Training iter #14344:   Batch Loss = 0.675161, Accuracy = 0.625\n",
      "Training iter #14352:   Batch Loss = 0.674930, Accuracy = 0.625\n",
      "Training iter #14360:   Batch Loss = 0.605523, Accuracy = 0.75\n",
      "Training iter #14368:   Batch Loss = 0.403004, Accuracy = 0.875\n",
      "Training iter #14376:   Batch Loss = 0.577082, Accuracy = 0.75\n",
      "Training iter #14384:   Batch Loss = 0.391370, Accuracy = 0.875\n",
      "Training iter #14392:   Batch Loss = 0.015923, Accuracy = 1.0\n",
      "Training iter #14400:   Batch Loss = 0.015349, Accuracy = 1.0\n",
      "Training iter #14408:   Batch Loss = 0.014888, Accuracy = 1.0\n",
      "Training iter #14416:   Batch Loss = 1.018109, Accuracy = 0.625\n",
      "Training iter #14424:   Batch Loss = 0.185505, Accuracy = 1.0\n",
      "Training iter #14432:   Batch Loss = 0.576439, Accuracy = 0.75\n",
      "Training iter #14440:   Batch Loss = 0.676278, Accuracy = 0.625\n",
      "Training iter #14448:   Batch Loss = 0.674601, Accuracy = 0.625\n",
      "Training iter #14456:   Batch Loss = 0.590377, Accuracy = 0.75\n",
      "Training iter #14464:   Batch Loss = 0.400783, Accuracy = 0.875\n",
      "Training iter #14472:   Batch Loss = 0.575997, Accuracy = 0.75\n",
      "Training iter #14480:   Batch Loss = 0.575574, Accuracy = 0.75\n",
      "Training iter #14488:   Batch Loss = 0.052021, Accuracy = 1.0\n",
      "Training iter #14496:   Batch Loss = 1.486390, Accuracy = 0.375\n",
      "Training iter #14504:   Batch Loss = 0.436060, Accuracy = 0.875\n",
      "Training iter #14512:   Batch Loss = 0.391462, Accuracy = 0.875\n",
      "Training iter #14520:   Batch Loss = 0.427211, Accuracy = 0.875\n",
      "Training iter #14528:   Batch Loss = 0.576082, Accuracy = 0.75\n",
      "Training iter #14536:   Batch Loss = 0.083352, Accuracy = 1.0\n",
      "Training iter #14544:   Batch Loss = 0.014774, Accuracy = 1.0\n",
      "Training iter #14552:   Batch Loss = 0.918207, Accuracy = 0.625\n",
      "Training iter #14560:   Batch Loss = 0.408029, Accuracy = 0.875\n",
      "Training iter #14568:   Batch Loss = 0.842917, Accuracy = 0.375\n",
      "Training iter #14576:   Batch Loss = 0.701360, Accuracy = 0.625\n",
      "Training iter #14584:   Batch Loss = 0.658683, Accuracy = 0.75\n",
      "Training iter #14592:   Batch Loss = 0.593597, Accuracy = 0.75\n",
      "Training iter #14600:   Batch Loss = 0.187063, Accuracy = 1.0\n",
      "Training iter #14608:   Batch Loss = 0.403308, Accuracy = 0.875\n",
      "Training iter #14616:   Batch Loss = 0.583381, Accuracy = 0.75\n",
      "Training iter #14624:   Batch Loss = 0.677107, Accuracy = 0.625\n",
      "Training iter #14632:   Batch Loss = 0.584341, Accuracy = 0.75\n",
      "Training iter #14640:   Batch Loss = 0.586566, Accuracy = 0.75\n",
      "Training iter #14648:   Batch Loss = 0.397670, Accuracy = 0.875\n",
      "Training iter #14656:   Batch Loss = 0.015407, Accuracy = 1.0\n",
      "Training iter #14664:   Batch Loss = 0.809523, Accuracy = 0.75\n",
      "Training iter #14672:   Batch Loss = 0.402000, Accuracy = 0.875\n",
      "Training iter #14680:   Batch Loss = 0.682985, Accuracy = 0.625\n",
      "Training iter #14688:   Batch Loss = 0.221965, Accuracy = 1.0\n",
      "Training iter #14696:   Batch Loss = 0.732253, Accuracy = 0.625\n",
      "Training iter #14704:   Batch Loss = 0.411346, Accuracy = 0.875\n",
      "Training iter #14712:   Batch Loss = 0.390267, Accuracy = 0.875\n",
      "Training iter #14720:   Batch Loss = 0.390032, Accuracy = 0.875\n",
      "Training iter #14728:   Batch Loss = 0.608704, Accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #14736:   Batch Loss = 0.390251, Accuracy = 0.875\n",
      "Training iter #14744:   Batch Loss = 0.021609, Accuracy = 1.0\n",
      "Training iter #14752:   Batch Loss = 0.014776, Accuracy = 1.0\n",
      "Training iter #14760:   Batch Loss = 1.039435, Accuracy = 0.625\n",
      "Training iter #14768:   Batch Loss = 0.688668, Accuracy = 0.625\n",
      "Training iter #14776:   Batch Loss = 0.711994, Accuracy = 0.5\n",
      "Training iter #14784:   Batch Loss = 0.674851, Accuracy = 0.625\n",
      "Training iter #14792:   Batch Loss = 0.394357, Accuracy = 0.875\n",
      "Training iter #14800:   Batch Loss = 0.447277, Accuracy = 0.875\n",
      "Training iter #14808:   Batch Loss = 0.392201, Accuracy = 0.875\n",
      "Training iter #14816:   Batch Loss = 0.491058, Accuracy = 0.875\n",
      "Training iter #14824:   Batch Loss = 0.674906, Accuracy = 0.625\n",
      "Training iter #14832:   Batch Loss = 0.577279, Accuracy = 0.75\n",
      "Training iter #14840:   Batch Loss = 0.106501, Accuracy = 1.0\n",
      "Training iter #14848:   Batch Loss = 0.390730, Accuracy = 0.875\n",
      "Training iter #14856:   Batch Loss = 0.021331, Accuracy = 1.0\n",
      "Training iter #14864:   Batch Loss = 0.389982, Accuracy = 0.875\n",
      "Training iter #14872:   Batch Loss = 0.389864, Accuracy = 0.875\n",
      "Training iter #14880:   Batch Loss = 0.400736, Accuracy = 0.875\n",
      "Training iter #14888:   Batch Loss = 0.014205, Accuracy = 1.0\n",
      "Training iter #14896:   Batch Loss = 0.473434, Accuracy = 0.875\n",
      "Training iter #14904:   Batch Loss = 0.575715, Accuracy = 0.75\n",
      "Training iter #14912:   Batch Loss = 0.037176, Accuracy = 1.0\n",
      "Training iter #14920:   Batch Loss = 0.014379, Accuracy = 1.0\n",
      "Training iter #14928:   Batch Loss = 0.960721, Accuracy = 0.625\n",
      "Training iter #14936:   Batch Loss = 0.576046, Accuracy = 0.75\n",
      "Training iter #14944:   Batch Loss = 0.717198, Accuracy = 0.5\n",
      "Training iter #14952:   Batch Loss = 0.492702, Accuracy = 0.875\n",
      "Training iter #14960:   Batch Loss = 0.014911, Accuracy = 1.0\n",
      "Training iter #14968:   Batch Loss = 0.390374, Accuracy = 0.875\n",
      "Training iter #14976:   Batch Loss = 0.395730, Accuracy = 0.875\n",
      "Training iter #14984:   Batch Loss = 0.575582, Accuracy = 0.75\n",
      "Training iter #14992:   Batch Loss = 0.575287, Accuracy = 0.75\n",
      "Training iter #15000:   Batch Loss = 0.575127, Accuracy = 0.75\n",
      "Training iter #15008:   Batch Loss = 0.483095, Accuracy = 0.875\n",
      "Training iter #15016:   Batch Loss = 0.674360, Accuracy = 0.625\n",
      "Training iter #15024:   Batch Loss = 0.674141, Accuracy = 0.625\n",
      "Training iter #15032:   Batch Loss = 0.605208, Accuracy = 0.75\n",
      "Training iter #15040:   Batch Loss = 0.401704, Accuracy = 0.875\n",
      "Training iter #15048:   Batch Loss = 0.576593, Accuracy = 0.75\n",
      "Training iter #15056:   Batch Loss = 0.389959, Accuracy = 0.875\n",
      "Training iter #15064:   Batch Loss = 0.015574, Accuracy = 1.0\n",
      "Training iter #15072:   Batch Loss = 0.014865, Accuracy = 1.0\n",
      "Training iter #15080:   Batch Loss = 0.014356, Accuracy = 1.0\n",
      "Training iter #15088:   Batch Loss = 1.002918, Accuracy = 0.625\n",
      "Training iter #15096:   Batch Loss = 0.194787, Accuracy = 1.0\n",
      "Training iter #15104:   Batch Loss = 0.575305, Accuracy = 0.75\n",
      "Training iter #15112:   Batch Loss = 0.675072, Accuracy = 0.625\n",
      "Training iter #15120:   Batch Loss = 0.673878, Accuracy = 0.625\n",
      "Training iter #15128:   Batch Loss = 0.591491, Accuracy = 0.75\n",
      "Training iter #15136:   Batch Loss = 0.400626, Accuracy = 0.875\n",
      "Training iter #15144:   Batch Loss = 0.575719, Accuracy = 0.75\n",
      "Training iter #15152:   Batch Loss = 0.574913, Accuracy = 0.75\n",
      "Training iter #15160:   Batch Loss = 0.050225, Accuracy = 1.0\n",
      "Training iter #15168:   Batch Loss = 1.485012, Accuracy = 0.375\n",
      "Training iter #15176:   Batch Loss = 0.436257, Accuracy = 0.875\n",
      "Training iter #15184:   Batch Loss = 0.391154, Accuracy = 0.875\n",
      "Training iter #15192:   Batch Loss = 0.446643, Accuracy = 0.875\n",
      "Training iter #15200:   Batch Loss = 0.574967, Accuracy = 0.75\n",
      "Training iter #15208:   Batch Loss = 0.057909, Accuracy = 1.0\n",
      "Training iter #15216:   Batch Loss = 0.014219, Accuracy = 1.0\n",
      "Training iter #15224:   Batch Loss = 0.950391, Accuracy = 0.625\n",
      "Training iter #15232:   Batch Loss = 0.411912, Accuracy = 0.875\n",
      "Training iter #15240:   Batch Loss = 0.845967, Accuracy = 0.375\n",
      "Training iter #15248:   Batch Loss = 0.721165, Accuracy = 0.375\n",
      "Training iter #15256:   Batch Loss = 0.645260, Accuracy = 0.75\n",
      "Training iter #15264:   Batch Loss = 0.593969, Accuracy = 0.75\n",
      "Training iter #15272:   Batch Loss = 0.191315, Accuracy = 1.0\n",
      "Training iter #15280:   Batch Loss = 0.401868, Accuracy = 0.875\n",
      "Training iter #15288:   Batch Loss = 0.583311, Accuracy = 0.75\n",
      "Training iter #15296:   Batch Loss = 0.675418, Accuracy = 0.625\n",
      "Training iter #15304:   Batch Loss = 0.580469, Accuracy = 0.75\n",
      "Training iter #15312:   Batch Loss = 0.590742, Accuracy = 0.75\n",
      "Training iter #15320:   Batch Loss = 0.397895, Accuracy = 0.875\n",
      "Training iter #15328:   Batch Loss = 0.014376, Accuracy = 1.0\n",
      "Training iter #15336:   Batch Loss = 0.785991, Accuracy = 0.75\n",
      "Training iter #15344:   Batch Loss = 0.399432, Accuracy = 0.875\n",
      "Training iter #15352:   Batch Loss = 0.682230, Accuracy = 0.625\n",
      "Training iter #15360:   Batch Loss = 0.213293, Accuracy = 1.0\n",
      "Training iter #15368:   Batch Loss = 0.736088, Accuracy = 0.625\n",
      "Training iter #15376:   Batch Loss = 0.409886, Accuracy = 0.875\n",
      "Training iter #15384:   Batch Loss = 0.389449, Accuracy = 0.875\n",
      "Training iter #15392:   Batch Loss = 0.398587, Accuracy = 0.875\n",
      "Training iter #15400:   Batch Loss = 0.396595, Accuracy = 0.875\n",
      "Training iter #15408:   Batch Loss = 0.413496, Accuracy = 0.875\n",
      "Training iter #15416:   Batch Loss = 0.075886, Accuracy = 1.0\n",
      "Training iter #15424:   Batch Loss = 0.014410, Accuracy = 1.0\n",
      "Training iter #15432:   Batch Loss = 1.028267, Accuracy = 0.625\n",
      "Training iter #15440:   Batch Loss = 0.689087, Accuracy = 0.625\n",
      "Training iter #15448:   Batch Loss = 0.710225, Accuracy = 0.5\n",
      "Training iter #15456:   Batch Loss = 0.674156, Accuracy = 0.625\n",
      "Training iter #15464:   Batch Loss = 0.393220, Accuracy = 0.875\n",
      "Training iter #15472:   Batch Loss = 0.448585, Accuracy = 0.875\n",
      "Training iter #15480:   Batch Loss = 0.389781, Accuracy = 0.875\n",
      "Training iter #15488:   Batch Loss = 0.471118, Accuracy = 0.875\n",
      "Training iter #15496:   Batch Loss = 0.684431, Accuracy = 0.625\n",
      "Training iter #15504:   Batch Loss = 0.576113, Accuracy = 0.75\n",
      "Training iter #15512:   Batch Loss = 0.048759, Accuracy = 1.0\n",
      "Training iter #15520:   Batch Loss = 0.403404, Accuracy = 0.875\n",
      "Training iter #15528:   Batch Loss = 0.036699, Accuracy = 1.0\n",
      "Training iter #15536:   Batch Loss = 0.391541, Accuracy = 0.875\n",
      "Training iter #15544:   Batch Loss = 0.399659, Accuracy = 0.875\n",
      "Training iter #15552:   Batch Loss = 0.411888, Accuracy = 0.875\n",
      "Training iter #15560:   Batch Loss = 0.076598, Accuracy = 1.0\n",
      "Training iter #15568:   Batch Loss = 0.392645, Accuracy = 0.875\n",
      "Training iter #15576:   Batch Loss = 0.585184, Accuracy = 0.75\n",
      "Training iter #15584:   Batch Loss = 0.042233, Accuracy = 1.0\n",
      "Training iter #15592:   Batch Loss = 0.014084, Accuracy = 1.0\n",
      "Training iter #15600:   Batch Loss = 0.964622, Accuracy = 0.625\n",
      "Training iter #15608:   Batch Loss = 0.575351, Accuracy = 0.75\n",
      "Training iter #15616:   Batch Loss = 0.718842, Accuracy = 0.5\n",
      "Training iter #15624:   Batch Loss = 0.493920, Accuracy = 0.875\n",
      "Training iter #15632:   Batch Loss = 0.014527, Accuracy = 1.0\n",
      "Training iter #15640:   Batch Loss = 0.390146, Accuracy = 0.875\n",
      "Training iter #15648:   Batch Loss = 0.391541, Accuracy = 0.875\n",
      "Training iter #15656:   Batch Loss = 0.577926, Accuracy = 0.75\n",
      "Training iter #15664:   Batch Loss = 0.574853, Accuracy = 0.75\n",
      "Training iter #15672:   Batch Loss = 0.577060, Accuracy = 0.75\n",
      "Training iter #15680:   Batch Loss = 0.394818, Accuracy = 0.875\n",
      "Training iter #15688:   Batch Loss = 0.703440, Accuracy = 0.625\n",
      "Training iter #15696:   Batch Loss = 0.674661, Accuracy = 0.625\n",
      "Training iter #15704:   Batch Loss = 0.576341, Accuracy = 0.75\n",
      "Training iter #15712:   Batch Loss = 0.424780, Accuracy = 0.875\n",
      "Training iter #15720:   Batch Loss = 0.576914, Accuracy = 0.75\n",
      "Training iter #15728:   Batch Loss = 0.397330, Accuracy = 0.875\n",
      "Training iter #15736:   Batch Loss = 0.057789, Accuracy = 1.0\n",
      "Training iter #15744:   Batch Loss = 0.013594, Accuracy = 1.0\n",
      "Training iter #15752:   Batch Loss = 0.013427, Accuracy = 1.0\n",
      "Training iter #15760:   Batch Loss = 1.006488, Accuracy = 0.625\n",
      "Training iter #15768:   Batch Loss = 0.185931, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #15776:   Batch Loss = 0.575049, Accuracy = 0.75\n",
      "Training iter #15784:   Batch Loss = 0.675411, Accuracy = 0.625\n",
      "Training iter #15792:   Batch Loss = 0.673551, Accuracy = 0.625\n",
      "Training iter #15800:   Batch Loss = 0.594835, Accuracy = 0.75\n",
      "Training iter #15808:   Batch Loss = 0.401719, Accuracy = 0.875\n",
      "Training iter #15816:   Batch Loss = 0.575257, Accuracy = 0.75\n",
      "Training iter #15824:   Batch Loss = 0.574504, Accuracy = 0.75\n",
      "Training iter #15832:   Batch Loss = 0.046350, Accuracy = 1.0\n",
      "Training iter #15840:   Batch Loss = 1.482473, Accuracy = 0.375\n",
      "Training iter #15848:   Batch Loss = 0.440046, Accuracy = 0.875\n",
      "Training iter #15856:   Batch Loss = 0.390484, Accuracy = 0.875\n",
      "Training iter #15864:   Batch Loss = 0.439662, Accuracy = 0.875\n",
      "Training iter #15872:   Batch Loss = 0.574653, Accuracy = 0.75\n",
      "Training iter #15880:   Batch Loss = 0.063808, Accuracy = 1.0\n",
      "Training iter #15888:   Batch Loss = 0.013670, Accuracy = 1.0\n",
      "Training iter #15896:   Batch Loss = 0.948032, Accuracy = 0.625\n",
      "Training iter #15904:   Batch Loss = 0.409400, Accuracy = 0.875\n",
      "Training iter #15912:   Batch Loss = 0.847526, Accuracy = 0.375\n",
      "Training iter #15920:   Batch Loss = 0.723341, Accuracy = 0.375\n",
      "Training iter #15928:   Batch Loss = 0.643412, Accuracy = 0.75\n",
      "Training iter #15936:   Batch Loss = 0.593755, Accuracy = 0.75\n",
      "Training iter #15944:   Batch Loss = 0.190471, Accuracy = 1.0\n",
      "Training iter #15952:   Batch Loss = 0.402608, Accuracy = 0.875\n",
      "Training iter #15960:   Batch Loss = 0.582033, Accuracy = 0.75\n",
      "Training iter #15968:   Batch Loss = 0.675111, Accuracy = 0.625\n",
      "Training iter #15976:   Batch Loss = 0.580576, Accuracy = 0.75\n",
      "Training iter #15984:   Batch Loss = 0.588430, Accuracy = 0.75\n",
      "Training iter #15992:   Batch Loss = 0.397538, Accuracy = 0.875\n",
      "Training iter #16000:   Batch Loss = 0.013969, Accuracy = 1.0\n",
      "Training iter #16008:   Batch Loss = 0.809767, Accuracy = 0.75\n",
      "Training iter #16016:   Batch Loss = 0.401847, Accuracy = 0.875\n",
      "Training iter #16024:   Batch Loss = 0.680319, Accuracy = 0.625\n",
      "Training iter #16032:   Batch Loss = 0.215515, Accuracy = 1.0\n",
      "Training iter #16040:   Batch Loss = 0.733392, Accuracy = 0.625\n",
      "Training iter #16048:   Batch Loss = 0.410789, Accuracy = 0.875\n",
      "Training iter #16056:   Batch Loss = 0.389332, Accuracy = 0.875\n",
      "Training iter #16064:   Batch Loss = 0.443799, Accuracy = 0.875\n",
      "Training iter #16072:   Batch Loss = 0.391129, Accuracy = 0.875\n",
      "Training iter #16080:   Batch Loss = 0.444771, Accuracy = 0.875\n",
      "Training iter #16088:   Batch Loss = 0.074554, Accuracy = 1.0\n",
      "Training iter #16096:   Batch Loss = 0.014094, Accuracy = 1.0\n",
      "Training iter #16104:   Batch Loss = 1.008067, Accuracy = 0.625\n",
      "Training iter #16112:   Batch Loss = 0.689434, Accuracy = 0.625\n",
      "Training iter #16120:   Batch Loss = 0.712204, Accuracy = 0.5\n",
      "Training iter #16128:   Batch Loss = 0.673844, Accuracy = 0.625\n",
      "Training iter #16136:   Batch Loss = 0.391300, Accuracy = 0.875\n",
      "Training iter #16144:   Batch Loss = 0.451579, Accuracy = 0.875\n",
      "Training iter #16152:   Batch Loss = 0.389207, Accuracy = 0.875\n",
      "Training iter #16160:   Batch Loss = 0.468406, Accuracy = 0.875\n",
      "Training iter #16168:   Batch Loss = 0.684691, Accuracy = 0.625\n",
      "Training iter #16176:   Batch Loss = 0.575596, Accuracy = 0.75\n",
      "Training iter #16184:   Batch Loss = 0.052651, Accuracy = 1.0\n",
      "Training iter #16192:   Batch Loss = 0.400410, Accuracy = 0.875\n",
      "Training iter #16200:   Batch Loss = 0.031483, Accuracy = 1.0\n",
      "Training iter #16208:   Batch Loss = 0.390216, Accuracy = 0.875\n",
      "Training iter #16216:   Batch Loss = 0.391106, Accuracy = 0.875\n",
      "Training iter #16224:   Batch Loss = 0.473808, Accuracy = 0.875\n",
      "Training iter #16232:   Batch Loss = 0.084202, Accuracy = 1.0\n",
      "Training iter #16240:   Batch Loss = 0.389206, Accuracy = 0.875\n",
      "Training iter #16248:   Batch Loss = 0.578560, Accuracy = 0.75\n",
      "Training iter #16256:   Batch Loss = 0.063847, Accuracy = 1.0\n",
      "Training iter #16264:   Batch Loss = 0.013680, Accuracy = 1.0\n",
      "Training iter #16272:   Batch Loss = 0.978511, Accuracy = 0.625\n",
      "Training iter #16280:   Batch Loss = 0.574387, Accuracy = 0.75\n",
      "Training iter #16288:   Batch Loss = 0.721698, Accuracy = 0.5\n",
      "Training iter #16296:   Batch Loss = 0.492554, Accuracy = 0.875\n",
      "Training iter #16304:   Batch Loss = 0.014066, Accuracy = 1.0\n",
      "Training iter #16312:   Batch Loss = 0.389995, Accuracy = 0.875\n",
      "Training iter #16320:   Batch Loss = 0.390202, Accuracy = 0.875\n",
      "Training iter #16328:   Batch Loss = 0.579230, Accuracy = 0.75\n",
      "Training iter #16336:   Batch Loss = 0.574308, Accuracy = 0.75\n",
      "Training iter #16344:   Batch Loss = 0.585995, Accuracy = 0.75\n",
      "Training iter #16352:   Batch Loss = 0.402066, Accuracy = 0.875\n",
      "Training iter #16360:   Batch Loss = 0.707799, Accuracy = 0.625\n",
      "Training iter #16368:   Batch Loss = 0.676514, Accuracy = 0.625\n",
      "Training iter #16376:   Batch Loss = 0.574323, Accuracy = 0.75\n",
      "Training iter #16384:   Batch Loss = 0.457957, Accuracy = 0.875\n",
      "Training iter #16392:   Batch Loss = 0.574367, Accuracy = 0.75\n",
      "Training iter #16400:   Batch Loss = 0.433466, Accuracy = 0.875\n",
      "Training iter #16408:   Batch Loss = 0.081412, Accuracy = 1.0\n",
      "Training iter #16416:   Batch Loss = 0.013787, Accuracy = 1.0\n",
      "Training iter #16424:   Batch Loss = 0.013321, Accuracy = 1.0\n",
      "Training iter #16432:   Batch Loss = 0.977236, Accuracy = 0.625\n",
      "Training iter #16440:   Batch Loss = 0.183468, Accuracy = 1.0\n",
      "Training iter #16448:   Batch Loss = 0.574677, Accuracy = 0.75\n",
      "Training iter #16456:   Batch Loss = 0.674807, Accuracy = 0.625\n",
      "Training iter #16464:   Batch Loss = 0.672988, Accuracy = 0.625\n",
      "Training iter #16472:   Batch Loss = 0.591254, Accuracy = 0.75\n",
      "Training iter #16480:   Batch Loss = 0.399941, Accuracy = 0.875\n",
      "Training iter #16488:   Batch Loss = 0.574435, Accuracy = 0.75\n",
      "Training iter #16496:   Batch Loss = 0.573960, Accuracy = 0.75\n",
      "Training iter #16504:   Batch Loss = 0.048482, Accuracy = 1.0\n",
      "Training iter #16512:   Batch Loss = 1.481912, Accuracy = 0.375\n",
      "Training iter #16520:   Batch Loss = 0.434464, Accuracy = 0.875\n",
      "Training iter #16528:   Batch Loss = 0.390106, Accuracy = 0.875\n",
      "Training iter #16536:   Batch Loss = 0.435663, Accuracy = 0.875\n",
      "Training iter #16544:   Batch Loss = 0.574300, Accuracy = 0.75\n",
      "Training iter #16552:   Batch Loss = 0.069443, Accuracy = 1.0\n",
      "Training iter #16560:   Batch Loss = 0.013171, Accuracy = 1.0\n",
      "Training iter #16568:   Batch Loss = 0.944941, Accuracy = 0.625\n",
      "Training iter #16576:   Batch Loss = 0.406997, Accuracy = 0.875\n",
      "Training iter #16584:   Batch Loss = 0.840543, Accuracy = 0.375\n",
      "Training iter #16592:   Batch Loss = 0.710663, Accuracy = 0.375\n",
      "Training iter #16600:   Batch Loss = 0.656912, Accuracy = 0.75\n",
      "Training iter #16608:   Batch Loss = 0.597341, Accuracy = 0.75\n",
      "Training iter #16616:   Batch Loss = 0.178540, Accuracy = 1.0\n",
      "Training iter #16624:   Batch Loss = 0.398503, Accuracy = 0.875\n",
      "Training iter #16632:   Batch Loss = 0.582245, Accuracy = 0.75\n",
      "Training iter #16640:   Batch Loss = 0.675265, Accuracy = 0.625\n",
      "Training iter #16648:   Batch Loss = 0.581844, Accuracy = 0.75\n",
      "Training iter #16656:   Batch Loss = 0.584572, Accuracy = 0.75\n",
      "Training iter #16664:   Batch Loss = 0.394764, Accuracy = 0.875\n",
      "Training iter #16672:   Batch Loss = 0.013405, Accuracy = 1.0\n",
      "Training iter #16680:   Batch Loss = 0.833618, Accuracy = 0.75\n",
      "Training iter #16688:   Batch Loss = 0.404128, Accuracy = 0.875\n",
      "Training iter #16696:   Batch Loss = 0.679697, Accuracy = 0.625\n",
      "Training iter #16704:   Batch Loss = 0.218241, Accuracy = 1.0\n",
      "Training iter #16712:   Batch Loss = 0.731985, Accuracy = 0.625\n",
      "Training iter #16720:   Batch Loss = 0.410496, Accuracy = 0.875\n",
      "Training iter #16728:   Batch Loss = 0.388772, Accuracy = 0.875\n",
      "Training iter #16736:   Batch Loss = 0.436949, Accuracy = 0.875\n",
      "Training iter #16744:   Batch Loss = 0.391006, Accuracy = 0.875\n",
      "Training iter #16752:   Batch Loss = 0.438880, Accuracy = 0.875\n",
      "Training iter #16760:   Batch Loss = 0.065598, Accuracy = 1.0\n",
      "Training iter #16768:   Batch Loss = 0.013649, Accuracy = 1.0\n",
      "Training iter #16776:   Batch Loss = 0.993482, Accuracy = 0.625\n",
      "Training iter #16784:   Batch Loss = 0.686041, Accuracy = 0.625\n",
      "Training iter #16792:   Batch Loss = 0.711416, Accuracy = 0.5\n",
      "Training iter #16800:   Batch Loss = 0.673665, Accuracy = 0.625\n",
      "Training iter #16808:   Batch Loss = 0.390891, Accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #16816:   Batch Loss = 0.451773, Accuracy = 0.875\n",
      "Training iter #16824:   Batch Loss = 0.388702, Accuracy = 0.875\n",
      "Training iter #16832:   Batch Loss = 0.556557, Accuracy = 0.875\n",
      "Training iter #16840:   Batch Loss = 0.676920, Accuracy = 0.625\n",
      "Training iter #16848:   Batch Loss = 0.573792, Accuracy = 0.75\n",
      "Training iter #16856:   Batch Loss = 0.045760, Accuracy = 1.0\n",
      "Training iter #16864:   Batch Loss = 0.404634, Accuracy = 0.875\n",
      "Training iter #16872:   Batch Loss = 0.040967, Accuracy = 1.0\n",
      "Training iter #16880:   Batch Loss = 0.390832, Accuracy = 0.875\n",
      "Training iter #16888:   Batch Loss = 0.411308, Accuracy = 0.875\n",
      "Training iter #16896:   Batch Loss = 0.395434, Accuracy = 0.875\n",
      "Training iter #16904:   Batch Loss = 0.047542, Accuracy = 1.0\n",
      "Training iter #16912:   Batch Loss = 0.399437, Accuracy = 0.875\n",
      "Training iter #16920:   Batch Loss = 0.587587, Accuracy = 0.75\n",
      "Training iter #16928:   Batch Loss = 0.019255, Accuracy = 1.0\n",
      "Training iter #16936:   Batch Loss = 0.012891, Accuracy = 1.0\n",
      "Training iter #16944:   Batch Loss = 0.912430, Accuracy = 0.625\n",
      "Training iter #16952:   Batch Loss = 0.574432, Accuracy = 0.75\n",
      "Training iter #16960:   Batch Loss = 0.715801, Accuracy = 0.5\n",
      "Training iter #16968:   Batch Loss = 0.493841, Accuracy = 0.875\n",
      "Training iter #16976:   Batch Loss = 0.013259, Accuracy = 1.0\n",
      "Training iter #16984:   Batch Loss = 0.388839, Accuracy = 0.875\n",
      "Training iter #16992:   Batch Loss = 0.389562, Accuracy = 0.875\n",
      "Training iter #17000:   Batch Loss = 0.577074, Accuracy = 0.75\n",
      "Training iter #17008:   Batch Loss = 0.573871, Accuracy = 0.75\n",
      "Training iter #17016:   Batch Loss = 0.576388, Accuracy = 0.75\n",
      "Training iter #17024:   Batch Loss = 0.392166, Accuracy = 0.875\n",
      "Training iter #17032:   Batch Loss = 0.696290, Accuracy = 0.625\n",
      "Training iter #17040:   Batch Loss = 0.674094, Accuracy = 0.625\n",
      "Training iter #17048:   Batch Loss = 0.574449, Accuracy = 0.75\n",
      "Training iter #17056:   Batch Loss = 0.444198, Accuracy = 0.875\n",
      "Training iter #17064:   Batch Loss = 0.574669, Accuracy = 0.75\n",
      "Training iter #17072:   Batch Loss = 0.417719, Accuracy = 0.875\n",
      "Training iter #17080:   Batch Loss = 0.080736, Accuracy = 1.0\n",
      "Training iter #17088:   Batch Loss = 0.012995, Accuracy = 1.0\n",
      "Training iter #17096:   Batch Loss = 0.012666, Accuracy = 1.0\n",
      "Training iter #17104:   Batch Loss = 0.977117, Accuracy = 0.625\n",
      "Training iter #17112:   Batch Loss = 0.185981, Accuracy = 1.0\n",
      "Training iter #17120:   Batch Loss = 0.573943, Accuracy = 0.75\n",
      "Training iter #17128:   Batch Loss = 0.674071, Accuracy = 0.625\n",
      "Training iter #17136:   Batch Loss = 0.672589, Accuracy = 0.625\n",
      "Training iter #17144:   Batch Loss = 0.595153, Accuracy = 0.75\n",
      "Training iter #17152:   Batch Loss = 0.399906, Accuracy = 0.875\n",
      "Training iter #17160:   Batch Loss = 0.574548, Accuracy = 0.75\n",
      "Training iter #17168:   Batch Loss = 0.573548, Accuracy = 0.75\n",
      "Training iter #17176:   Batch Loss = 0.043071, Accuracy = 1.0\n",
      "Training iter #17184:   Batch Loss = 1.478995, Accuracy = 0.375\n",
      "Training iter #17192:   Batch Loss = 0.439492, Accuracy = 0.875\n",
      "Training iter #17200:   Batch Loss = 0.389839, Accuracy = 0.875\n",
      "Training iter #17208:   Batch Loss = 0.450354, Accuracy = 0.875\n",
      "Training iter #17216:   Batch Loss = 0.573667, Accuracy = 0.75\n",
      "Training iter #17224:   Batch Loss = 0.054504, Accuracy = 1.0\n",
      "Training iter #17232:   Batch Loss = 0.012768, Accuracy = 1.0\n",
      "Training iter #17240:   Batch Loss = 0.951741, Accuracy = 0.625\n",
      "Training iter #17248:   Batch Loss = 0.410844, Accuracy = 0.875\n",
      "Training iter #17256:   Batch Loss = 0.842534, Accuracy = 0.375\n",
      "Training iter #17264:   Batch Loss = 0.721260, Accuracy = 0.375\n",
      "Training iter #17272:   Batch Loss = 0.642618, Accuracy = 0.75\n",
      "Training iter #17280:   Batch Loss = 0.593225, Accuracy = 0.75\n",
      "Training iter #17288:   Batch Loss = 0.192358, Accuracy = 1.0\n",
      "Training iter #17296:   Batch Loss = 0.402677, Accuracy = 0.875\n",
      "Training iter #17304:   Batch Loss = 0.582116, Accuracy = 0.75\n",
      "Training iter #17312:   Batch Loss = 0.674256, Accuracy = 0.625\n",
      "Training iter #17320:   Batch Loss = 0.579891, Accuracy = 0.75\n",
      "Training iter #17328:   Batch Loss = 0.588005, Accuracy = 0.75\n",
      "Training iter #17336:   Batch Loss = 0.396608, Accuracy = 0.875\n",
      "Training iter #17344:   Batch Loss = 0.013055, Accuracy = 1.0\n",
      "Training iter #17352:   Batch Loss = 0.796851, Accuracy = 0.75\n",
      "Training iter #17360:   Batch Loss = 0.399767, Accuracy = 0.875\n",
      "Training iter #17368:   Batch Loss = 0.679312, Accuracy = 0.625\n",
      "Training iter #17376:   Batch Loss = 0.212716, Accuracy = 1.0\n",
      "Training iter #17384:   Batch Loss = 0.733682, Accuracy = 0.625\n",
      "Training iter #17392:   Batch Loss = 0.409092, Accuracy = 0.875\n",
      "Training iter #17400:   Batch Loss = 0.388212, Accuracy = 0.875\n",
      "Training iter #17408:   Batch Loss = 0.412247, Accuracy = 0.875\n",
      "Training iter #17416:   Batch Loss = 0.391303, Accuracy = 0.875\n",
      "Training iter #17424:   Batch Loss = 0.442601, Accuracy = 0.875\n",
      "Training iter #17432:   Batch Loss = 0.073966, Accuracy = 1.0\n",
      "Training iter #17440:   Batch Loss = 0.013217, Accuracy = 1.0\n",
      "Training iter #17448:   Batch Loss = 1.004262, Accuracy = 0.625\n",
      "Training iter #17456:   Batch Loss = 0.687094, Accuracy = 0.625\n",
      "Training iter #17464:   Batch Loss = 0.709763, Accuracy = 0.5\n",
      "Training iter #17472:   Batch Loss = 0.672903, Accuracy = 0.625\n",
      "Training iter #17480:   Batch Loss = 0.390520, Accuracy = 0.875\n",
      "Training iter #17488:   Batch Loss = 0.453104, Accuracy = 0.875\n",
      "Training iter #17496:   Batch Loss = 0.389833, Accuracy = 0.875\n",
      "Training iter #17504:   Batch Loss = 0.449977, Accuracy = 0.875\n",
      "Training iter #17512:   Batch Loss = 0.679786, Accuracy = 0.625\n",
      "Training iter #17520:   Batch Loss = 0.574645, Accuracy = 0.75\n",
      "Training iter #17528:   Batch Loss = 0.045970, Accuracy = 1.0\n",
      "Training iter #17536:   Batch Loss = 0.401119, Accuracy = 0.875\n",
      "Training iter #17544:   Batch Loss = 0.034765, Accuracy = 1.0\n",
      "Training iter #17552:   Batch Loss = 0.389912, Accuracy = 0.875\n",
      "Training iter #17560:   Batch Loss = 0.393748, Accuracy = 0.875\n",
      "Training iter #17568:   Batch Loss = 0.431040, Accuracy = 0.875\n",
      "Training iter #17576:   Batch Loss = 0.082975, Accuracy = 1.0\n",
      "Training iter #17584:   Batch Loss = 0.389226, Accuracy = 0.875\n",
      "Training iter #17592:   Batch Loss = 0.579091, Accuracy = 0.75\n",
      "Training iter #17600:   Batch Loss = 0.053163, Accuracy = 1.0\n",
      "Training iter #17608:   Batch Loss = 0.013229, Accuracy = 1.0\n",
      "Training iter #17616:   Batch Loss = 0.949989, Accuracy = 0.625\n",
      "Training iter #17624:   Batch Loss = 0.574007, Accuracy = 0.75\n",
      "Training iter #17632:   Batch Loss = 0.718983, Accuracy = 0.5\n",
      "Training iter #17640:   Batch Loss = 0.491956, Accuracy = 0.875\n",
      "Training iter #17648:   Batch Loss = 0.013469, Accuracy = 1.0\n",
      "Training iter #17656:   Batch Loss = 0.388819, Accuracy = 0.875\n",
      "Training iter #17664:   Batch Loss = 0.388981, Accuracy = 0.875\n",
      "Training iter #17672:   Batch Loss = 0.580117, Accuracy = 0.75\n",
      "Training iter #17680:   Batch Loss = 0.573568, Accuracy = 0.75\n",
      "Training iter #17688:   Batch Loss = 0.594759, Accuracy = 0.75\n",
      "Training iter #17696:   Batch Loss = 0.397626, Accuracy = 0.875\n",
      "Training iter #17704:   Batch Loss = 0.702931, Accuracy = 0.625\n",
      "Training iter #17712:   Batch Loss = 0.675771, Accuracy = 0.625\n",
      "Training iter #17720:   Batch Loss = 0.573581, Accuracy = 0.75\n",
      "Training iter #17728:   Batch Loss = 0.468836, Accuracy = 0.875\n",
      "Training iter #17736:   Batch Loss = 0.573566, Accuracy = 0.75\n",
      "Training iter #17744:   Batch Loss = 0.435819, Accuracy = 0.875\n",
      "Training iter #17752:   Batch Loss = 0.085509, Accuracy = 1.0\n",
      "Training iter #17760:   Batch Loss = 0.013268, Accuracy = 1.0\n",
      "Training iter #17768:   Batch Loss = 0.012703, Accuracy = 1.0\n",
      "Training iter #17776:   Batch Loss = 0.982888, Accuracy = 0.625\n",
      "Training iter #17784:   Batch Loss = 0.176967, Accuracy = 1.0\n",
      "Training iter #17792:   Batch Loss = 0.574175, Accuracy = 0.75\n",
      "Training iter #17800:   Batch Loss = 0.673937, Accuracy = 0.625\n",
      "Training iter #17808:   Batch Loss = 0.672198, Accuracy = 0.625\n",
      "Training iter #17816:   Batch Loss = 0.590656, Accuracy = 0.75\n",
      "Training iter #17824:   Batch Loss = 0.400030, Accuracy = 0.875\n",
      "Training iter #17832:   Batch Loss = 0.573451, Accuracy = 0.75\n",
      "Training iter #17840:   Batch Loss = 0.573180, Accuracy = 0.75\n",
      "Training iter #17848:   Batch Loss = 0.046462, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #17856:   Batch Loss = 1.486628, Accuracy = 0.375\n",
      "Training iter #17864:   Batch Loss = 0.436016, Accuracy = 0.875\n",
      "Training iter #17872:   Batch Loss = 0.389135, Accuracy = 0.875\n",
      "Training iter #17880:   Batch Loss = 0.433600, Accuracy = 0.875\n",
      "Training iter #17888:   Batch Loss = 0.573583, Accuracy = 0.75\n",
      "Training iter #17896:   Batch Loss = 0.070420, Accuracy = 1.0\n",
      "Training iter #17904:   Batch Loss = 0.012486, Accuracy = 1.0\n",
      "Training iter #17912:   Batch Loss = 0.936817, Accuracy = 0.625\n",
      "Training iter #17920:   Batch Loss = 0.406286, Accuracy = 0.875\n",
      "Training iter #17928:   Batch Loss = 0.846688, Accuracy = 0.375\n",
      "Training iter #17936:   Batch Loss = 0.722560, Accuracy = 0.375\n",
      "Training iter #17944:   Batch Loss = 0.641854, Accuracy = 0.75\n",
      "Training iter #17952:   Batch Loss = 0.592888, Accuracy = 0.75\n",
      "Training iter #17960:   Batch Loss = 0.188972, Accuracy = 1.0\n",
      "Training iter #17968:   Batch Loss = 0.401082, Accuracy = 0.875\n",
      "Training iter #17976:   Batch Loss = 0.581001, Accuracy = 0.75\n",
      "Training iter #17984:   Batch Loss = 0.673863, Accuracy = 0.625\n",
      "Training iter #17992:   Batch Loss = 0.579096, Accuracy = 0.75\n",
      "Training iter #18000:   Batch Loss = 0.588661, Accuracy = 0.75\n",
      "Training iter #18008:   Batch Loss = 0.397700, Accuracy = 0.875\n",
      "Training iter #18016:   Batch Loss = 0.013263, Accuracy = 1.0\n",
      "Training iter #18024:   Batch Loss = 0.779857, Accuracy = 0.75\n",
      "Training iter #18032:   Batch Loss = 0.399110, Accuracy = 0.875\n",
      "Training iter #18040:   Batch Loss = 0.679120, Accuracy = 0.625\n",
      "Training iter #18048:   Batch Loss = 0.209792, Accuracy = 1.0\n",
      "Training iter #18056:   Batch Loss = 0.734708, Accuracy = 0.625\n",
      "Training iter #18064:   Batch Loss = 0.408496, Accuracy = 0.875\n",
      "Training iter #18072:   Batch Loss = 0.388031, Accuracy = 0.875\n",
      "Training iter #18080:   Batch Loss = 0.398536, Accuracy = 0.875\n",
      "Training iter #18088:   Batch Loss = 0.394690, Accuracy = 0.875\n",
      "Training iter #18096:   Batch Loss = 0.415343, Accuracy = 0.875\n",
      "Training iter #18104:   Batch Loss = 0.070907, Accuracy = 1.0\n",
      "Training iter #18112:   Batch Loss = 0.012785, Accuracy = 1.0\n",
      "Training iter #18120:   Batch Loss = 1.008610, Accuracy = 0.625\n",
      "Training iter #18128:   Batch Loss = 0.684479, Accuracy = 0.625\n",
      "Training iter #18136:   Batch Loss = 0.704574, Accuracy = 0.5\n",
      "Training iter #18144:   Batch Loss = 0.675538, Accuracy = 0.625\n",
      "Training iter #18152:   Batch Loss = 0.421516, Accuracy = 0.875\n",
      "Training iter #18160:   Batch Loss = 0.446252, Accuracy = 0.875\n",
      "Training iter #18168:   Batch Loss = 0.389195, Accuracy = 0.875\n",
      "Training iter #18176:   Batch Loss = 0.456242, Accuracy = 0.875\n",
      "Training iter #18184:   Batch Loss = 0.682558, Accuracy = 0.625\n",
      "Training iter #18192:   Batch Loss = 0.573974, Accuracy = 0.75\n",
      "Training iter #18200:   Batch Loss = 0.056408, Accuracy = 1.0\n",
      "Training iter #18208:   Batch Loss = 0.395487, Accuracy = 0.875\n",
      "Training iter #18216:   Batch Loss = 0.027897, Accuracy = 1.0\n",
      "Training iter #18224:   Batch Loss = 0.388964, Accuracy = 0.875\n",
      "Training iter #18232:   Batch Loss = 0.388801, Accuracy = 0.875\n",
      "Training iter #18240:   Batch Loss = 0.480769, Accuracy = 0.875\n",
      "Training iter #18248:   Batch Loss = 0.074579, Accuracy = 1.0\n",
      "Training iter #18256:   Batch Loss = 0.388185, Accuracy = 0.875\n",
      "Training iter #18264:   Batch Loss = 0.577097, Accuracy = 0.75\n",
      "Training iter #18272:   Batch Loss = 0.059512, Accuracy = 1.0\n",
      "Training iter #18280:   Batch Loss = 0.012739, Accuracy = 1.0\n",
      "Training iter #18288:   Batch Loss = 0.980994, Accuracy = 0.625\n",
      "Training iter #18296:   Batch Loss = 0.573383, Accuracy = 0.75\n",
      "Training iter #18304:   Batch Loss = 0.719593, Accuracy = 0.5\n",
      "Training iter #18312:   Batch Loss = 0.491029, Accuracy = 0.875\n",
      "Training iter #18320:   Batch Loss = 0.013353, Accuracy = 1.0\n",
      "Training iter #18328:   Batch Loss = 0.388179, Accuracy = 0.875\n",
      "Training iter #18336:   Batch Loss = 0.388732, Accuracy = 0.875\n",
      "Training iter #18344:   Batch Loss = 0.579325, Accuracy = 0.75\n",
      "Training iter #18352:   Batch Loss = 0.573085, Accuracy = 0.75\n",
      "Training iter #18360:   Batch Loss = 0.581656, Accuracy = 0.75\n",
      "Training iter #18368:   Batch Loss = 0.400468, Accuracy = 0.875\n",
      "Training iter #18376:   Batch Loss = 0.704187, Accuracy = 0.625\n",
      "Training iter #18384:   Batch Loss = 0.674180, Accuracy = 0.625\n",
      "Training iter #18392:   Batch Loss = 0.573312, Accuracy = 0.75\n",
      "Training iter #18400:   Batch Loss = 0.451939, Accuracy = 0.875\n",
      "Training iter #18408:   Batch Loss = 0.573489, Accuracy = 0.75\n",
      "Training iter #18416:   Batch Loss = 0.425702, Accuracy = 0.875\n",
      "Training iter #18424:   Batch Loss = 0.081162, Accuracy = 1.0\n",
      "Training iter #18432:   Batch Loss = 0.012799, Accuracy = 1.0\n",
      "Training iter #18440:   Batch Loss = 0.012268, Accuracy = 1.0\n",
      "Training iter #18448:   Batch Loss = 1.000327, Accuracy = 0.625\n",
      "Training iter #18456:   Batch Loss = 0.178105, Accuracy = 1.0\n",
      "Training iter #18464:   Batch Loss = 0.573573, Accuracy = 0.75\n",
      "Training iter #18472:   Batch Loss = 0.673027, Accuracy = 0.625\n",
      "Training iter #18480:   Batch Loss = 0.671896, Accuracy = 0.625\n",
      "Training iter #18488:   Batch Loss = 0.593947, Accuracy = 0.75\n",
      "Training iter #18496:   Batch Loss = 0.398769, Accuracy = 0.875\n",
      "Training iter #18504:   Batch Loss = 0.573569, Accuracy = 0.75\n",
      "Training iter #18512:   Batch Loss = 0.572905, Accuracy = 0.75\n",
      "Training iter #18520:   Batch Loss = 0.043577, Accuracy = 1.0\n",
      "Training iter #18528:   Batch Loss = 1.475240, Accuracy = 0.375\n",
      "Training iter #18536:   Batch Loss = 0.438374, Accuracy = 0.875\n",
      "Training iter #18544:   Batch Loss = 0.389179, Accuracy = 0.875\n",
      "Training iter #18552:   Batch Loss = 0.446119, Accuracy = 0.875\n",
      "Training iter #18560:   Batch Loss = 0.573039, Accuracy = 0.75\n",
      "Training iter #18568:   Batch Loss = 0.057462, Accuracy = 1.0\n",
      "Training iter #18576:   Batch Loss = 0.012180, Accuracy = 1.0\n",
      "Training iter #18584:   Batch Loss = 0.943740, Accuracy = 0.625\n",
      "Training iter #18592:   Batch Loss = 0.406996, Accuracy = 0.875\n",
      "Training iter #18600:   Batch Loss = 0.839212, Accuracy = 0.375\n",
      "Training iter #18608:   Batch Loss = 0.717359, Accuracy = 0.375\n",
      "Training iter #18616:   Batch Loss = 0.643668, Accuracy = 0.75\n",
      "Training iter #18624:   Batch Loss = 0.591749, Accuracy = 0.75\n",
      "Training iter #18632:   Batch Loss = 0.195159, Accuracy = 1.0\n",
      "Training iter #18640:   Batch Loss = 0.402958, Accuracy = 0.875\n",
      "Training iter #18648:   Batch Loss = 0.580527, Accuracy = 0.75\n",
      "Training iter #18656:   Batch Loss = 0.673912, Accuracy = 0.625\n",
      "Training iter #18664:   Batch Loss = 0.581267, Accuracy = 0.75\n",
      "Training iter #18672:   Batch Loss = 0.583299, Accuracy = 0.75\n",
      "Training iter #18680:   Batch Loss = 0.395241, Accuracy = 0.875\n",
      "Training iter #18688:   Batch Loss = 0.012378, Accuracy = 1.0\n",
      "Training iter #18696:   Batch Loss = 0.787687, Accuracy = 0.75\n",
      "Training iter #18704:   Batch Loss = 0.396378, Accuracy = 0.875\n",
      "Training iter #18712:   Batch Loss = 0.679848, Accuracy = 0.625\n",
      "Training iter #18720:   Batch Loss = 0.210189, Accuracy = 1.0\n",
      "Training iter #18728:   Batch Loss = 0.735898, Accuracy = 0.625\n",
      "Training iter #18736:   Batch Loss = 0.407476, Accuracy = 0.875\n",
      "Training iter #18744:   Batch Loss = 0.387442, Accuracy = 0.875\n",
      "Training iter #18752:   Batch Loss = 0.389330, Accuracy = 0.875\n",
      "Training iter #18760:   Batch Loss = 0.403805, Accuracy = 0.875\n",
      "Training iter #18768:   Batch Loss = 0.399096, Accuracy = 0.875\n",
      "Training iter #18776:   Batch Loss = 0.064385, Accuracy = 1.0\n",
      "Training iter #18784:   Batch Loss = 0.012202, Accuracy = 1.0\n",
      "Training iter #18792:   Batch Loss = 1.025322, Accuracy = 0.625\n",
      "Training iter #18800:   Batch Loss = 0.685855, Accuracy = 0.625\n",
      "Training iter #18808:   Batch Loss = 0.706400, Accuracy = 0.5\n",
      "Training iter #18816:   Batch Loss = 0.672377, Accuracy = 0.625\n",
      "Training iter #18824:   Batch Loss = 0.391375, Accuracy = 0.875\n",
      "Training iter #18832:   Batch Loss = 0.451662, Accuracy = 0.875\n",
      "Training iter #18840:   Batch Loss = 0.387627, Accuracy = 0.875\n",
      "Training iter #18848:   Batch Loss = 0.452450, Accuracy = 0.875\n",
      "Training iter #18856:   Batch Loss = 0.690525, Accuracy = 0.625\n",
      "Training iter #18864:   Batch Loss = 0.573305, Accuracy = 0.75\n",
      "Training iter #18872:   Batch Loss = 0.055117, Accuracy = 1.0\n",
      "Training iter #18880:   Batch Loss = 0.393220, Accuracy = 0.875\n",
      "Training iter #18888:   Batch Loss = 0.022671, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #18896:   Batch Loss = 0.387535, Accuracy = 0.875\n",
      "Training iter #18904:   Batch Loss = 0.387435, Accuracy = 0.875\n",
      "Training iter #18912:   Batch Loss = 0.466161, Accuracy = 0.875\n",
      "Training iter #18920:   Batch Loss = 0.079182, Accuracy = 1.0\n",
      "Training iter #18928:   Batch Loss = 0.387761, Accuracy = 0.875\n",
      "Training iter #18936:   Batch Loss = 0.578690, Accuracy = 0.75\n",
      "Training iter #18944:   Batch Loss = 0.052874, Accuracy = 1.0\n",
      "Training iter #18952:   Batch Loss = 0.012245, Accuracy = 1.0\n",
      "Training iter #18960:   Batch Loss = 0.965041, Accuracy = 0.625\n",
      "Training iter #18968:   Batch Loss = 0.572971, Accuracy = 0.75\n",
      "Training iter #18976:   Batch Loss = 0.717967, Accuracy = 0.5\n",
      "Training iter #18984:   Batch Loss = 0.490276, Accuracy = 0.875\n",
      "Training iter #18992:   Batch Loss = 0.012451, Accuracy = 1.0\n",
      "Training iter #19000:   Batch Loss = 0.388126, Accuracy = 0.875\n",
      "Training iter #19008:   Batch Loss = 0.389542, Accuracy = 0.875\n",
      "Training iter #19016:   Batch Loss = 0.575778, Accuracy = 0.75\n",
      "Training iter #19024:   Batch Loss = 0.572677, Accuracy = 0.75\n",
      "Training iter #19032:   Batch Loss = 0.576078, Accuracy = 0.75\n",
      "Training iter #19040:   Batch Loss = 0.394785, Accuracy = 0.875\n",
      "Training iter #19048:   Batch Loss = 0.701772, Accuracy = 0.625\n",
      "Training iter #19056:   Batch Loss = 0.673607, Accuracy = 0.625\n",
      "Training iter #19064:   Batch Loss = 0.572991, Accuracy = 0.75\n",
      "Training iter #19072:   Batch Loss = 0.452562, Accuracy = 0.875\n",
      "Training iter #19080:   Batch Loss = 0.573213, Accuracy = 0.75\n",
      "Training iter #19088:   Batch Loss = 0.427418, Accuracy = 0.875\n",
      "Training iter #19096:   Batch Loss = 0.078877, Accuracy = 1.0\n",
      "Training iter #19104:   Batch Loss = 0.012282, Accuracy = 1.0\n",
      "Training iter #19112:   Batch Loss = 0.011801, Accuracy = 1.0\n",
      "Training iter #19120:   Batch Loss = 0.982219, Accuracy = 0.625\n",
      "Training iter #19128:   Batch Loss = 0.182364, Accuracy = 1.0\n",
      "Training iter #19136:   Batch Loss = 0.572887, Accuracy = 0.75\n",
      "Training iter #19144:   Batch Loss = 0.672446, Accuracy = 0.625\n",
      "Training iter #19152:   Batch Loss = 0.671466, Accuracy = 0.625\n",
      "Training iter #19160:   Batch Loss = 0.597150, Accuracy = 0.75\n",
      "Training iter #19168:   Batch Loss = 0.398993, Accuracy = 0.875\n",
      "Training iter #19176:   Batch Loss = 0.573681, Accuracy = 0.75\n",
      "Training iter #19184:   Batch Loss = 0.572485, Accuracy = 0.75\n",
      "Training iter #19192:   Batch Loss = 0.041725, Accuracy = 1.0\n",
      "Training iter #19200:   Batch Loss = 1.451612, Accuracy = 0.375\n",
      "Training iter #19208:   Batch Loss = 0.437903, Accuracy = 0.875\n",
      "Training iter #19216:   Batch Loss = 0.388971, Accuracy = 0.875\n",
      "Training iter #19224:   Batch Loss = 0.452755, Accuracy = 0.875\n",
      "Training iter #19232:   Batch Loss = 0.572580, Accuracy = 0.75\n",
      "Training iter #19240:   Batch Loss = 0.052701, Accuracy = 1.0\n",
      "Training iter #19248:   Batch Loss = 0.011708, Accuracy = 1.0\n",
      "Training iter #19256:   Batch Loss = 0.945462, Accuracy = 0.625\n",
      "Training iter #19264:   Batch Loss = 0.407684, Accuracy = 0.875\n",
      "Training iter #19272:   Batch Loss = 0.835844, Accuracy = 0.375\n",
      "Training iter #19280:   Batch Loss = 0.713319, Accuracy = 0.375\n",
      "Training iter #19288:   Batch Loss = 0.645204, Accuracy = 0.75\n",
      "Training iter #19296:   Batch Loss = 0.589260, Accuracy = 0.75\n",
      "Training iter #19304:   Batch Loss = 0.196308, Accuracy = 1.0\n",
      "Training iter #19312:   Batch Loss = 0.402393, Accuracy = 0.875\n",
      "Training iter #19320:   Batch Loss = 0.582232, Accuracy = 0.75\n",
      "Training iter #19328:   Batch Loss = 0.673937, Accuracy = 0.625\n",
      "Training iter #19336:   Batch Loss = 0.581816, Accuracy = 0.75\n",
      "Training iter #19344:   Batch Loss = 0.582846, Accuracy = 0.75\n",
      "Training iter #19352:   Batch Loss = 0.394071, Accuracy = 0.875\n",
      "Training iter #19360:   Batch Loss = 0.012112, Accuracy = 1.0\n",
      "Training iter #19368:   Batch Loss = 0.778220, Accuracy = 0.75\n",
      "Training iter #19376:   Batch Loss = 0.396023, Accuracy = 0.875\n",
      "Training iter #19384:   Batch Loss = 0.677772, Accuracy = 0.625\n",
      "Training iter #19392:   Batch Loss = 0.204312, Accuracy = 1.0\n",
      "Training iter #19400:   Batch Loss = 0.737391, Accuracy = 0.625\n",
      "Training iter #19408:   Batch Loss = 0.405730, Accuracy = 0.875\n",
      "Training iter #19416:   Batch Loss = 0.387140, Accuracy = 0.875\n",
      "Training iter #19424:   Batch Loss = 0.410686, Accuracy = 0.875\n",
      "Training iter #19432:   Batch Loss = 0.414523, Accuracy = 0.875\n",
      "Training iter #19440:   Batch Loss = 0.388988, Accuracy = 0.875\n",
      "Training iter #19448:   Batch Loss = 0.011907, Accuracy = 1.0\n",
      "Training iter #19456:   Batch Loss = 0.011534, Accuracy = 1.0\n",
      "Training iter #19464:   Batch Loss = 1.135939, Accuracy = 0.625\n",
      "Training iter #19472:   Batch Loss = 0.687647, Accuracy = 0.625\n",
      "Training iter #19480:   Batch Loss = 0.705577, Accuracy = 0.5\n",
      "Training iter #19488:   Batch Loss = 0.672276, Accuracy = 0.625\n",
      "Training iter #19496:   Batch Loss = 0.392950, Accuracy = 0.875\n",
      "Training iter #19504:   Batch Loss = 0.447669, Accuracy = 0.875\n",
      "Training iter #19512:   Batch Loss = 0.388968, Accuracy = 0.875\n",
      "Training iter #19520:   Batch Loss = 0.453837, Accuracy = 0.875\n",
      "Training iter #19528:   Batch Loss = 0.680798, Accuracy = 0.625\n",
      "Training iter #19536:   Batch Loss = 0.573213, Accuracy = 0.75\n",
      "Training iter #19544:   Batch Loss = 0.049061, Accuracy = 1.0\n",
      "Training iter #19552:   Batch Loss = 0.395304, Accuracy = 0.875\n",
      "Training iter #19560:   Batch Loss = 0.026752, Accuracy = 1.0\n",
      "Training iter #19568:   Batch Loss = 0.387571, Accuracy = 0.875\n",
      "Training iter #19576:   Batch Loss = 0.387325, Accuracy = 0.875\n",
      "Training iter #19584:   Batch Loss = 0.482353, Accuracy = 0.875\n",
      "Training iter #19592:   Batch Loss = 0.072460, Accuracy = 1.0\n",
      "Training iter #19600:   Batch Loss = 0.387630, Accuracy = 0.875\n",
      "Training iter #19608:   Batch Loss = 0.577818, Accuracy = 0.75\n",
      "Training iter #19616:   Batch Loss = 0.059709, Accuracy = 1.0\n",
      "Training iter #19624:   Batch Loss = 0.011999, Accuracy = 1.0\n",
      "Training iter #19632:   Batch Loss = 0.996794, Accuracy = 0.625\n",
      "Training iter #19640:   Batch Loss = 0.572422, Accuracy = 0.75\n",
      "Training iter #19648:   Batch Loss = 0.718551, Accuracy = 0.5\n",
      "Training iter #19656:   Batch Loss = 0.489290, Accuracy = 0.875\n",
      "Training iter #19664:   Batch Loss = 0.012417, Accuracy = 1.0\n",
      "Training iter #19672:   Batch Loss = 0.387207, Accuracy = 0.875\n",
      "Training iter #19680:   Batch Loss = 0.387561, Accuracy = 0.875\n",
      "Training iter #19688:   Batch Loss = 0.580725, Accuracy = 0.75\n",
      "Training iter #19696:   Batch Loss = 0.572233, Accuracy = 0.75\n",
      "Training iter #19704:   Batch Loss = 0.582325, Accuracy = 0.75\n",
      "Training iter #19712:   Batch Loss = 0.399956, Accuracy = 0.875\n",
      "Training iter #19720:   Batch Loss = 0.702850, Accuracy = 0.625\n",
      "Training iter #19728:   Batch Loss = 0.673418, Accuracy = 0.625\n",
      "Training iter #19736:   Batch Loss = 0.572661, Accuracy = 0.75\n",
      "Training iter #19744:   Batch Loss = 0.444086, Accuracy = 0.875\n",
      "Training iter #19752:   Batch Loss = 0.572876, Accuracy = 0.75\n",
      "Training iter #19760:   Batch Loss = 0.419827, Accuracy = 0.875\n",
      "Training iter #19768:   Batch Loss = 0.076952, Accuracy = 1.0\n",
      "Training iter #19776:   Batch Loss = 0.012044, Accuracy = 1.0\n",
      "Training iter #19784:   Batch Loss = 0.011513, Accuracy = 1.0\n",
      "Training iter #19792:   Batch Loss = 1.009092, Accuracy = 0.625\n",
      "Training iter #19800:   Batch Loss = 0.175676, Accuracy = 1.0\n",
      "Training iter #19808:   Batch Loss = 0.572833, Accuracy = 0.75\n",
      "Training iter #19816:   Batch Loss = 0.672009, Accuracy = 0.625\n",
      "Training iter #19824:   Batch Loss = 0.671072, Accuracy = 0.625\n",
      "Training iter #19832:   Batch Loss = 0.595057, Accuracy = 0.75\n",
      "Training iter #19840:   Batch Loss = 0.398413, Accuracy = 0.875\n",
      "Training iter #19848:   Batch Loss = 0.573248, Accuracy = 0.75\n",
      "Training iter #19856:   Batch Loss = 0.572190, Accuracy = 0.75\n",
      "Training iter #19864:   Batch Loss = 0.042781, Accuracy = 1.0\n",
      "Training iter #19872:   Batch Loss = 1.464933, Accuracy = 0.375\n",
      "Training iter #19880:   Batch Loss = 0.435132, Accuracy = 0.875\n",
      "Training iter #19888:   Batch Loss = 0.388250, Accuracy = 0.875\n",
      "Training iter #19896:   Batch Loss = 0.443916, Accuracy = 0.875\n",
      "Training iter #19904:   Batch Loss = 0.572326, Accuracy = 0.75\n",
      "Training iter #19912:   Batch Loss = 0.059264, Accuracy = 1.0\n",
      "Training iter #19920:   Batch Loss = 0.011463, Accuracy = 1.0\n",
      "Training iter #19928:   Batch Loss = 0.942741, Accuracy = 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #19936:   Batch Loss = 0.405892, Accuracy = 0.875\n",
      "Training iter #19944:   Batch Loss = 0.838456, Accuracy = 0.375\n",
      "Training iter #19952:   Batch Loss = 0.719862, Accuracy = 0.375\n",
      "Training iter #19960:   Batch Loss = 0.640913, Accuracy = 0.75\n",
      "Training iter #19968:   Batch Loss = 0.590708, Accuracy = 0.75\n",
      "Training iter #19976:   Batch Loss = 0.186822, Accuracy = 1.0\n",
      "Training iter #19984:   Batch Loss = 0.399531, Accuracy = 0.875\n",
      "Training iter #19992:   Batch Loss = 0.581608, Accuracy = 0.75\n",
      "Training iter #20000:   Batch Loss = 0.673191, Accuracy = 0.625\n",
      "Training iter #20008:   Batch Loss = 0.578959, Accuracy = 0.75\n",
      "Training iter #20016:   Batch Loss = 0.585789, Accuracy = 0.75\n",
      "Training iter #20024:   Batch Loss = 0.395084, Accuracy = 0.875\n",
      "Training iter #20032:   Batch Loss = 0.011685, Accuracy = 1.0\n",
      "Training iter #20040:   Batch Loss = 0.782447, Accuracy = 0.75\n",
      "Training iter #20048:   Batch Loss = 0.394314, Accuracy = 0.875\n",
      "Training iter #20056:   Batch Loss = 0.678307, Accuracy = 0.625\n",
      "Training iter #20064:   Batch Loss = 0.206902, Accuracy = 1.0\n",
      "Training iter #20072:   Batch Loss = 0.730188, Accuracy = 0.625\n",
      "Training iter #20080:   Batch Loss = 0.403070, Accuracy = 0.875\n",
      "Training iter #20088:   Batch Loss = 0.387190, Accuracy = 0.875\n",
      "Training iter #20096:   Batch Loss = 0.411454, Accuracy = 0.875\n",
      "Training iter #20104:   Batch Loss = 0.414116, Accuracy = 0.875\n",
      "Training iter #20112:   Batch Loss = 0.391005, Accuracy = 0.875\n",
      "Training iter #20120:   Batch Loss = 0.011376, Accuracy = 1.0\n",
      "Training iter #20128:   Batch Loss = 0.011092, Accuracy = 1.0\n",
      "Training iter #20136:   Batch Loss = 1.166237, Accuracy = 0.625\n",
      "Training iter #20144:   Batch Loss = 0.688519, Accuracy = 0.625\n",
      "Training iter #20152:   Batch Loss = 0.708778, Accuracy = 0.5\n",
      "Training iter #20160:   Batch Loss = 0.673197, Accuracy = 0.625\n",
      "Training iter #20168:   Batch Loss = 0.424773, Accuracy = 0.875\n",
      "Training iter #20176:   Batch Loss = 0.443507, Accuracy = 0.875\n",
      "Training iter #20184:   Batch Loss = 0.391339, Accuracy = 0.875\n",
      "Training iter #20192:   Batch Loss = 0.431125, Accuracy = 0.875\n",
      "Training iter #20200:   Batch Loss = 0.680131, Accuracy = 0.625\n",
      "Training iter #20208:   Batch Loss = 0.572877, Accuracy = 0.75\n",
      "Training iter #20216:   Batch Loss = 0.055378, Accuracy = 1.0\n",
      "Training iter #20224:   Batch Loss = 0.393899, Accuracy = 0.875\n",
      "Training iter #20232:   Batch Loss = 0.028879, Accuracy = 1.0\n",
      "Training iter #20240:   Batch Loss = 0.387938, Accuracy = 0.875\n",
      "Training iter #20248:   Batch Loss = 0.386842, Accuracy = 0.875\n",
      "Training iter #20256:   Batch Loss = 0.436433, Accuracy = 0.875\n",
      "Training iter #20264:   Batch Loss = 0.075259, Accuracy = 1.0\n",
      "Training iter #20272:   Batch Loss = 0.388334, Accuracy = 0.875\n",
      "Training iter #20280:   Batch Loss = 0.576125, Accuracy = 0.75\n",
      "Training iter #20288:   Batch Loss = 0.062445, Accuracy = 1.0\n",
      "Training iter #20296:   Batch Loss = 0.011574, Accuracy = 1.0\n",
      "Training iter #20304:   Batch Loss = 1.004077, Accuracy = 0.625\n",
      "Training iter #20312:   Batch Loss = 0.572282, Accuracy = 0.75\n",
      "Training iter #20320:   Batch Loss = 0.717100, Accuracy = 0.5\n",
      "Training iter #20328:   Batch Loss = 0.487726, Accuracy = 0.875\n",
      "Training iter #20336:   Batch Loss = 0.011785, Accuracy = 1.0\n",
      "Training iter #20344:   Batch Loss = 0.386899, Accuracy = 0.875\n",
      "Training iter #20352:   Batch Loss = 0.386684, Accuracy = 0.875\n",
      "Training iter #20360:   Batch Loss = 0.592987, Accuracy = 0.75\n",
      "Training iter #20368:   Batch Loss = 0.572747, Accuracy = 0.75\n",
      "Training iter #20376:   Batch Loss = 0.612157, Accuracy = 0.75\n",
      "Training iter #20384:   Batch Loss = 0.544971, Accuracy = 0.875\n",
      "Training iter #20392:   Batch Loss = 0.672636, Accuracy = 0.625\n",
      "Training iter #20400:   Batch Loss = 0.670912, Accuracy = 0.625\n",
      "Training iter #20408:   Batch Loss = 0.596627, Accuracy = 0.75\n",
      "Training iter #20416:   Batch Loss = 0.399393, Accuracy = 0.875\n",
      "Training iter #20424:   Batch Loss = 0.573107, Accuracy = 0.75\n",
      "Training iter #20432:   Batch Loss = 0.387329, Accuracy = 0.875\n",
      "Training iter #20440:   Batch Loss = 0.012186, Accuracy = 1.0\n",
      "Training iter #20448:   Batch Loss = 0.011463, Accuracy = 1.0\n",
      "Training iter #20456:   Batch Loss = 0.011006, Accuracy = 1.0\n",
      "Training iter #20464:   Batch Loss = 1.028522, Accuracy = 0.625\n",
      "Training iter #20472:   Batch Loss = 0.177187, Accuracy = 1.0\n",
      "Training iter #20480:   Batch Loss = 0.572504, Accuracy = 0.75\n",
      "Training iter #20488:   Batch Loss = 0.671900, Accuracy = 0.625\n",
      "Training iter #20496:   Batch Loss = 0.670644, Accuracy = 0.625\n",
      "Training iter #20504:   Batch Loss = 0.588751, Accuracy = 0.75\n",
      "Training iter #20512:   Batch Loss = 0.396320, Accuracy = 0.875\n",
      "Training iter #20520:   Batch Loss = 0.572148, Accuracy = 0.75\n",
      "Training iter #20528:   Batch Loss = 0.571632, Accuracy = 0.75\n",
      "Training iter #20536:   Batch Loss = 0.042886, Accuracy = 1.0\n",
      "Training iter #20544:   Batch Loss = 1.471391, Accuracy = 0.375\n",
      "Training iter #20552:   Batch Loss = 0.434035, Accuracy = 0.875\n",
      "Training iter #20560:   Batch Loss = 0.387671, Accuracy = 0.875\n",
      "Training iter #20568:   Batch Loss = 0.438647, Accuracy = 0.875\n",
      "Training iter #20576:   Batch Loss = 0.571951, Accuracy = 0.75\n",
      "Training iter #20584:   Batch Loss = 0.061099, Accuracy = 1.0\n",
      "Training iter #20592:   Batch Loss = 0.010794, Accuracy = 1.0\n",
      "Training iter #20600:   Batch Loss = 0.937563, Accuracy = 0.625\n",
      "Training iter #20608:   Batch Loss = 0.405186, Accuracy = 0.875\n",
      "Training iter #20616:   Batch Loss = 0.834037, Accuracy = 0.375\n",
      "Training iter #20624:   Batch Loss = 0.698281, Accuracy = 0.625\n",
      "Training iter #20632:   Batch Loss = 0.737664, Accuracy = 0.25\n",
      "Training iter #20640:   Batch Loss = 0.575049, Accuracy = 0.75\n",
      "Training iter #20648:   Batch Loss = 0.013356, Accuracy = 1.0\n",
      "Training iter #20656:   Batch Loss = 0.390160, Accuracy = 0.875\n",
      "Training iter #20664:   Batch Loss = 0.590672, Accuracy = 0.75\n",
      "Training iter #20672:   Batch Loss = 0.672383, Accuracy = 0.625\n",
      "Training iter #20680:   Batch Loss = 0.578366, Accuracy = 0.75\n",
      "Training iter #20688:   Batch Loss = 0.585237, Accuracy = 0.75\n",
      "Training iter #20696:   Batch Loss = 0.393528, Accuracy = 0.875\n",
      "Training iter #20704:   Batch Loss = 0.011538, Accuracy = 1.0\n",
      "Training iter #20712:   Batch Loss = 0.804043, Accuracy = 0.75\n",
      "Training iter #20720:   Batch Loss = 0.398290, Accuracy = 0.875\n",
      "Training iter #20728:   Batch Loss = 0.678392, Accuracy = 0.625\n",
      "Training iter #20736:   Batch Loss = 0.213292, Accuracy = 1.0\n",
      "Training iter #20744:   Batch Loss = 0.733267, Accuracy = 0.625\n",
      "Training iter #20752:   Batch Loss = 0.407046, Accuracy = 0.875\n",
      "Training iter #20760:   Batch Loss = 0.386376, Accuracy = 0.875\n",
      "Training iter #20768:   Batch Loss = 0.401095, Accuracy = 0.875\n",
      "Training iter #20776:   Batch Loss = 0.406476, Accuracy = 0.875\n",
      "Training iter #20784:   Batch Loss = 0.386900, Accuracy = 0.875\n",
      "Training iter #20792:   Batch Loss = 0.012042, Accuracy = 1.0\n",
      "Training iter #20800:   Batch Loss = 0.011087, Accuracy = 1.0\n",
      "Training iter #20808:   Batch Loss = 1.103882, Accuracy = 0.625\n",
      "Training iter #20816:   Batch Loss = 0.687524, Accuracy = 0.625\n",
      "Training iter #20824:   Batch Loss = 0.707866, Accuracy = 0.5\n",
      "Training iter #20832:   Batch Loss = 0.671918, Accuracy = 0.625\n",
      "Training iter #20840:   Batch Loss = 0.394496, Accuracy = 0.875\n",
      "Training iter #20848:   Batch Loss = 0.438527, Accuracy = 0.875\n",
      "Training iter #20856:   Batch Loss = 0.386492, Accuracy = 0.875\n",
      "Training iter #20864:   Batch Loss = 0.423178, Accuracy = 0.875\n",
      "Training iter #20872:   Batch Loss = 0.714086, Accuracy = 0.625\n",
      "Training iter #20880:   Batch Loss = 0.571978, Accuracy = 0.75\n",
      "Training iter #20888:   Batch Loss = 0.070433, Accuracy = 1.0\n",
      "Training iter #20896:   Batch Loss = 0.388690, Accuracy = 0.875\n",
      "Training iter #20904:   Batch Loss = 0.021608, Accuracy = 1.0\n",
      "Training iter #20912:   Batch Loss = 0.386925, Accuracy = 0.875\n",
      "Training iter #20920:   Batch Loss = 0.386760, Accuracy = 0.875\n",
      "Training iter #20928:   Batch Loss = 0.482111, Accuracy = 0.875\n",
      "Training iter #20936:   Batch Loss = 0.072926, Accuracy = 1.0\n",
      "Training iter #20944:   Batch Loss = 0.387141, Accuracy = 0.875\n",
      "Training iter #20952:   Batch Loss = 0.576479, Accuracy = 0.75\n",
      "Training iter #20960:   Batch Loss = 0.059444, Accuracy = 1.0\n",
      "Training iter #20968:   Batch Loss = 0.011421, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #20976:   Batch Loss = 1.002036, Accuracy = 0.625\n",
      "Training iter #20984:   Batch Loss = 0.571815, Accuracy = 0.75\n",
      "Training iter #20992:   Batch Loss = 0.719448, Accuracy = 0.5\n",
      "Training iter #21000:   Batch Loss = 0.491317, Accuracy = 0.875\n",
      "Training iter #21008:   Batch Loss = 0.012575, Accuracy = 1.0\n",
      "Training iter #21016:   Batch Loss = 0.386644, Accuracy = 0.875\n",
      "Training iter #21024:   Batch Loss = 0.386825, Accuracy = 0.875\n",
      "Training iter #21032:   Batch Loss = 0.578154, Accuracy = 0.75\n",
      "Training iter #21040:   Batch Loss = 0.571610, Accuracy = 0.75\n",
      "Training iter #21048:   Batch Loss = 0.588029, Accuracy = 0.75\n",
      "Training iter #21056:   Batch Loss = 0.398174, Accuracy = 0.875\n",
      "Training iter #21064:   Batch Loss = 0.704545, Accuracy = 0.625\n",
      "Training iter #21072:   Batch Loss = 0.673302, Accuracy = 0.625\n",
      "Training iter #21080:   Batch Loss = 0.571626, Accuracy = 0.75\n",
      "Training iter #21088:   Batch Loss = 0.446992, Accuracy = 0.875\n",
      "Training iter #21096:   Batch Loss = 0.571526, Accuracy = 0.75\n",
      "Training iter #21104:   Batch Loss = 0.428216, Accuracy = 0.875\n",
      "Training iter #21112:   Batch Loss = 0.080393, Accuracy = 1.0\n",
      "Training iter #21120:   Batch Loss = 0.011388, Accuracy = 1.0\n",
      "Training iter #21128:   Batch Loss = 0.010739, Accuracy = 1.0\n",
      "Training iter #21136:   Batch Loss = 1.036481, Accuracy = 0.625\n",
      "Training iter #21144:   Batch Loss = 0.177225, Accuracy = 1.0\n",
      "Training iter #21152:   Batch Loss = 0.572088, Accuracy = 0.75\n",
      "Training iter #21160:   Batch Loss = 0.671803, Accuracy = 0.625\n",
      "Training iter #21168:   Batch Loss = 0.670357, Accuracy = 0.625\n",
      "Training iter #21176:   Batch Loss = 0.586089, Accuracy = 0.75\n",
      "Training iter #21184:   Batch Loss = 0.396423, Accuracy = 0.875\n",
      "Training iter #21192:   Batch Loss = 0.571698, Accuracy = 0.75\n",
      "Training iter #21200:   Batch Loss = 0.571345, Accuracy = 0.75\n",
      "Training iter #21208:   Batch Loss = 0.047159, Accuracy = 1.0\n",
      "Training iter #21216:   Batch Loss = 1.467205, Accuracy = 0.375\n",
      "Training iter #21224:   Batch Loss = 0.432285, Accuracy = 0.875\n",
      "Training iter #21232:   Batch Loss = 0.387417, Accuracy = 0.875\n",
      "Training iter #21240:   Batch Loss = 0.428452, Accuracy = 0.875\n",
      "Training iter #21248:   Batch Loss = 0.571936, Accuracy = 0.75\n",
      "Training iter #21256:   Batch Loss = 0.074231, Accuracy = 1.0\n",
      "Training iter #21264:   Batch Loss = 0.010789, Accuracy = 1.0\n",
      "Training iter #21272:   Batch Loss = 0.945050, Accuracy = 0.625\n",
      "Training iter #21280:   Batch Loss = 0.403875, Accuracy = 0.875\n",
      "Training iter #21288:   Batch Loss = 0.843681, Accuracy = 0.375\n",
      "Training iter #21296:   Batch Loss = 0.718578, Accuracy = 0.375\n",
      "Training iter #21304:   Batch Loss = 0.641778, Accuracy = 0.75\n",
      "Training iter #21312:   Batch Loss = 0.591792, Accuracy = 0.75\n",
      "Training iter #21320:   Batch Loss = 0.190431, Accuracy = 1.0\n",
      "Training iter #21328:   Batch Loss = 0.399412, Accuracy = 0.875\n",
      "Training iter #21336:   Batch Loss = 0.578955, Accuracy = 0.75\n",
      "Training iter #21344:   Batch Loss = 0.671843, Accuracy = 0.625\n",
      "Training iter #21352:   Batch Loss = 0.576989, Accuracy = 0.75\n",
      "Training iter #21360:   Batch Loss = 0.585982, Accuracy = 0.75\n",
      "Training iter #21368:   Batch Loss = 0.395994, Accuracy = 0.875\n",
      "Training iter #21376:   Batch Loss = 0.011070, Accuracy = 1.0\n",
      "Training iter #21384:   Batch Loss = 0.773173, Accuracy = 0.75\n",
      "Training iter #21392:   Batch Loss = 0.394325, Accuracy = 0.875\n",
      "Training iter #21400:   Batch Loss = 0.678548, Accuracy = 0.625\n",
      "Training iter #21408:   Batch Loss = 0.212217, Accuracy = 1.0\n",
      "Training iter #21416:   Batch Loss = 0.730339, Accuracy = 0.625\n",
      "Training iter #21424:   Batch Loss = 0.406101, Accuracy = 0.875\n",
      "Training iter #21432:   Batch Loss = 0.386027, Accuracy = 0.875\n",
      "Training iter #21440:   Batch Loss = 0.394957, Accuracy = 0.875\n",
      "Training iter #21448:   Batch Loss = 0.400008, Accuracy = 0.875\n",
      "Training iter #21456:   Batch Loss = 0.386268, Accuracy = 0.875\n",
      "Training iter #21464:   Batch Loss = 0.011849, Accuracy = 1.0\n",
      "Training iter #21472:   Batch Loss = 0.010675, Accuracy = 1.0\n",
      "Training iter #21480:   Batch Loss = 1.086583, Accuracy = 0.625\n",
      "Training iter #21488:   Batch Loss = 0.686538, Accuracy = 0.625\n",
      "Training iter #21496:   Batch Loss = 0.706240, Accuracy = 0.5\n",
      "Training iter #21504:   Batch Loss = 0.670820, Accuracy = 0.625\n",
      "Training iter #21512:   Batch Loss = 0.389847, Accuracy = 0.875\n",
      "Training iter #21520:   Batch Loss = 0.446657, Accuracy = 0.875\n",
      "Training iter #21528:   Batch Loss = 0.390324, Accuracy = 0.875\n",
      "Training iter #21536:   Batch Loss = 0.434584, Accuracy = 0.875\n",
      "Training iter #21544:   Batch Loss = 0.678198, Accuracy = 0.625\n",
      "Training iter #21552:   Batch Loss = 0.572207, Accuracy = 0.75\n",
      "Training iter #21560:   Batch Loss = 0.051591, Accuracy = 1.0\n",
      "Training iter #21568:   Batch Loss = 0.395615, Accuracy = 0.875\n",
      "Training iter #21576:   Batch Loss = 0.028015, Accuracy = 1.0\n",
      "Training iter #21584:   Batch Loss = 0.387510, Accuracy = 0.875\n",
      "Training iter #21592:   Batch Loss = 0.386788, Accuracy = 0.875\n",
      "Training iter #21600:   Batch Loss = 0.475010, Accuracy = 0.875\n",
      "Training iter #21608:   Batch Loss = 0.074602, Accuracy = 1.0\n",
      "Training iter #21616:   Batch Loss = 0.386636, Accuracy = 0.875\n",
      "Training iter #21624:   Batch Loss = 0.575099, Accuracy = 0.75\n",
      "Training iter #21632:   Batch Loss = 0.065172, Accuracy = 1.0\n",
      "Training iter #21640:   Batch Loss = 0.011118, Accuracy = 1.0\n",
      "Training iter #21648:   Batch Loss = 1.000548, Accuracy = 0.625\n",
      "Training iter #21656:   Batch Loss = 0.571554, Accuracy = 0.75\n",
      "Training iter #21664:   Batch Loss = 0.719908, Accuracy = 0.5\n",
      "Training iter #21672:   Batch Loss = 0.488841, Accuracy = 0.875\n",
      "Training iter #21680:   Batch Loss = 0.011858, Accuracy = 1.0\n",
      "Training iter #21688:   Batch Loss = 0.386246, Accuracy = 0.875\n",
      "Training iter #21696:   Batch Loss = 0.386301, Accuracy = 0.875\n",
      "Training iter #21704:   Batch Loss = 0.581036, Accuracy = 0.75\n",
      "Training iter #21712:   Batch Loss = 0.571264, Accuracy = 0.75\n",
      "Training iter #21720:   Batch Loss = 0.587820, Accuracy = 0.75\n",
      "Training iter #21728:   Batch Loss = 0.399758, Accuracy = 0.875\n",
      "Training iter #21736:   Batch Loss = 0.704559, Accuracy = 0.625\n",
      "Training iter #21744:   Batch Loss = 0.673051, Accuracy = 0.625\n",
      "Training iter #21752:   Batch Loss = 0.571332, Accuracy = 0.75\n",
      "Training iter #21760:   Batch Loss = 0.449489, Accuracy = 0.875\n",
      "Training iter #21768:   Batch Loss = 0.571411, Accuracy = 0.75\n",
      "Training iter #21776:   Batch Loss = 0.424954, Accuracy = 0.875\n",
      "Training iter #21784:   Batch Loss = 0.076072, Accuracy = 1.0\n",
      "Training iter #21792:   Batch Loss = 0.011082, Accuracy = 1.0\n",
      "Training iter #21800:   Batch Loss = 0.010557, Accuracy = 1.0\n",
      "Training iter #21808:   Batch Loss = 1.010443, Accuracy = 0.625\n",
      "Training iter #21816:   Batch Loss = 0.179546, Accuracy = 1.0\n",
      "Training iter #21824:   Batch Loss = 0.571724, Accuracy = 0.75\n",
      "Training iter #21832:   Batch Loss = 0.671714, Accuracy = 0.625\n",
      "Training iter #21840:   Batch Loss = 0.670080, Accuracy = 0.625\n",
      "Training iter #21848:   Batch Loss = 0.586593, Accuracy = 0.75\n",
      "Training iter #21856:   Batch Loss = 0.396921, Accuracy = 0.875\n",
      "Training iter #21864:   Batch Loss = 0.571352, Accuracy = 0.75\n",
      "Training iter #21872:   Batch Loss = 0.571058, Accuracy = 0.75\n",
      "Training iter #21880:   Batch Loss = 0.048477, Accuracy = 1.0\n",
      "Training iter #21888:   Batch Loss = 1.479378, Accuracy = 0.375\n",
      "Training iter #21896:   Batch Loss = 0.425664, Accuracy = 0.875\n",
      "Training iter #21904:   Batch Loss = 0.386644, Accuracy = 0.875\n",
      "Training iter #21912:   Batch Loss = 0.412101, Accuracy = 0.875\n",
      "Training iter #21920:   Batch Loss = 0.573399, Accuracy = 0.75\n",
      "Training iter #21928:   Batch Loss = 0.125067, Accuracy = 1.0\n",
      "Training iter #21936:   Batch Loss = 0.010645, Accuracy = 1.0\n",
      "Training iter #21944:   Batch Loss = 0.887295, Accuracy = 0.625\n",
      "Training iter #21952:   Batch Loss = 0.399526, Accuracy = 0.875\n",
      "Training iter #21960:   Batch Loss = 0.838098, Accuracy = 0.375\n",
      "Training iter #21968:   Batch Loss = 0.705172, Accuracy = 0.375\n",
      "Training iter #21976:   Batch Loss = 0.661046, Accuracy = 0.75\n",
      "Training iter #21984:   Batch Loss = 0.599603, Accuracy = 0.75\n",
      "Training iter #21992:   Batch Loss = 0.176379, Accuracy = 1.0\n",
      "Training iter #22000:   Batch Loss = 0.395886, Accuracy = 0.875\n",
      "Training iter #22008:   Batch Loss = 0.580465, Accuracy = 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #22016:   Batch Loss = 0.672521, Accuracy = 0.625\n",
      "Training iter #22024:   Batch Loss = 0.579820, Accuracy = 0.75\n",
      "Training iter #22032:   Batch Loss = 0.581759, Accuracy = 0.75\n",
      "Training iter #22040:   Batch Loss = 0.393063, Accuracy = 0.875\n",
      "Training iter #22048:   Batch Loss = 0.010666, Accuracy = 1.0\n",
      "Training iter #22056:   Batch Loss = 0.794195, Accuracy = 0.75\n",
      "Training iter #22064:   Batch Loss = 0.395232, Accuracy = 0.875\n",
      "Training iter #22072:   Batch Loss = 0.678138, Accuracy = 0.625\n",
      "Training iter #22080:   Batch Loss = 0.211603, Accuracy = 1.0\n",
      "Training iter #22088:   Batch Loss = 0.730893, Accuracy = 0.625\n",
      "Training iter #22096:   Batch Loss = 0.406721, Accuracy = 0.875\n",
      "Training iter #22104:   Batch Loss = 0.385775, Accuracy = 0.875\n",
      "Training iter #22112:   Batch Loss = 0.390109, Accuracy = 0.875\n",
      "Training iter #22120:   Batch Loss = 0.394131, Accuracy = 0.875\n",
      "Training iter #22128:   Batch Loss = 0.406901, Accuracy = 0.875\n",
      "Training iter #22136:   Batch Loss = 0.069969, Accuracy = 1.0\n",
      "Training iter #22144:   Batch Loss = 0.010789, Accuracy = 1.0\n",
      "Training iter #22152:   Batch Loss = 1.017272, Accuracy = 0.625\n",
      "Training iter #22160:   Batch Loss = 0.685247, Accuracy = 0.625\n",
      "Training iter #22168:   Batch Loss = 0.708224, Accuracy = 0.5\n",
      "Training iter #22176:   Batch Loss = 0.670659, Accuracy = 0.625\n",
      "Training iter #22184:   Batch Loss = 0.387163, Accuracy = 0.875\n",
      "Training iter #22192:   Batch Loss = 0.460508, Accuracy = 0.875\n",
      "Training iter #22200:   Batch Loss = 0.390376, Accuracy = 0.875\n",
      "Training iter #22208:   Batch Loss = 0.427930, Accuracy = 0.875\n",
      "Training iter #22216:   Batch Loss = 0.676242, Accuracy = 0.625\n",
      "Training iter #22224:   Batch Loss = 0.572077, Accuracy = 0.75\n",
      "Training iter #22232:   Batch Loss = 0.045327, Accuracy = 1.0\n",
      "Training iter #22240:   Batch Loss = 0.396967, Accuracy = 0.875\n",
      "Training iter #22248:   Batch Loss = 0.028543, Accuracy = 1.0\n",
      "Training iter #22256:   Batch Loss = 0.386875, Accuracy = 0.875\n",
      "Training iter #22264:   Batch Loss = 0.387434, Accuracy = 0.875\n",
      "Training iter #22272:   Batch Loss = 0.481792, Accuracy = 0.875\n",
      "Training iter #22280:   Batch Loss = 0.078936, Accuracy = 1.0\n",
      "Training iter #22288:   Batch Loss = 0.386211, Accuracy = 0.875\n",
      "Training iter #22296:   Batch Loss = 0.576000, Accuracy = 0.75\n",
      "Training iter #22304:   Batch Loss = 0.062255, Accuracy = 1.0\n",
      "Training iter #22312:   Batch Loss = 0.010772, Accuracy = 1.0\n",
      "Training iter #22320:   Batch Loss = 0.988413, Accuracy = 0.625\n",
      "Training iter #22328:   Batch Loss = 0.571246, Accuracy = 0.75\n",
      "Training iter #22336:   Batch Loss = 0.719194, Accuracy = 0.5\n",
      "Training iter #22344:   Batch Loss = 0.488961, Accuracy = 0.875\n",
      "Training iter #22352:   Batch Loss = 0.011267, Accuracy = 1.0\n",
      "Training iter #22360:   Batch Loss = 0.386332, Accuracy = 0.875\n",
      "Training iter #22368:   Batch Loss = 0.386295, Accuracy = 0.875\n",
      "Training iter #22376:   Batch Loss = 0.579257, Accuracy = 0.75\n",
      "Training iter #22384:   Batch Loss = 0.571148, Accuracy = 0.75\n",
      "Training iter #22392:   Batch Loss = 0.597877, Accuracy = 0.75\n",
      "Training iter #22400:   Batch Loss = 0.398010, Accuracy = 0.875\n",
      "Training iter #22408:   Batch Loss = 0.701015, Accuracy = 0.625\n",
      "Training iter #22416:   Batch Loss = 0.672864, Accuracy = 0.625\n",
      "Training iter #22424:   Batch Loss = 0.571154, Accuracy = 0.75\n",
      "Training iter #22432:   Batch Loss = 0.452360, Accuracy = 0.875\n",
      "Training iter #22440:   Batch Loss = 0.571186, Accuracy = 0.75\n",
      "Training iter #22448:   Batch Loss = 0.427855, Accuracy = 0.875\n",
      "Training iter #22456:   Batch Loss = 0.080480, Accuracy = 1.0\n",
      "Training iter #22464:   Batch Loss = 0.010956, Accuracy = 1.0\n",
      "Training iter #22472:   Batch Loss = 0.010336, Accuracy = 1.0\n",
      "Training iter #22480:   Batch Loss = 1.018532, Accuracy = 0.625\n",
      "Training iter #22488:   Batch Loss = 0.178134, Accuracy = 1.0\n",
      "Training iter #22496:   Batch Loss = 0.571755, Accuracy = 0.75\n",
      "Training iter #22504:   Batch Loss = 0.671324, Accuracy = 0.625\n",
      "Training iter #22512:   Batch Loss = 0.669926, Accuracy = 0.625\n",
      "Training iter #22520:   Batch Loss = 0.586563, Accuracy = 0.75\n",
      "Training iter #22528:   Batch Loss = 0.396847, Accuracy = 0.875\n",
      "Training iter #22536:   Batch Loss = 0.571321, Accuracy = 0.75\n",
      "Training iter #22544:   Batch Loss = 0.570892, Accuracy = 0.75\n",
      "Training iter #22552:   Batch Loss = 0.048342, Accuracy = 1.0\n",
      "Training iter #22560:   Batch Loss = 1.491557, Accuracy = 0.375\n",
      "Training iter #22568:   Batch Loss = 0.425856, Accuracy = 0.875\n",
      "Training iter #22576:   Batch Loss = 0.386462, Accuracy = 0.875\n",
      "Training iter #22584:   Batch Loss = 0.415463, Accuracy = 0.875\n",
      "Training iter #22592:   Batch Loss = 0.572584, Accuracy = 0.75\n",
      "Training iter #22600:   Batch Loss = 0.111235, Accuracy = 1.0\n",
      "Training iter #22608:   Batch Loss = 0.010356, Accuracy = 1.0\n",
      "Training iter #22616:   Batch Loss = 0.903418, Accuracy = 0.625\n",
      "Training iter #22624:   Batch Loss = 0.400045, Accuracy = 0.875\n",
      "Training iter #22632:   Batch Loss = 0.837589, Accuracy = 0.375\n",
      "Training iter #22640:   Batch Loss = 0.705729, Accuracy = 0.375\n",
      "Training iter #22648:   Batch Loss = 0.657877, Accuracy = 0.75\n",
      "Training iter #22656:   Batch Loss = 0.598421, Accuracy = 0.75\n",
      "Training iter #22664:   Batch Loss = 0.181217, Accuracy = 1.0\n",
      "Training iter #22672:   Batch Loss = 0.397172, Accuracy = 0.875\n",
      "Training iter #22680:   Batch Loss = 0.579872, Accuracy = 0.75\n",
      "Training iter #22688:   Batch Loss = 0.672303, Accuracy = 0.625\n",
      "Training iter #22696:   Batch Loss = 0.579229, Accuracy = 0.75\n",
      "Training iter #22704:   Batch Loss = 0.582273, Accuracy = 0.75\n",
      "Training iter #22712:   Batch Loss = 0.393806, Accuracy = 0.875\n",
      "Training iter #22720:   Batch Loss = 0.010365, Accuracy = 1.0\n",
      "Training iter #22728:   Batch Loss = 0.770195, Accuracy = 0.75\n",
      "Training iter #22736:   Batch Loss = 0.392324, Accuracy = 0.875\n",
      "Training iter #22744:   Batch Loss = 0.677911, Accuracy = 0.625\n",
      "Training iter #22752:   Batch Loss = 0.207299, Accuracy = 1.0\n",
      "Training iter #22760:   Batch Loss = 0.732990, Accuracy = 0.625\n",
      "Training iter #22768:   Batch Loss = 0.405535, Accuracy = 0.875\n",
      "Training iter #22776:   Batch Loss = 0.385582, Accuracy = 0.875\n",
      "Training iter #22784:   Batch Loss = 0.385555, Accuracy = 0.875\n",
      "Training iter #22792:   Batch Loss = 0.451753, Accuracy = 0.875\n",
      "Training iter #22800:   Batch Loss = 0.409658, Accuracy = 0.875\n",
      "Training iter #22808:   Batch Loss = 0.010439, Accuracy = 1.0\n",
      "Training iter #22816:   Batch Loss = 0.010089, Accuracy = 1.0\n",
      "Training iter #22824:   Batch Loss = 1.200811, Accuracy = 0.625\n",
      "Training iter #22832:   Batch Loss = 0.685811, Accuracy = 0.625\n",
      "Training iter #22840:   Batch Loss = 0.938651, Accuracy = 0.5\n",
      "Training iter #22848:   Batch Loss = 0.672165, Accuracy = 0.625\n",
      "Training iter #22856:   Batch Loss = 0.398519, Accuracy = 0.875\n",
      "Training iter #22864:   Batch Loss = 0.439659, Accuracy = 0.875\n",
      "Training iter #22872:   Batch Loss = 0.392460, Accuracy = 0.875\n",
      "Training iter #22880:   Batch Loss = 0.426929, Accuracy = 0.875\n",
      "Training iter #22888:   Batch Loss = 0.685241, Accuracy = 0.625\n",
      "Training iter #22896:   Batch Loss = 0.571860, Accuracy = 0.75\n",
      "Training iter #22904:   Batch Loss = 0.059359, Accuracy = 1.0\n",
      "Training iter #22912:   Batch Loss = 0.393220, Accuracy = 0.875\n",
      "Training iter #22920:   Batch Loss = 0.027803, Accuracy = 1.0\n",
      "Training iter #22928:   Batch Loss = 0.387828, Accuracy = 0.875\n",
      "Training iter #22936:   Batch Loss = 0.387384, Accuracy = 0.875\n",
      "Training iter #22944:   Batch Loss = 0.457411, Accuracy = 0.875\n",
      "Training iter #22952:   Batch Loss = 0.073857, Accuracy = 1.0\n",
      "Training iter #22960:   Batch Loss = 0.386877, Accuracy = 0.875\n",
      "Training iter #22968:   Batch Loss = 0.575301, Accuracy = 0.75\n",
      "Training iter #22976:   Batch Loss = 0.065791, Accuracy = 1.0\n",
      "Training iter #22984:   Batch Loss = 0.010839, Accuracy = 1.0\n",
      "Training iter #22992:   Batch Loss = 1.011515, Accuracy = 0.625\n",
      "Training iter #23000:   Batch Loss = 0.571129, Accuracy = 0.75\n",
      "Training iter #23008:   Batch Loss = 0.720595, Accuracy = 0.5\n",
      "Training iter #23016:   Batch Loss = 0.490362, Accuracy = 0.875\n",
      "Training iter #23024:   Batch Loss = 0.012934, Accuracy = 1.0\n",
      "Training iter #23032:   Batch Loss = 0.385822, Accuracy = 0.875\n",
      "Training iter #23040:   Batch Loss = 0.385624, Accuracy = 0.875\n",
      "Training iter #23048:   Batch Loss = 0.596361, Accuracy = 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #23056:   Batch Loss = 0.574614, Accuracy = 0.75\n",
      "Training iter #23064:   Batch Loss = 0.595150, Accuracy = 0.75\n",
      "Training iter #23072:   Batch Loss = 0.437766, Accuracy = 0.875\n",
      "Training iter #23080:   Batch Loss = 0.670304, Accuracy = 0.625\n",
      "Training iter #23088:   Batch Loss = 0.669929, Accuracy = 0.625\n",
      "Training iter #23096:   Batch Loss = 0.585344, Accuracy = 0.75\n",
      "Training iter #23104:   Batch Loss = 0.397659, Accuracy = 0.875\n",
      "Training iter #23112:   Batch Loss = 0.571142, Accuracy = 0.75\n",
      "Training iter #23120:   Batch Loss = 0.385947, Accuracy = 0.875\n",
      "Training iter #23128:   Batch Loss = 0.011419, Accuracy = 1.0\n",
      "Training iter #23136:   Batch Loss = 0.010401, Accuracy = 1.0\n",
      "Training iter #23144:   Batch Loss = 0.009977, Accuracy = 1.0\n",
      "Training iter #23152:   Batch Loss = 1.037953, Accuracy = 0.625\n",
      "Training iter #23160:   Batch Loss = 0.178856, Accuracy = 1.0\n",
      "Training iter #23168:   Batch Loss = 0.571866, Accuracy = 0.75\n",
      "Training iter #23176:   Batch Loss = 0.671385, Accuracy = 0.625\n",
      "Training iter #23184:   Batch Loss = 0.669637, Accuracy = 0.625\n",
      "Training iter #23192:   Batch Loss = 0.581569, Accuracy = 0.75\n",
      "Training iter #23200:   Batch Loss = 0.395222, Accuracy = 0.875\n",
      "Training iter #23208:   Batch Loss = 0.570806, Accuracy = 0.75\n",
      "Training iter #23216:   Batch Loss = 0.570624, Accuracy = 0.75\n",
      "Training iter #23224:   Batch Loss = 0.052308, Accuracy = 1.0\n",
      "Training iter #23232:   Batch Loss = 1.484831, Accuracy = 0.375\n",
      "Training iter #23240:   Batch Loss = 0.423531, Accuracy = 0.875\n",
      "Training iter #23248:   Batch Loss = 0.386091, Accuracy = 0.875\n",
      "Training iter #23256:   Batch Loss = 0.402330, Accuracy = 0.875\n",
      "Training iter #23264:   Batch Loss = 0.574499, Accuracy = 0.75\n",
      "Training iter #23272:   Batch Loss = 0.168685, Accuracy = 1.0\n",
      "Training iter #23280:   Batch Loss = 0.010194, Accuracy = 1.0\n",
      "Training iter #23288:   Batch Loss = 0.860639, Accuracy = 0.625\n",
      "Training iter #23296:   Batch Loss = 0.399281, Accuracy = 0.875\n",
      "Training iter #23304:   Batch Loss = 0.840725, Accuracy = 0.375\n",
      "Training iter #23312:   Batch Loss = 0.695585, Accuracy = 0.625\n",
      "Training iter #23320:   Batch Loss = 0.688728, Accuracy = 0.75\n",
      "Training iter #23328:   Batch Loss = 0.584990, Accuracy = 0.75\n",
      "Training iter #23336:   Batch Loss = 0.163533, Accuracy = 1.0\n",
      "Training iter #23344:   Batch Loss = 0.391364, Accuracy = 0.875\n",
      "Training iter #23352:   Batch Loss = 0.578349, Accuracy = 0.75\n",
      "Training iter #23360:   Batch Loss = 0.671492, Accuracy = 0.625\n",
      "Training iter #23368:   Batch Loss = 0.576397, Accuracy = 0.75\n",
      "Training iter #23376:   Batch Loss = 0.584184, Accuracy = 0.75\n",
      "Training iter #23384:   Batch Loss = 0.394105, Accuracy = 0.875\n",
      "Training iter #23392:   Batch Loss = 0.010450, Accuracy = 1.0\n",
      "Training iter #23400:   Batch Loss = 0.779052, Accuracy = 0.75\n",
      "Training iter #23408:   Batch Loss = 0.392919, Accuracy = 0.875\n",
      "Training iter #23416:   Batch Loss = 0.681126, Accuracy = 0.625\n",
      "Training iter #23424:   Batch Loss = 0.219478, Accuracy = 1.0\n",
      "Training iter #23432:   Batch Loss = 0.729186, Accuracy = 0.625\n",
      "Training iter #23440:   Batch Loss = 0.406494, Accuracy = 0.875\n",
      "Training iter #23448:   Batch Loss = 0.385330, Accuracy = 0.875\n",
      "Training iter #23456:   Batch Loss = 0.391253, Accuracy = 0.875\n",
      "Training iter #23464:   Batch Loss = 0.396237, Accuracy = 0.875\n",
      "Training iter #23472:   Batch Loss = 0.387347, Accuracy = 0.875\n",
      "Training iter #23480:   Batch Loss = 0.011106, Accuracy = 1.0\n",
      "Training iter #23488:   Batch Loss = 0.010108, Accuracy = 1.0\n",
      "Training iter #23496:   Batch Loss = 1.199973, Accuracy = 0.625\n",
      "Training iter #23504:   Batch Loss = 0.695609, Accuracy = 0.625\n",
      "Training iter #23512:   Batch Loss = 0.708586, Accuracy = 0.5\n",
      "Training iter #23520:   Batch Loss = 0.670656, Accuracy = 0.625\n",
      "Training iter #23528:   Batch Loss = 0.396627, Accuracy = 0.875\n",
      "Training iter #23536:   Batch Loss = 0.429585, Accuracy = 0.875\n",
      "Training iter #23544:   Batch Loss = 0.395445, Accuracy = 0.875\n",
      "Training iter #23552:   Batch Loss = 0.418446, Accuracy = 0.875\n",
      "Training iter #23560:   Batch Loss = 0.687778, Accuracy = 0.625\n",
      "Training iter #23568:   Batch Loss = 0.571513, Accuracy = 0.75\n",
      "Training iter #23576:   Batch Loss = 0.073699, Accuracy = 1.0\n",
      "Training iter #23584:   Batch Loss = 0.390795, Accuracy = 0.875\n",
      "Training iter #23592:   Batch Loss = 0.029614, Accuracy = 1.0\n",
      "Training iter #23600:   Batch Loss = 0.391007, Accuracy = 0.875\n",
      "Training iter #23608:   Batch Loss = 0.387811, Accuracy = 0.875\n",
      "Training iter #23616:   Batch Loss = 0.434398, Accuracy = 0.875\n",
      "Training iter #23624:   Batch Loss = 0.072140, Accuracy = 1.0\n",
      "Training iter #23632:   Batch Loss = 0.388912, Accuracy = 0.875\n",
      "Training iter #23640:   Batch Loss = 0.574526, Accuracy = 0.75\n",
      "Training iter #23648:   Batch Loss = 0.069275, Accuracy = 1.0\n",
      "Training iter #23656:   Batch Loss = 0.010816, Accuracy = 1.0\n",
      "Training iter #23664:   Batch Loss = 1.013477, Accuracy = 0.625\n",
      "Training iter #23672:   Batch Loss = 0.571038, Accuracy = 0.75\n",
      "Training iter #23680:   Batch Loss = 0.722318, Accuracy = 0.5\n",
      "Training iter #23688:   Batch Loss = 0.491589, Accuracy = 0.875\n",
      "Training iter #23696:   Batch Loss = 0.016721, Accuracy = 1.0\n",
      "Training iter #23704:   Batch Loss = 0.388467, Accuracy = 0.875\n",
      "Training iter #23712:   Batch Loss = 0.403497, Accuracy = 0.875\n",
      "Training iter #23720:   Batch Loss = 0.571034, Accuracy = 0.75\n",
      "Training iter #23728:   Batch Loss = 0.627839, Accuracy = 0.75\n",
      "Training iter #23736:   Batch Loss = 0.571196, Accuracy = 0.75\n",
      "Training iter #23744:   Batch Loss = 0.397177, Accuracy = 0.875\n",
      "Training iter #23752:   Batch Loss = 0.671732, Accuracy = 0.625\n",
      "Training iter #23760:   Batch Loss = 0.670605, Accuracy = 0.625\n",
      "Training iter #23768:   Batch Loss = 0.603163, Accuracy = 0.75\n",
      "Training iter #23776:   Batch Loss = 0.393208, Accuracy = 0.875\n",
      "Training iter #23784:   Batch Loss = 0.570807, Accuracy = 0.75\n",
      "Training iter #23792:   Batch Loss = 0.385804, Accuracy = 0.875\n",
      "Training iter #23800:   Batch Loss = 0.011169, Accuracy = 1.0\n",
      "Training iter #23808:   Batch Loss = 0.010260, Accuracy = 1.0\n",
      "Training iter #23816:   Batch Loss = 0.009814, Accuracy = 1.0\n",
      "Training iter #23824:   Batch Loss = 1.049770, Accuracy = 0.625\n",
      "Training iter #23832:   Batch Loss = 0.182541, Accuracy = 1.0\n",
      "Training iter #23840:   Batch Loss = 0.572151, Accuracy = 0.75\n",
      "Training iter #23848:   Batch Loss = 0.672209, Accuracy = 0.625\n",
      "Training iter #23856:   Batch Loss = 0.669480, Accuracy = 0.625\n",
      "Training iter #23864:   Batch Loss = 0.576168, Accuracy = 0.75\n",
      "Training iter #23872:   Batch Loss = 0.393652, Accuracy = 0.875\n",
      "Training iter #23880:   Batch Loss = 0.570945, Accuracy = 0.75\n",
      "Training iter #23888:   Batch Loss = 0.570542, Accuracy = 0.75\n",
      "Training iter #23896:   Batch Loss = 0.062662, Accuracy = 1.0\n",
      "Training iter #23904:   Batch Loss = 1.517573, Accuracy = 0.375\n",
      "Training iter #23912:   Batch Loss = 0.414575, Accuracy = 0.875\n",
      "Training iter #23920:   Batch Loss = 0.385431, Accuracy = 0.875\n",
      "Training iter #23928:   Batch Loss = 0.387703, Accuracy = 0.875\n",
      "Training iter #23936:   Batch Loss = 0.572083, Accuracy = 0.75\n",
      "Training iter #23944:   Batch Loss = 0.011026, Accuracy = 1.0\n",
      "Training iter #23952:   Batch Loss = 0.009807, Accuracy = 1.0\n",
      "Training iter #23960:   Batch Loss = 0.971690, Accuracy = 0.625\n",
      "Training iter #23968:   Batch Loss = 0.400051, Accuracy = 0.875\n",
      "Training iter #23976:   Batch Loss = 0.867868, Accuracy = 0.375\n",
      "Training iter #23984:   Batch Loss = 0.728094, Accuracy = 0.375\n",
      "Training iter #23992:   Batch Loss = 0.638174, Accuracy = 0.75\n",
      "Training iter #24000:   Batch Loss = 0.583459, Accuracy = 0.75\n",
      "Training iter #24008:   Batch Loss = 0.186385, Accuracy = 1.0\n",
      "Training iter #24016:   Batch Loss = 0.397089, Accuracy = 0.875\n",
      "Training iter #24024:   Batch Loss = 0.572581, Accuracy = 0.75\n",
      "Training iter #24032:   Batch Loss = 0.669866, Accuracy = 0.625\n",
      "Training iter #24040:   Batch Loss = 0.571554, Accuracy = 0.75\n",
      "Training iter #24048:   Batch Loss = 0.607253, Accuracy = 0.75\n",
      "Training iter #24056:   Batch Loss = 0.395659, Accuracy = 0.875\n",
      "Training iter #24064:   Batch Loss = 0.010008, Accuracy = 1.0\n",
      "Training iter #24072:   Batch Loss = 0.742453, Accuracy = 0.75\n",
      "Training iter #24080:   Batch Loss = 0.391863, Accuracy = 0.875\n",
      "Training iter #24088:   Batch Loss = 0.682187, Accuracy = 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #24096:   Batch Loss = 0.219215, Accuracy = 1.0\n",
      "Training iter #24104:   Batch Loss = 0.729561, Accuracy = 0.625\n",
      "Training iter #24112:   Batch Loss = 0.406445, Accuracy = 0.875\n",
      "Training iter #24120:   Batch Loss = 0.385168, Accuracy = 0.875\n",
      "Training iter #24128:   Batch Loss = 0.425087, Accuracy = 0.875\n",
      "Training iter #24136:   Batch Loss = 0.387338, Accuracy = 0.875\n",
      "Training iter #24144:   Batch Loss = 0.431417, Accuracy = 0.875\n",
      "Training iter #24152:   Batch Loss = 0.072911, Accuracy = 1.0\n",
      "Training iter #24160:   Batch Loss = 0.010336, Accuracy = 1.0\n",
      "Training iter #24168:   Batch Loss = 1.018757, Accuracy = 0.625\n",
      "Training iter #24176:   Batch Loss = 0.689845, Accuracy = 0.625\n",
      "Training iter #24184:   Batch Loss = 0.711895, Accuracy = 0.5\n",
      "Training iter #24192:   Batch Loss = 0.670401, Accuracy = 0.625\n",
      "Training iter #24200:   Batch Loss = 0.395002, Accuracy = 0.875\n",
      "Training iter #24208:   Batch Loss = 0.426825, Accuracy = 0.875\n",
      "Training iter #24216:   Batch Loss = 0.385377, Accuracy = 0.875\n",
      "Training iter #24224:   Batch Loss = 0.416025, Accuracy = 0.875\n",
      "Training iter #24232:   Batch Loss = 0.726120, Accuracy = 0.625\n",
      "Training iter #24240:   Batch Loss = 0.571036, Accuracy = 0.75\n",
      "Training iter #24248:   Batch Loss = 0.088832, Accuracy = 1.0\n",
      "Training iter #24256:   Batch Loss = 0.386306, Accuracy = 0.875\n",
      "Training iter #24264:   Batch Loss = 0.030955, Accuracy = 1.0\n",
      "Training iter #24272:   Batch Loss = 0.388840, Accuracy = 0.875\n",
      "Training iter #24280:   Batch Loss = 0.385398, Accuracy = 0.875\n",
      "Training iter #24288:   Batch Loss = 0.413515, Accuracy = 0.875\n",
      "Training iter #24296:   Batch Loss = 0.077551, Accuracy = 1.0\n",
      "Training iter #24304:   Batch Loss = 0.387864, Accuracy = 0.875\n",
      "Training iter #24312:   Batch Loss = 0.573716, Accuracy = 0.75\n",
      "Training iter #24320:   Batch Loss = 0.068967, Accuracy = 1.0\n",
      "Training iter #24328:   Batch Loss = 0.010032, Accuracy = 1.0\n",
      "Training iter #24336:   Batch Loss = 0.981816, Accuracy = 0.625\n",
      "Training iter #24344:   Batch Loss = 0.570850, Accuracy = 0.75\n",
      "Training iter #24352:   Batch Loss = 0.719383, Accuracy = 0.5\n",
      "Training iter #24360:   Batch Loss = 0.491998, Accuracy = 0.875\n",
      "Training iter #24368:   Batch Loss = 0.012182, Accuracy = 1.0\n",
      "Training iter #24376:   Batch Loss = 0.385790, Accuracy = 0.875\n",
      "Training iter #24384:   Batch Loss = 0.385640, Accuracy = 0.875\n",
      "Training iter #24392:   Batch Loss = 0.580937, Accuracy = 0.75\n",
      "Training iter #24400:   Batch Loss = 0.570665, Accuracy = 0.75\n",
      "Training iter #24408:   Batch Loss = 0.576803, Accuracy = 0.75\n",
      "Training iter #24416:   Batch Loss = 0.404174, Accuracy = 0.875\n",
      "Training iter #24424:   Batch Loss = 0.709134, Accuracy = 0.625\n",
      "Training iter #24432:   Batch Loss = 0.672303, Accuracy = 0.625\n",
      "Training iter #24440:   Batch Loss = 0.570697, Accuracy = 0.75\n",
      "Training iter #24448:   Batch Loss = 0.447507, Accuracy = 0.875\n",
      "Training iter #24456:   Batch Loss = 0.570607, Accuracy = 0.75\n",
      "Training iter #24464:   Batch Loss = 0.418214, Accuracy = 0.875\n",
      "Training iter #24472:   Batch Loss = 0.077899, Accuracy = 1.0\n",
      "Training iter #24480:   Batch Loss = 0.010006, Accuracy = 1.0\n",
      "Training iter #24488:   Batch Loss = 0.009600, Accuracy = 1.0\n",
      "Training iter #24496:   Batch Loss = 0.989574, Accuracy = 0.625\n",
      "Training iter #24504:   Batch Loss = 0.179445, Accuracy = 1.0\n",
      "Training iter #24512:   Batch Loss = 0.572227, Accuracy = 0.75\n",
      "Training iter #24520:   Batch Loss = 0.671469, Accuracy = 0.625\n",
      "Training iter #24528:   Batch Loss = 0.669412, Accuracy = 0.625\n",
      "Training iter #24536:   Batch Loss = 0.580990, Accuracy = 0.75\n",
      "Training iter #24544:   Batch Loss = 0.392890, Accuracy = 0.875\n",
      "Training iter #24552:   Batch Loss = 0.570609, Accuracy = 0.75\n",
      "Training iter #24560:   Batch Loss = 0.570431, Accuracy = 0.75\n",
      "Training iter #24568:   Batch Loss = 0.053172, Accuracy = 1.0\n",
      "Training iter #24576:   Batch Loss = 1.518991, Accuracy = 0.375\n",
      "Training iter #24584:   Batch Loss = 0.428115, Accuracy = 0.875\n",
      "Training iter #24592:   Batch Loss = 0.385652, Accuracy = 0.875\n",
      "Training iter #24600:   Batch Loss = 0.397569, Accuracy = 0.875\n",
      "Training iter #24608:   Batch Loss = 0.575886, Accuracy = 0.75\n",
      "Training iter #24616:   Batch Loss = 0.183875, Accuracy = 1.0\n",
      "Training iter #24624:   Batch Loss = 0.009810, Accuracy = 1.0\n",
      "Training iter #24632:   Batch Loss = 0.894005, Accuracy = 0.625\n",
      "Training iter #24640:   Batch Loss = 0.403449, Accuracy = 0.875\n",
      "Training iter #24648:   Batch Loss = 0.852061, Accuracy = 0.375\n",
      "Training iter #24656:   Batch Loss = 0.720137, Accuracy = 0.375\n",
      "Training iter #24664:   Batch Loss = 0.640426, Accuracy = 0.75\n",
      "Training iter #24672:   Batch Loss = 0.589502, Accuracy = 0.75\n",
      "Training iter #24680:   Batch Loss = 0.192362, Accuracy = 1.0\n",
      "Training iter #24688:   Batch Loss = 0.399603, Accuracy = 0.875\n",
      "Training iter #24696:   Batch Loss = 0.572804, Accuracy = 0.75\n",
      "Training iter #24704:   Batch Loss = 0.670483, Accuracy = 0.625\n",
      "Training iter #24712:   Batch Loss = 0.574272, Accuracy = 0.75\n",
      "Training iter #24720:   Batch Loss = 0.590275, Accuracy = 0.75\n",
      "Training iter #24728:   Batch Loss = 0.395606, Accuracy = 0.875\n",
      "Training iter #24736:   Batch Loss = 0.009698, Accuracy = 1.0\n",
      "Training iter #24744:   Batch Loss = 0.817339, Accuracy = 0.75\n",
      "Training iter #24752:   Batch Loss = 0.396474, Accuracy = 0.875\n",
      "Training iter #24760:   Batch Loss = 0.679738, Accuracy = 0.625\n",
      "Training iter #24768:   Batch Loss = 0.221662, Accuracy = 1.0\n",
      "Training iter #24776:   Batch Loss = 0.731004, Accuracy = 0.625\n",
      "Training iter #24784:   Batch Loss = 0.408061, Accuracy = 0.875\n",
      "Training iter #24792:   Batch Loss = 0.385857, Accuracy = 0.875\n",
      "Training iter #24800:   Batch Loss = 0.456421, Accuracy = 0.875\n",
      "Training iter #24808:   Batch Loss = 0.385411, Accuracy = 0.875\n",
      "Training iter #24816:   Batch Loss = 0.395678, Accuracy = 0.875\n",
      "Training iter #24824:   Batch Loss = 0.086265, Accuracy = 1.0\n",
      "Training iter #24832:   Batch Loss = 0.010218, Accuracy = 1.0\n",
      "Training iter #24840:   Batch Loss = 0.976764, Accuracy = 0.625\n",
      "Training iter #24848:   Batch Loss = 0.686998, Accuracy = 0.625\n",
      "Training iter #24856:   Batch Loss = 0.710525, Accuracy = 0.5\n",
      "Training iter #24864:   Batch Loss = 0.670354, Accuracy = 0.625\n",
      "Training iter #24872:   Batch Loss = 0.393276, Accuracy = 0.875\n",
      "Training iter #24880:   Batch Loss = 0.427121, Accuracy = 0.875\n",
      "Training iter #24888:   Batch Loss = 0.388163, Accuracy = 0.875\n",
      "Training iter #24896:   Batch Loss = 0.446751, Accuracy = 0.875\n",
      "Training iter #24904:   Batch Loss = 0.671881, Accuracy = 0.625\n",
      "Training iter #24912:   Batch Loss = 0.570715, Accuracy = 0.75\n",
      "Training iter #24920:   Batch Loss = 0.060174, Accuracy = 1.0\n",
      "Training iter #24928:   Batch Loss = 0.409938, Accuracy = 0.875\n",
      "Training iter #24936:   Batch Loss = 0.051977, Accuracy = 1.0\n",
      "Training iter #24944:   Batch Loss = 0.391224, Accuracy = 0.875\n",
      "Training iter #24952:   Batch Loss = 0.394551, Accuracy = 0.875\n",
      "Training iter #24960:   Batch Loss = 0.405055, Accuracy = 0.875\n",
      "Training iter #24968:   Batch Loss = 0.076250, Accuracy = 1.0\n",
      "Training iter #24976:   Batch Loss = 0.390015, Accuracy = 0.875\n",
      "Training iter #24984:   Batch Loss = 0.573819, Accuracy = 0.75\n",
      "Training iter #24992:   Batch Loss = 0.065960, Accuracy = 1.0\n",
      "Training iter #25000:   Batch Loss = 0.009663, Accuracy = 1.0\n",
      "Training iter #25008:   Batch Loss = 0.947001, Accuracy = 0.625\n",
      "Training iter #25016:   Batch Loss = 0.570774, Accuracy = 0.75\n",
      "Training iter #25024:   Batch Loss = 0.719668, Accuracy = 0.5\n",
      "Training iter #25032:   Batch Loss = 0.493456, Accuracy = 0.875\n",
      "Training iter #25040:   Batch Loss = 0.010090, Accuracy = 1.0\n",
      "Training iter #25048:   Batch Loss = 0.388124, Accuracy = 0.875\n",
      "Training iter #25056:   Batch Loss = 0.391532, Accuracy = 0.875\n",
      "Training iter #25064:   Batch Loss = 0.570946, Accuracy = 0.75\n",
      "Training iter #25072:   Batch Loss = 0.570538, Accuracy = 0.75\n",
      "Training iter #25080:   Batch Loss = 0.570373, Accuracy = 0.75\n",
      "Training iter #25088:   Batch Loss = 0.432446, Accuracy = 0.875\n",
      "Training iter #25096:   Batch Loss = 0.669689, Accuracy = 0.625\n",
      "Training iter #25104:   Batch Loss = 0.669389, Accuracy = 0.625\n",
      "Training iter #25112:   Batch Loss = 0.579533, Accuracy = 0.75\n",
      "Training iter #25120:   Batch Loss = 0.394787, Accuracy = 0.875\n",
      "Training iter #25128:   Batch Loss = 0.570671, Accuracy = 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #25136:   Batch Loss = 0.390946, Accuracy = 0.875\n",
      "Training iter #25144:   Batch Loss = 0.010664, Accuracy = 1.0\n",
      "Training iter #25152:   Batch Loss = 0.009988, Accuracy = 1.0\n",
      "Training iter #25160:   Batch Loss = 0.009482, Accuracy = 1.0\n",
      "Training iter #25168:   Batch Loss = 0.958931, Accuracy = 0.625\n",
      "Training iter #25176:   Batch Loss = 0.176713, Accuracy = 1.0\n",
      "Training iter #25184:   Batch Loss = 0.572483, Accuracy = 0.75\n",
      "Training iter #25192:   Batch Loss = 0.672765, Accuracy = 0.625\n",
      "Training iter #25200:   Batch Loss = 0.669098, Accuracy = 0.625\n",
      "Training iter #25208:   Batch Loss = 0.577844, Accuracy = 0.75\n",
      "Training iter #25216:   Batch Loss = 0.393569, Accuracy = 0.875\n",
      "Training iter #25224:   Batch Loss = 0.570566, Accuracy = 0.75\n",
      "Training iter #25232:   Batch Loss = 0.570168, Accuracy = 0.75\n",
      "Training iter #25240:   Batch Loss = 0.060677, Accuracy = 1.0\n",
      "Training iter #25248:   Batch Loss = 1.498563, Accuracy = 0.375\n",
      "Training iter #25256:   Batch Loss = 0.425557, Accuracy = 0.875\n",
      "Training iter #25264:   Batch Loss = 0.442915, Accuracy = 0.875\n",
      "Training iter #25272:   Batch Loss = 0.401317, Accuracy = 0.875\n",
      "Training iter #25280:   Batch Loss = 0.577094, Accuracy = 0.75\n",
      "Training iter #25288:   Batch Loss = 0.143970, Accuracy = 1.0\n",
      "Training iter #25296:   Batch Loss = 0.011353, Accuracy = 1.0\n",
      "Training iter #25304:   Batch Loss = 0.970232, Accuracy = 0.625\n",
      "Training iter #25312:   Batch Loss = 0.396725, Accuracy = 0.875\n",
      "Training iter #25320:   Batch Loss = 0.882908, Accuracy = 0.375\n",
      "Training iter #25328:   Batch Loss = 0.723231, Accuracy = 0.375\n",
      "Training iter #25336:   Batch Loss = 0.635564, Accuracy = 0.75\n",
      "Training iter #25344:   Batch Loss = 0.591585, Accuracy = 0.75\n",
      "Training iter #25352:   Batch Loss = 0.227735, Accuracy = 1.0\n",
      "Training iter #25360:   Batch Loss = 0.410952, Accuracy = 0.875\n",
      "Training iter #25368:   Batch Loss = 0.571977, Accuracy = 0.75\n",
      "Training iter #25376:   Batch Loss = 0.670168, Accuracy = 0.625\n",
      "Training iter #25384:   Batch Loss = 0.571084, Accuracy = 0.75\n",
      "Training iter #25392:   Batch Loss = 0.597224, Accuracy = 0.75\n",
      "Training iter #25400:   Batch Loss = 0.396446, Accuracy = 0.875\n",
      "Training iter #25408:   Batch Loss = 0.010796, Accuracy = 1.0\n",
      "Training iter #25416:   Batch Loss = 0.764666, Accuracy = 0.75\n",
      "Training iter #25424:   Batch Loss = 0.394554, Accuracy = 0.875\n",
      "Training iter #25432:   Batch Loss = 0.685427, Accuracy = 0.625\n",
      "Training iter #25440:   Batch Loss = 0.226565, Accuracy = 1.0\n",
      "Training iter #25448:   Batch Loss = 0.724052, Accuracy = 0.625\n",
      "Training iter #25456:   Batch Loss = 0.407695, Accuracy = 0.875\n",
      "Training iter #25464:   Batch Loss = 0.385453, Accuracy = 0.875\n",
      "Training iter #25472:   Batch Loss = 0.439945, Accuracy = 0.875\n",
      "Training iter #25480:   Batch Loss = 0.386102, Accuracy = 0.875\n",
      "Training iter #25488:   Batch Loss = 0.411749, Accuracy = 0.875\n",
      "Training iter #25496:   Batch Loss = 0.078052, Accuracy = 1.0\n",
      "Training iter #25504:   Batch Loss = 0.010203, Accuracy = 1.0\n",
      "Training iter #25512:   Batch Loss = 1.015026, Accuracy = 0.625\n",
      "Training iter #25520:   Batch Loss = 0.695553, Accuracy = 0.625\n",
      "Training iter #25528:   Batch Loss = 0.715966, Accuracy = 0.5\n",
      "Training iter #25536:   Batch Loss = 0.670307, Accuracy = 0.625\n",
      "Training iter #25544:   Batch Loss = 0.399168, Accuracy = 0.875\n",
      "Training iter #25552:   Batch Loss = 0.420756, Accuracy = 0.875\n",
      "Training iter #25560:   Batch Loss = 0.387020, Accuracy = 0.875\n",
      "Training iter #25568:   Batch Loss = 0.448575, Accuracy = 0.875\n",
      "Training iter #25576:   Batch Loss = 0.673379, Accuracy = 0.625\n",
      "Training iter #25584:   Batch Loss = 0.570961, Accuracy = 0.75\n",
      "Training iter #25592:   Batch Loss = 0.061465, Accuracy = 1.0\n",
      "Training iter #25600:   Batch Loss = 0.407644, Accuracy = 0.875\n",
      "Training iter #25608:   Batch Loss = 0.050313, Accuracy = 1.0\n",
      "Training iter #25616:   Batch Loss = 0.390859, Accuracy = 0.875\n",
      "Training iter #25624:   Batch Loss = 0.389286, Accuracy = 0.875\n",
      "Training iter #25632:   Batch Loss = 0.424081, Accuracy = 0.875\n",
      "Training iter #25640:   Batch Loss = 0.084090, Accuracy = 1.0\n",
      "Training iter #25648:   Batch Loss = 0.387057, Accuracy = 0.875\n",
      "Training iter #25656:   Batch Loss = 0.571158, Accuracy = 0.75\n",
      "Training iter #25664:   Batch Loss = 0.088385, Accuracy = 1.0\n",
      "Training iter #25672:   Batch Loss = 0.009730, Accuracy = 1.0\n",
      "Training iter #25680:   Batch Loss = 0.933418, Accuracy = 0.625\n",
      "Training iter #25688:   Batch Loss = 0.570739, Accuracy = 0.75\n",
      "Training iter #25696:   Batch Loss = 0.720013, Accuracy = 0.5\n",
      "Training iter #25704:   Batch Loss = 0.496740, Accuracy = 0.875\n",
      "Training iter #25712:   Batch Loss = 0.010669, Accuracy = 1.0\n",
      "Training iter #25720:   Batch Loss = 0.389461, Accuracy = 0.875\n",
      "Training iter #25728:   Batch Loss = 0.386848, Accuracy = 0.875\n",
      "Training iter #25736:   Batch Loss = 0.572653, Accuracy = 0.75\n",
      "Training iter #25744:   Batch Loss = 0.570484, Accuracy = 0.75\n",
      "Training iter #25752:   Batch Loss = 0.570450, Accuracy = 0.75\n",
      "Training iter #25760:   Batch Loss = 0.385866, Accuracy = 0.875\n",
      "Training iter #25768:   Batch Loss = 0.670223, Accuracy = 0.625\n",
      "Training iter #25776:   Batch Loss = 0.669321, Accuracy = 0.625\n",
      "Training iter #25784:   Batch Loss = 0.585675, Accuracy = 0.75\n",
      "Training iter #25792:   Batch Loss = 0.398403, Accuracy = 0.875\n",
      "Training iter #25800:   Batch Loss = 0.570533, Accuracy = 0.75\n",
      "Training iter #25808:   Batch Loss = 0.387456, Accuracy = 0.875\n",
      "Training iter #25816:   Batch Loss = 0.010206, Accuracy = 1.0\n",
      "Training iter #25824:   Batch Loss = 0.009696, Accuracy = 1.0\n",
      "Training iter #25832:   Batch Loss = 0.009262, Accuracy = 1.0\n",
      "Training iter #25840:   Batch Loss = 0.987099, Accuracy = 0.625\n",
      "Training iter #25848:   Batch Loss = 0.177313, Accuracy = 1.0\n",
      "Training iter #25856:   Batch Loss = 0.572520, Accuracy = 0.75\n",
      "Training iter #25864:   Batch Loss = 0.671899, Accuracy = 0.625\n",
      "Training iter #25872:   Batch Loss = 0.668860, Accuracy = 0.625\n",
      "Training iter #25880:   Batch Loss = 0.575051, Accuracy = 0.75\n",
      "Training iter #25888:   Batch Loss = 0.391693, Accuracy = 0.875\n",
      "Training iter #25896:   Batch Loss = 0.570277, Accuracy = 0.75\n",
      "Training iter #25904:   Batch Loss = 0.569818, Accuracy = 0.75\n",
      "Training iter #25912:   Batch Loss = 0.058856, Accuracy = 1.0\n",
      "Training iter #25920:   Batch Loss = 1.522412, Accuracy = 0.375\n",
      "Training iter #25928:   Batch Loss = 0.424188, Accuracy = 0.875\n",
      "Training iter #25936:   Batch Loss = 0.384822, Accuracy = 0.875\n",
      "Training iter #25944:   Batch Loss = 0.388113, Accuracy = 0.875\n",
      "Training iter #25952:   Batch Loss = 0.571290, Accuracy = 0.75\n",
      "Training iter #25960:   Batch Loss = 0.196605, Accuracy = 1.0\n",
      "Training iter #25968:   Batch Loss = 0.009050, Accuracy = 1.0\n",
      "Training iter #25976:   Batch Loss = 0.853763, Accuracy = 0.625\n",
      "Training iter #25984:   Batch Loss = 0.405951, Accuracy = 0.875\n",
      "Training iter #25992:   Batch Loss = 0.864768, Accuracy = 0.375\n",
      "Training iter #26000:   Batch Loss = 0.733804, Accuracy = 0.375\n",
      "Training iter #26008:   Batch Loss = 0.631756, Accuracy = 0.75\n",
      "Training iter #26016:   Batch Loss = 0.586060, Accuracy = 0.75\n",
      "Training iter #26024:   Batch Loss = 0.194857, Accuracy = 1.0\n",
      "Training iter #26032:   Batch Loss = 0.399870, Accuracy = 0.875\n",
      "Training iter #26040:   Batch Loss = 0.571689, Accuracy = 0.75\n",
      "Training iter #26048:   Batch Loss = 0.669322, Accuracy = 0.625\n",
      "Training iter #26056:   Batch Loss = 0.571547, Accuracy = 0.75\n",
      "Training iter #26064:   Batch Loss = 0.605805, Accuracy = 0.75\n",
      "Training iter #26072:   Batch Loss = 0.398510, Accuracy = 0.875\n",
      "Training iter #26080:   Batch Loss = 0.009399, Accuracy = 1.0\n",
      "Training iter #26088:   Batch Loss = 0.789765, Accuracy = 0.75\n",
      "Training iter #26096:   Batch Loss = 0.398712, Accuracy = 0.875\n",
      "Training iter #26104:   Batch Loss = 0.678957, Accuracy = 0.625\n",
      "Training iter #26112:   Batch Loss = 0.243451, Accuracy = 1.0\n",
      "Training iter #26120:   Batch Loss = 0.709486, Accuracy = 0.625\n",
      "Training iter #26128:   Batch Loss = 0.411552, Accuracy = 0.875\n",
      "Training iter #26136:   Batch Loss = 0.384685, Accuracy = 0.875\n",
      "Training iter #26144:   Batch Loss = 0.429354, Accuracy = 0.875\n",
      "Training iter #26152:   Batch Loss = 0.385495, Accuracy = 0.875\n",
      "Training iter #26160:   Batch Loss = 0.430154, Accuracy = 0.875\n",
      "Training iter #26168:   Batch Loss = 0.077883, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #26176:   Batch Loss = 0.009599, Accuracy = 1.0\n",
      "Training iter #26184:   Batch Loss = 0.994103, Accuracy = 0.625\n",
      "Training iter #26192:   Batch Loss = 0.689874, Accuracy = 0.625\n",
      "Training iter #26200:   Batch Loss = 0.711269, Accuracy = 0.5\n",
      "Training iter #26208:   Batch Loss = 0.669787, Accuracy = 0.625\n",
      "Training iter #26216:   Batch Loss = 0.392587, Accuracy = 0.875\n",
      "Training iter #26224:   Batch Loss = 0.425745, Accuracy = 0.875\n",
      "Training iter #26232:   Batch Loss = 0.386553, Accuracy = 0.875\n",
      "Training iter #26240:   Batch Loss = 0.457880, Accuracy = 0.875\n",
      "Training iter #26248:   Batch Loss = 0.672227, Accuracy = 0.625\n",
      "Training iter #26256:   Batch Loss = 0.570200, Accuracy = 0.75\n",
      "Training iter #26264:   Batch Loss = 0.058772, Accuracy = 1.0\n",
      "Training iter #26272:   Batch Loss = 0.404272, Accuracy = 0.875\n",
      "Training iter #26280:   Batch Loss = 0.045160, Accuracy = 1.0\n",
      "Training iter #26288:   Batch Loss = 0.391282, Accuracy = 0.875\n",
      "Training iter #26296:   Batch Loss = 0.398099, Accuracy = 0.875\n",
      "Training iter #26304:   Batch Loss = 0.398627, Accuracy = 0.875\n",
      "Training iter #26312:   Batch Loss = 0.069126, Accuracy = 1.0\n",
      "Training iter #26320:   Batch Loss = 0.391532, Accuracy = 0.875\n",
      "Training iter #26328:   Batch Loss = 0.575574, Accuracy = 0.75\n",
      "Training iter #26336:   Batch Loss = 0.052714, Accuracy = 1.0\n",
      "Training iter #26344:   Batch Loss = 0.009140, Accuracy = 1.0\n",
      "Training iter #26352:   Batch Loss = 0.965542, Accuracy = 0.625\n",
      "Training iter #26360:   Batch Loss = 0.570282, Accuracy = 0.75\n",
      "Training iter #26368:   Batch Loss = 0.718701, Accuracy = 0.5\n",
      "Training iter #26376:   Batch Loss = 0.491698, Accuracy = 0.875\n",
      "Training iter #26384:   Batch Loss = 0.009779, Accuracy = 1.0\n",
      "Training iter #26392:   Batch Loss = 0.387346, Accuracy = 0.875\n",
      "Training iter #26400:   Batch Loss = 0.388144, Accuracy = 0.875\n",
      "Training iter #26408:   Batch Loss = 0.571153, Accuracy = 0.75\n",
      "Training iter #26416:   Batch Loss = 0.570020, Accuracy = 0.75\n",
      "Training iter #26424:   Batch Loss = 0.569979, Accuracy = 0.75\n",
      "Training iter #26432:   Batch Loss = 0.396917, Accuracy = 0.875\n",
      "Training iter #26440:   Batch Loss = 0.671133, Accuracy = 0.625\n",
      "Training iter #26448:   Batch Loss = 0.669681, Accuracy = 0.625\n",
      "Training iter #26456:   Batch Loss = 0.618975, Accuracy = 0.75\n",
      "Training iter #26464:   Batch Loss = 0.396306, Accuracy = 0.875\n",
      "Training iter #26472:   Batch Loss = 0.570058, Accuracy = 0.75\n",
      "Training iter #26480:   Batch Loss = 0.385011, Accuracy = 0.875\n",
      "Training iter #26488:   Batch Loss = 0.010102, Accuracy = 1.0\n",
      "Training iter #26496:   Batch Loss = 0.009613, Accuracy = 1.0\n",
      "Training iter #26504:   Batch Loss = 0.009147, Accuracy = 1.0\n",
      "Training iter #26512:   Batch Loss = 0.998178, Accuracy = 0.625\n",
      "Training iter #26520:   Batch Loss = 0.174511, Accuracy = 1.0\n",
      "Training iter #26528:   Batch Loss = 0.572366, Accuracy = 0.75\n",
      "Training iter #26536:   Batch Loss = 0.671794, Accuracy = 0.625\n",
      "Training iter #26544:   Batch Loss = 0.668575, Accuracy = 0.625\n",
      "Training iter #26552:   Batch Loss = 0.574918, Accuracy = 0.75\n",
      "Training iter #26560:   Batch Loss = 0.392872, Accuracy = 0.875\n",
      "Training iter #26568:   Batch Loss = 0.569949, Accuracy = 0.75\n",
      "Training iter #26576:   Batch Loss = 0.569527, Accuracy = 0.75\n",
      "Training iter #26584:   Batch Loss = 0.058493, Accuracy = 1.0\n",
      "Training iter #26592:   Batch Loss = 1.523411, Accuracy = 0.375\n",
      "Training iter #26600:   Batch Loss = 0.422896, Accuracy = 0.875\n",
      "Training iter #26608:   Batch Loss = 0.384820, Accuracy = 0.875\n",
      "Training iter #26616:   Batch Loss = 0.391992, Accuracy = 0.875\n",
      "Training iter #26624:   Batch Loss = 0.572923, Accuracy = 0.75\n",
      "Training iter #26632:   Batch Loss = 0.194316, Accuracy = 1.0\n",
      "Training iter #26640:   Batch Loss = 0.009392, Accuracy = 1.0\n",
      "Training iter #26648:   Batch Loss = 0.892650, Accuracy = 0.625\n",
      "Training iter #26656:   Batch Loss = 0.407827, Accuracy = 0.875\n",
      "Training iter #26664:   Batch Loss = 0.862024, Accuracy = 0.375\n",
      "Training iter #26672:   Batch Loss = 0.732010, Accuracy = 0.375\n",
      "Training iter #26680:   Batch Loss = 0.633835, Accuracy = 0.75\n",
      "Training iter #26688:   Batch Loss = 0.585356, Accuracy = 0.75\n",
      "Training iter #26696:   Batch Loss = 0.192797, Accuracy = 1.0\n",
      "Training iter #26704:   Batch Loss = 0.399321, Accuracy = 0.875\n",
      "Training iter #26712:   Batch Loss = 0.571240, Accuracy = 0.75\n",
      "Training iter #26720:   Batch Loss = 0.669129, Accuracy = 0.625\n",
      "Training iter #26728:   Batch Loss = 0.571061, Accuracy = 0.75\n",
      "Training iter #26736:   Batch Loss = 0.606974, Accuracy = 0.75\n",
      "Training iter #26744:   Batch Loss = 0.398019, Accuracy = 0.875\n",
      "Training iter #26752:   Batch Loss = 0.009293, Accuracy = 1.0\n",
      "Training iter #26760:   Batch Loss = 0.761378, Accuracy = 0.75\n",
      "Training iter #26768:   Batch Loss = 0.395884, Accuracy = 0.875\n",
      "Training iter #26776:   Batch Loss = 0.681317, Accuracy = 0.625\n",
      "Training iter #26784:   Batch Loss = 0.222222, Accuracy = 1.0\n",
      "Training iter #26792:   Batch Loss = 0.727000, Accuracy = 0.625\n",
      "Training iter #26800:   Batch Loss = 0.407974, Accuracy = 0.875\n",
      "Training iter #26808:   Batch Loss = 0.385118, Accuracy = 0.875\n",
      "Training iter #26816:   Batch Loss = 0.454971, Accuracy = 0.875\n",
      "Training iter #26824:   Batch Loss = 0.384773, Accuracy = 0.875\n",
      "Training iter #26832:   Batch Loss = 0.400212, Accuracy = 0.875\n",
      "Training iter #26840:   Batch Loss = 0.084079, Accuracy = 1.0\n",
      "Training iter #26848:   Batch Loss = 0.009347, Accuracy = 1.0\n",
      "Training iter #26856:   Batch Loss = 0.974568, Accuracy = 0.625\n",
      "Training iter #26864:   Batch Loss = 0.690065, Accuracy = 0.625\n",
      "Training iter #26872:   Batch Loss = 0.713226, Accuracy = 0.5\n",
      "Training iter #26880:   Batch Loss = 0.669904, Accuracy = 0.625\n",
      "Training iter #26888:   Batch Loss = 0.390940, Accuracy = 0.875\n",
      "Training iter #26896:   Batch Loss = 0.422289, Accuracy = 0.875\n",
      "Training iter #26904:   Batch Loss = 0.386874, Accuracy = 0.875\n",
      "Training iter #26912:   Batch Loss = 0.441456, Accuracy = 0.875\n",
      "Training iter #26920:   Batch Loss = 0.670180, Accuracy = 0.625\n",
      "Training iter #26928:   Batch Loss = 0.569870, Accuracy = 0.75\n",
      "Training iter #26936:   Batch Loss = 0.053869, Accuracy = 1.0\n",
      "Training iter #26944:   Batch Loss = 0.410842, Accuracy = 0.875\n",
      "Training iter #26952:   Batch Loss = 0.054451, Accuracy = 1.0\n",
      "Training iter #26960:   Batch Loss = 0.388527, Accuracy = 0.875\n",
      "Training iter #26968:   Batch Loss = 0.390767, Accuracy = 0.875\n",
      "Training iter #26976:   Batch Loss = 0.414113, Accuracy = 0.875\n",
      "Training iter #26984:   Batch Loss = 0.082003, Accuracy = 1.0\n",
      "Training iter #26992:   Batch Loss = 0.386607, Accuracy = 0.875\n",
      "Training iter #27000:   Batch Loss = 0.571612, Accuracy = 0.75\n",
      "Training iter #27008:   Batch Loss = 0.074118, Accuracy = 1.0\n",
      "Training iter #27016:   Batch Loss = 0.008918, Accuracy = 1.0\n",
      "Training iter #27024:   Batch Loss = 0.926391, Accuracy = 0.625\n",
      "Training iter #27032:   Batch Loss = 0.569934, Accuracy = 0.75\n",
      "Training iter #27040:   Batch Loss = 0.719372, Accuracy = 0.5\n",
      "Training iter #27048:   Batch Loss = 0.494229, Accuracy = 0.875\n",
      "Training iter #27056:   Batch Loss = 0.009629, Accuracy = 1.0\n",
      "Training iter #27064:   Batch Loss = 0.388216, Accuracy = 0.875\n",
      "Training iter #27072:   Batch Loss = 0.385670, Accuracy = 0.875\n",
      "Training iter #27080:   Batch Loss = 0.574581, Accuracy = 0.75\n",
      "Training iter #27088:   Batch Loss = 0.569792, Accuracy = 0.75\n",
      "Training iter #27096:   Batch Loss = 0.574211, Accuracy = 0.75\n",
      "Training iter #27104:   Batch Loss = 0.399960, Accuracy = 0.875\n",
      "Training iter #27112:   Batch Loss = 0.717851, Accuracy = 0.625\n",
      "Training iter #27120:   Batch Loss = 0.674635, Accuracy = 0.625\n",
      "Training iter #27128:   Batch Loss = 0.570467, Accuracy = 0.75\n",
      "Training iter #27136:   Batch Loss = 0.450880, Accuracy = 0.875\n",
      "Training iter #27144:   Batch Loss = 0.569853, Accuracy = 0.75\n",
      "Training iter #27152:   Batch Loss = 0.410832, Accuracy = 0.875\n",
      "Training iter #27160:   Batch Loss = 0.082528, Accuracy = 1.0\n",
      "Training iter #27168:   Batch Loss = 0.009128, Accuracy = 1.0\n",
      "Training iter #27176:   Batch Loss = 0.008826, Accuracy = 1.0\n",
      "Training iter #27184:   Batch Loss = 0.952285, Accuracy = 0.625\n",
      "Training iter #27192:   Batch Loss = 0.168565, Accuracy = 1.0\n",
      "Training iter #27200:   Batch Loss = 0.572632, Accuracy = 0.75\n",
      "Training iter #27208:   Batch Loss = 0.672156, Accuracy = 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #27216:   Batch Loss = 0.668518, Accuracy = 0.625\n",
      "Training iter #27224:   Batch Loss = 0.577047, Accuracy = 0.75\n",
      "Training iter #27232:   Batch Loss = 0.394078, Accuracy = 0.875\n",
      "Training iter #27240:   Batch Loss = 0.569896, Accuracy = 0.75\n",
      "Training iter #27248:   Batch Loss = 0.569483, Accuracy = 0.75\n",
      "Training iter #27256:   Batch Loss = 0.058852, Accuracy = 1.0\n",
      "Training iter #27264:   Batch Loss = 1.494938, Accuracy = 0.375\n",
      "Training iter #27272:   Batch Loss = 0.428038, Accuracy = 0.875\n",
      "Training iter #27280:   Batch Loss = 0.384552, Accuracy = 0.875\n",
      "Training iter #27288:   Batch Loss = 0.389826, Accuracy = 0.875\n",
      "Training iter #27296:   Batch Loss = 0.575206, Accuracy = 0.75\n",
      "Training iter #27304:   Batch Loss = 0.202579, Accuracy = 1.0\n",
      "Training iter #27312:   Batch Loss = 0.009227, Accuracy = 1.0\n",
      "Training iter #27320:   Batch Loss = 0.889992, Accuracy = 0.625\n",
      "Training iter #27328:   Batch Loss = 0.410802, Accuracy = 0.875\n",
      "Training iter #27336:   Batch Loss = 0.855826, Accuracy = 0.375\n",
      "Training iter #27344:   Batch Loss = 0.733877, Accuracy = 0.375\n",
      "Training iter #27352:   Batch Loss = 0.633050, Accuracy = 0.75\n",
      "Training iter #27360:   Batch Loss = 0.585558, Accuracy = 0.75\n",
      "Training iter #27368:   Batch Loss = 0.194935, Accuracy = 1.0\n",
      "Training iter #27376:   Batch Loss = 0.399787, Accuracy = 0.875\n",
      "Training iter #27384:   Batch Loss = 0.570428, Accuracy = 0.75\n",
      "Training iter #27392:   Batch Loss = 0.668888, Accuracy = 0.625\n",
      "Training iter #27400:   Batch Loss = 0.570904, Accuracy = 0.75\n",
      "Training iter #27408:   Batch Loss = 0.609025, Accuracy = 0.75\n",
      "Training iter #27416:   Batch Loss = 0.399152, Accuracy = 0.875\n",
      "Training iter #27424:   Batch Loss = 0.008889, Accuracy = 1.0\n",
      "Training iter #27432:   Batch Loss = 0.767596, Accuracy = 0.75\n",
      "Training iter #27440:   Batch Loss = 0.396656, Accuracy = 0.875\n",
      "Training iter #27448:   Batch Loss = 0.680120, Accuracy = 0.625\n",
      "Training iter #27456:   Batch Loss = 0.220963, Accuracy = 1.0\n",
      "Training iter #27464:   Batch Loss = 0.726492, Accuracy = 0.625\n",
      "Training iter #27472:   Batch Loss = 0.408415, Accuracy = 0.875\n",
      "Training iter #27480:   Batch Loss = 0.385195, Accuracy = 0.875\n",
      "Training iter #27488:   Batch Loss = 0.442834, Accuracy = 0.875\n",
      "Training iter #27496:   Batch Loss = 0.384748, Accuracy = 0.875\n",
      "Training iter #27504:   Batch Loss = 0.405034, Accuracy = 0.875\n",
      "Training iter #27512:   Batch Loss = 0.081789, Accuracy = 1.0\n",
      "Training iter #27520:   Batch Loss = 0.009403, Accuracy = 1.0\n",
      "Training iter #27528:   Batch Loss = 0.970794, Accuracy = 0.625\n",
      "Training iter #27536:   Batch Loss = 0.690763, Accuracy = 0.625\n",
      "Training iter #27544:   Batch Loss = 0.713959, Accuracy = 0.5\n",
      "Training iter #27552:   Batch Loss = 0.669935, Accuracy = 0.625\n",
      "Training iter #27560:   Batch Loss = 0.391338, Accuracy = 0.875\n",
      "Training iter #27568:   Batch Loss = 0.422294, Accuracy = 0.875\n",
      "Training iter #27576:   Batch Loss = 0.386006, Accuracy = 0.875\n",
      "Training iter #27584:   Batch Loss = 0.424854, Accuracy = 0.875\n",
      "Training iter #27592:   Batch Loss = 0.669119, Accuracy = 0.625\n",
      "Training iter #27600:   Batch Loss = 0.569669, Accuracy = 0.75\n",
      "Training iter #27608:   Batch Loss = 0.062122, Accuracy = 1.0\n",
      "Training iter #27616:   Batch Loss = 0.401167, Accuracy = 0.875\n",
      "Training iter #27624:   Batch Loss = 0.040652, Accuracy = 1.0\n",
      "Training iter #27632:   Batch Loss = 0.389944, Accuracy = 0.875\n",
      "Training iter #27640:   Batch Loss = 0.395272, Accuracy = 0.875\n",
      "Training iter #27648:   Batch Loss = 0.400217, Accuracy = 0.875\n",
      "Training iter #27656:   Batch Loss = 0.072636, Accuracy = 1.0\n",
      "Training iter #27664:   Batch Loss = 0.389547, Accuracy = 0.875\n",
      "Training iter #27672:   Batch Loss = 0.573601, Accuracy = 0.75\n",
      "Training iter #27680:   Batch Loss = 0.061732, Accuracy = 1.0\n",
      "Training iter #27688:   Batch Loss = 0.008919, Accuracy = 1.0\n",
      "Training iter #27696:   Batch Loss = 0.939201, Accuracy = 0.625\n",
      "Training iter #27704:   Batch Loss = 0.569914, Accuracy = 0.75\n",
      "Training iter #27712:   Batch Loss = 0.719824, Accuracy = 0.5\n",
      "Training iter #27720:   Batch Loss = 0.495795, Accuracy = 0.875\n",
      "Training iter #27728:   Batch Loss = 0.009606, Accuracy = 1.0\n",
      "Training iter #27736:   Batch Loss = 0.388708, Accuracy = 0.875\n",
      "Training iter #27744:   Batch Loss = 0.385109, Accuracy = 0.875\n",
      "Training iter #27752:   Batch Loss = 0.576467, Accuracy = 0.75\n",
      "Training iter #27760:   Batch Loss = 0.569794, Accuracy = 0.75\n",
      "Training iter #27768:   Batch Loss = 0.576729, Accuracy = 0.75\n",
      "Training iter #27776:   Batch Loss = 0.402199, Accuracy = 0.875\n",
      "Training iter #27784:   Batch Loss = 0.713936, Accuracy = 0.625\n",
      "Training iter #27792:   Batch Loss = 0.671671, Accuracy = 0.625\n",
      "Training iter #27800:   Batch Loss = 0.569979, Accuracy = 0.75\n",
      "Training iter #27808:   Batch Loss = 0.450139, Accuracy = 0.875\n",
      "Training iter #27816:   Batch Loss = 0.569861, Accuracy = 0.75\n",
      "Training iter #27824:   Batch Loss = 0.413564, Accuracy = 0.875\n",
      "Training iter #27832:   Batch Loss = 0.080124, Accuracy = 1.0\n",
      "Training iter #27840:   Batch Loss = 0.008950, Accuracy = 1.0\n",
      "Training iter #27848:   Batch Loss = 0.008672, Accuracy = 1.0\n",
      "Training iter #27856:   Batch Loss = 0.954928, Accuracy = 0.625\n",
      "Training iter #27864:   Batch Loss = 0.165538, Accuracy = 1.0\n",
      "Training iter #27872:   Batch Loss = 0.573186, Accuracy = 0.75\n",
      "Training iter #27880:   Batch Loss = 0.671401, Accuracy = 0.625\n",
      "Training iter #27888:   Batch Loss = 0.668426, Accuracy = 0.625\n",
      "Training iter #27896:   Batch Loss = 0.575910, Accuracy = 0.75\n",
      "Training iter #27904:   Batch Loss = 0.391618, Accuracy = 0.875\n",
      "Training iter #27912:   Batch Loss = 0.569604, Accuracy = 0.75\n",
      "Training iter #27920:   Batch Loss = 0.569357, Accuracy = 0.75\n",
      "Training iter #27928:   Batch Loss = 0.059294, Accuracy = 1.0\n",
      "Training iter #27936:   Batch Loss = 1.504114, Accuracy = 0.375\n",
      "Training iter #27944:   Batch Loss = 0.426004, Accuracy = 0.875\n",
      "Training iter #27952:   Batch Loss = 0.384462, Accuracy = 0.875\n",
      "Training iter #27960:   Batch Loss = 0.389892, Accuracy = 0.875\n",
      "Training iter #27968:   Batch Loss = 0.574500, Accuracy = 0.75\n",
      "Training iter #27976:   Batch Loss = 0.206108, Accuracy = 1.0\n",
      "Training iter #27984:   Batch Loss = 0.008979, Accuracy = 1.0\n",
      "Training iter #27992:   Batch Loss = 0.929618, Accuracy = 0.625\n",
      "Training iter #28000:   Batch Loss = 0.412767, Accuracy = 0.875\n",
      "Training iter #28008:   Batch Loss = 0.852285, Accuracy = 0.375\n",
      "Training iter #28016:   Batch Loss = 0.728517, Accuracy = 0.375\n",
      "Training iter #28024:   Batch Loss = 0.637254, Accuracy = 0.75\n",
      "Training iter #28032:   Batch Loss = 0.585314, Accuracy = 0.75\n",
      "Training iter #28040:   Batch Loss = 0.192761, Accuracy = 1.0\n",
      "Training iter #28048:   Batch Loss = 0.398358, Accuracy = 0.875\n",
      "Training iter #28056:   Batch Loss = 0.569963, Accuracy = 0.75\n",
      "Training iter #28064:   Batch Loss = 0.668762, Accuracy = 0.625\n",
      "Training iter #28072:   Batch Loss = 0.571039, Accuracy = 0.75\n",
      "Training iter #28080:   Batch Loss = 0.606133, Accuracy = 0.75\n",
      "Training iter #28088:   Batch Loss = 0.399208, Accuracy = 0.875\n",
      "Training iter #28096:   Batch Loss = 0.008858, Accuracy = 1.0\n",
      "Training iter #28104:   Batch Loss = 0.784033, Accuracy = 0.75\n",
      "Training iter #28112:   Batch Loss = 0.398199, Accuracy = 0.875\n",
      "Training iter #28120:   Batch Loss = 0.678546, Accuracy = 0.625\n",
      "Training iter #28128:   Batch Loss = 0.223492, Accuracy = 1.0\n",
      "Training iter #28136:   Batch Loss = 0.725671, Accuracy = 0.625\n",
      "Training iter #28144:   Batch Loss = 0.408187, Accuracy = 0.875\n",
      "Training iter #28152:   Batch Loss = 0.385020, Accuracy = 0.875\n",
      "Training iter #28160:   Batch Loss = 0.449303, Accuracy = 0.875\n",
      "Training iter #28168:   Batch Loss = 0.384711, Accuracy = 0.875\n",
      "Training iter #28176:   Batch Loss = 0.406095, Accuracy = 0.875\n",
      "Training iter #28184:   Batch Loss = 0.083974, Accuracy = 1.0\n",
      "Training iter #28192:   Batch Loss = 0.009099, Accuracy = 1.0\n",
      "Training iter #28200:   Batch Loss = 0.977264, Accuracy = 0.625\n",
      "Training iter #28208:   Batch Loss = 0.691304, Accuracy = 0.625\n",
      "Training iter #28216:   Batch Loss = 0.713733, Accuracy = 0.5\n",
      "Training iter #28224:   Batch Loss = 0.669682, Accuracy = 0.625\n",
      "Training iter #28232:   Batch Loss = 0.391499, Accuracy = 0.875\n",
      "Training iter #28240:   Batch Loss = 0.421068, Accuracy = 0.875\n",
      "Training iter #28248:   Batch Loss = 0.386056, Accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #28256:   Batch Loss = 0.429401, Accuracy = 0.875\n",
      "Training iter #28264:   Batch Loss = 0.669478, Accuracy = 0.625\n",
      "Training iter #28272:   Batch Loss = 0.569690, Accuracy = 0.75\n",
      "Training iter #28280:   Batch Loss = 0.061067, Accuracy = 1.0\n",
      "Training iter #28288:   Batch Loss = 0.397250, Accuracy = 0.875\n",
      "Training iter #28296:   Batch Loss = 0.035331, Accuracy = 1.0\n",
      "Training iter #28304:   Batch Loss = 0.389847, Accuracy = 0.875\n",
      "Training iter #28312:   Batch Loss = 0.398049, Accuracy = 0.875\n",
      "Training iter #28320:   Batch Loss = 0.395863, Accuracy = 0.875\n",
      "Training iter #28328:   Batch Loss = 0.064615, Accuracy = 1.0\n",
      "Training iter #28336:   Batch Loss = 0.393193, Accuracy = 0.875\n",
      "Training iter #28344:   Batch Loss = 0.574759, Accuracy = 0.75\n",
      "Training iter #28352:   Batch Loss = 0.052387, Accuracy = 1.0\n",
      "Training iter #28360:   Batch Loss = 0.008678, Accuracy = 1.0\n",
      "Training iter #28368:   Batch Loss = 0.954777, Accuracy = 0.625\n",
      "Training iter #28376:   Batch Loss = 0.569859, Accuracy = 0.75\n",
      "Training iter #28384:   Batch Loss = 0.718318, Accuracy = 0.5\n",
      "Training iter #28392:   Batch Loss = 0.489851, Accuracy = 0.875\n",
      "Training iter #28400:   Batch Loss = 0.009561, Accuracy = 1.0\n",
      "Training iter #28408:   Batch Loss = 0.387019, Accuracy = 0.875\n",
      "Training iter #28416:   Batch Loss = 0.389555, Accuracy = 0.875\n",
      "Training iter #28424:   Batch Loss = 0.570504, Accuracy = 0.75\n",
      "Training iter #28432:   Batch Loss = 0.569730, Accuracy = 0.75\n",
      "Training iter #28440:   Batch Loss = 0.569438, Accuracy = 0.75\n",
      "Training iter #28448:   Batch Loss = 0.420869, Accuracy = 0.875\n",
      "Training iter #28456:   Batch Loss = 0.668542, Accuracy = 0.625\n",
      "Training iter #28464:   Batch Loss = 0.668249, Accuracy = 0.625\n",
      "Training iter #28472:   Batch Loss = 0.582736, Accuracy = 0.75\n",
      "Training iter #28480:   Batch Loss = 0.397362, Accuracy = 0.875\n",
      "Training iter #28488:   Batch Loss = 0.569481, Accuracy = 0.75\n",
      "Training iter #28496:   Batch Loss = 0.388124, Accuracy = 0.875\n",
      "Training iter #28504:   Batch Loss = 0.009537, Accuracy = 1.0\n",
      "Training iter #28512:   Batch Loss = 0.009075, Accuracy = 1.0\n",
      "Training iter #28520:   Batch Loss = 0.008622, Accuracy = 1.0\n",
      "Training iter #28528:   Batch Loss = 0.945280, Accuracy = 0.625\n",
      "Training iter #28536:   Batch Loss = 0.170770, Accuracy = 1.0\n",
      "Training iter #28544:   Batch Loss = 0.571046, Accuracy = 0.75\n",
      "Training iter #28552:   Batch Loss = 0.671544, Accuracy = 0.625\n",
      "Training iter #28560:   Batch Loss = 0.668082, Accuracy = 0.625\n",
      "Training iter #28568:   Batch Loss = 0.577419, Accuracy = 0.75\n",
      "Training iter #28576:   Batch Loss = 0.394288, Accuracy = 0.875\n",
      "Training iter #28584:   Batch Loss = 0.569245, Accuracy = 0.75\n",
      "Training iter #28592:   Batch Loss = 0.569043, Accuracy = 0.75\n",
      "Training iter #28600:   Batch Loss = 0.053012, Accuracy = 1.0\n",
      "Training iter #28608:   Batch Loss = 1.500145, Accuracy = 0.375\n",
      "Training iter #28616:   Batch Loss = 0.428266, Accuracy = 0.875\n",
      "Training iter #28624:   Batch Loss = 0.384209, Accuracy = 0.875\n",
      "Training iter #28632:   Batch Loss = 0.395133, Accuracy = 0.875\n",
      "Training iter #28640:   Batch Loss = 0.574341, Accuracy = 0.75\n",
      "Training iter #28648:   Batch Loss = 0.184499, Accuracy = 1.0\n",
      "Training iter #28656:   Batch Loss = 0.008375, Accuracy = 1.0\n",
      "Training iter #28664:   Batch Loss = 0.863792, Accuracy = 0.625\n",
      "Training iter #28672:   Batch Loss = 0.401222, Accuracy = 0.875\n",
      "Training iter #28680:   Batch Loss = 0.851121, Accuracy = 0.375\n",
      "Training iter #28688:   Batch Loss = 0.709845, Accuracy = 0.375\n",
      "Training iter #28696:   Batch Loss = 0.682258, Accuracy = 0.75\n",
      "Training iter #28704:   Batch Loss = 0.570641, Accuracy = 0.75\n",
      "Training iter #28712:   Batch Loss = 0.026017, Accuracy = 1.0\n",
      "Training iter #28720:   Batch Loss = 0.419564, Accuracy = 0.875\n",
      "Training iter #28728:   Batch Loss = 0.570357, Accuracy = 0.75\n",
      "Training iter #28736:   Batch Loss = 0.686520, Accuracy = 0.625\n",
      "Training iter #28744:   Batch Loss = 0.606673, Accuracy = 0.75\n",
      "Training iter #28752:   Batch Loss = 0.569689, Accuracy = 0.75\n",
      "Training iter #28760:   Batch Loss = 0.419045, Accuracy = 0.875\n",
      "Training iter #28768:   Batch Loss = 0.094377, Accuracy = 1.0\n",
      "Training iter #28776:   Batch Loss = 0.632395, Accuracy = 0.75\n",
      "Training iter #28784:   Batch Loss = 0.385613, Accuracy = 0.875\n",
      "Training iter #28792:   Batch Loss = 0.681406, Accuracy = 0.625\n",
      "Training iter #28800:   Batch Loss = 0.223534, Accuracy = 1.0\n",
      "Training iter #28808:   Batch Loss = 0.725005, Accuracy = 0.625\n",
      "Training iter #28816:   Batch Loss = 0.407135, Accuracy = 0.875\n",
      "Training iter #28824:   Batch Loss = 0.384381, Accuracy = 0.875\n",
      "Training iter #28832:   Batch Loss = 0.463375, Accuracy = 0.875\n",
      "Training iter #28840:   Batch Loss = 0.384136, Accuracy = 0.875\n",
      "Training iter #28848:   Batch Loss = 0.400637, Accuracy = 0.875\n",
      "Training iter #28856:   Batch Loss = 0.082327, Accuracy = 1.0\n",
      "Training iter #28864:   Batch Loss = 0.009323, Accuracy = 1.0\n",
      "Training iter #28872:   Batch Loss = 0.992632, Accuracy = 0.625\n",
      "Training iter #28880:   Batch Loss = 0.687516, Accuracy = 0.625\n",
      "Training iter #28888:   Batch Loss = 0.714309, Accuracy = 0.5\n",
      "Training iter #28896:   Batch Loss = 0.669588, Accuracy = 0.625\n",
      "Training iter #28904:   Batch Loss = 0.395598, Accuracy = 0.875\n",
      "Training iter #28912:   Batch Loss = 0.424868, Accuracy = 0.875\n",
      "Training iter #28920:   Batch Loss = 0.386669, Accuracy = 0.875\n",
      "Training iter #28928:   Batch Loss = 0.452329, Accuracy = 0.875\n",
      "Training iter #28936:   Batch Loss = 0.670593, Accuracy = 0.625\n",
      "Training iter #28944:   Batch Loss = 0.569320, Accuracy = 0.75\n",
      "Training iter #28952:   Batch Loss = 0.040004, Accuracy = 1.0\n",
      "Training iter #28960:   Batch Loss = 0.417454, Accuracy = 0.875\n",
      "Training iter #28968:   Batch Loss = 0.060359, Accuracy = 1.0\n",
      "Training iter #28976:   Batch Loss = 0.386558, Accuracy = 0.875\n",
      "Training iter #28984:   Batch Loss = 0.390558, Accuracy = 0.875\n",
      "Training iter #28992:   Batch Loss = 0.412492, Accuracy = 0.875\n",
      "Training iter #29000:   Batch Loss = 0.083640, Accuracy = 1.0\n",
      "Training iter #29008:   Batch Loss = 0.386055, Accuracy = 0.875\n",
      "Training iter #29016:   Batch Loss = 0.573185, Accuracy = 0.75\n",
      "Training iter #29024:   Batch Loss = 0.064668, Accuracy = 1.0\n",
      "Training iter #29032:   Batch Loss = 0.008605, Accuracy = 1.0\n",
      "Training iter #29040:   Batch Loss = 0.960200, Accuracy = 0.625\n",
      "Training iter #29048:   Batch Loss = 0.569448, Accuracy = 0.75\n",
      "Training iter #29056:   Batch Loss = 0.722876, Accuracy = 0.5\n",
      "Training iter #29064:   Batch Loss = 0.491505, Accuracy = 0.875\n",
      "Training iter #29072:   Batch Loss = 0.009445, Accuracy = 1.0\n",
      "Training iter #29080:   Batch Loss = 0.386909, Accuracy = 0.875\n",
      "Training iter #29088:   Batch Loss = 0.385148, Accuracy = 0.875\n",
      "Training iter #29096:   Batch Loss = 0.573431, Accuracy = 0.75\n",
      "Training iter #29104:   Batch Loss = 0.569313, Accuracy = 0.75\n",
      "Training iter #29112:   Batch Loss = 0.574536, Accuracy = 0.75\n",
      "Training iter #29120:   Batch Loss = 0.398728, Accuracy = 0.875\n",
      "Training iter #29128:   Batch Loss = 0.710204, Accuracy = 0.625\n",
      "Training iter #29136:   Batch Loss = 0.673378, Accuracy = 0.625\n",
      "Training iter #29144:   Batch Loss = 0.569894, Accuracy = 0.75\n",
      "Training iter #29152:   Batch Loss = 0.456432, Accuracy = 0.875\n",
      "Training iter #29160:   Batch Loss = 0.569235, Accuracy = 0.75\n",
      "Training iter #29168:   Batch Loss = 0.413187, Accuracy = 0.875\n",
      "Training iter #29176:   Batch Loss = 0.082982, Accuracy = 1.0\n",
      "Training iter #29184:   Batch Loss = 0.008603, Accuracy = 1.0\n",
      "Training iter #29192:   Batch Loss = 0.008267, Accuracy = 1.0\n",
      "Training iter #29200:   Batch Loss = 0.980422, Accuracy = 0.625\n",
      "Training iter #29208:   Batch Loss = 0.173113, Accuracy = 1.0\n",
      "Training iter #29216:   Batch Loss = 0.570810, Accuracy = 0.75\n",
      "Training iter #29224:   Batch Loss = 0.671595, Accuracy = 0.625\n",
      "Training iter #29232:   Batch Loss = 0.668071, Accuracy = 0.625\n",
      "Training iter #29240:   Batch Loss = 0.576871, Accuracy = 0.75\n",
      "Training iter #29248:   Batch Loss = 0.393881, Accuracy = 0.875\n",
      "Training iter #29256:   Batch Loss = 0.569240, Accuracy = 0.75\n",
      "Training iter #29264:   Batch Loss = 0.569001, Accuracy = 0.75\n",
      "Training iter #29272:   Batch Loss = 0.055607, Accuracy = 1.0\n",
      "Training iter #29280:   Batch Loss = 1.499319, Accuracy = 0.375\n",
      "Training iter #29288:   Batch Loss = 0.424498, Accuracy = 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #29296:   Batch Loss = 0.384252, Accuracy = 0.875\n",
      "Training iter #29304:   Batch Loss = 0.396567, Accuracy = 0.875\n",
      "Training iter #29312:   Batch Loss = 0.573201, Accuracy = 0.75\n",
      "Training iter #29320:   Batch Loss = 0.175165, Accuracy = 1.0\n",
      "Training iter #29328:   Batch Loss = 0.008388, Accuracy = 1.0\n",
      "Training iter #29336:   Batch Loss = 0.851320, Accuracy = 0.625\n",
      "Training iter #29344:   Batch Loss = 0.398045, Accuracy = 0.875\n",
      "Training iter #29352:   Batch Loss = 0.847946, Accuracy = 0.375\n",
      "Training iter #29360:   Batch Loss = 0.703970, Accuracy = 0.375\n",
      "Training iter #29368:   Batch Loss = 0.677973, Accuracy = 0.75\n",
      "Training iter #29376:   Batch Loss = 0.573252, Accuracy = 0.75\n",
      "Training iter #29384:   Batch Loss = 0.129410, Accuracy = 1.0\n",
      "Training iter #29392:   Batch Loss = 0.384815, Accuracy = 0.875\n",
      "Training iter #29400:   Batch Loss = 0.579921, Accuracy = 0.75\n",
      "Training iter #29408:   Batch Loss = 0.669395, Accuracy = 0.625\n",
      "Training iter #29416:   Batch Loss = 0.572054, Accuracy = 0.75\n",
      "Training iter #29424:   Batch Loss = 0.590385, Accuracy = 0.75\n",
      "Training iter #29432:   Batch Loss = 0.393632, Accuracy = 0.875\n",
      "Training iter #29440:   Batch Loss = 0.008848, Accuracy = 1.0\n",
      "Training iter #29448:   Batch Loss = 0.808561, Accuracy = 0.75\n",
      "Training iter #29456:   Batch Loss = 0.394153, Accuracy = 0.875\n",
      "Training iter #29464:   Batch Loss = 0.681421, Accuracy = 0.625\n",
      "Training iter #29472:   Batch Loss = 0.227040, Accuracy = 1.0\n",
      "Training iter #29480:   Batch Loss = 0.723945, Accuracy = 0.625\n",
      "Training iter #29488:   Batch Loss = 0.406891, Accuracy = 0.875\n",
      "Training iter #29496:   Batch Loss = 0.383794, Accuracy = 0.875\n",
      "Training iter #29504:   Batch Loss = 0.414689, Accuracy = 0.875\n",
      "Training iter #29512:   Batch Loss = 0.387074, Accuracy = 0.875\n",
      "Training iter #29520:   Batch Loss = 0.415479, Accuracy = 0.875\n",
      "Training iter #29528:   Batch Loss = 0.071472, Accuracy = 1.0\n",
      "Training iter #29536:   Batch Loss = 0.009435, Accuracy = 1.0\n",
      "Training iter #29544:   Batch Loss = 1.080961, Accuracy = 0.625\n",
      "Training iter #29552:   Batch Loss = 0.691229, Accuracy = 0.625\n",
      "Training iter #29560:   Batch Loss = 0.716023, Accuracy = 0.5\n",
      "Training iter #29568:   Batch Loss = 0.668938, Accuracy = 0.625\n",
      "Training iter #29576:   Batch Loss = 0.405750, Accuracy = 0.875\n",
      "Training iter #29584:   Batch Loss = 0.425685, Accuracy = 0.875\n",
      "Training iter #29592:   Batch Loss = 0.384026, Accuracy = 0.875\n",
      "Training iter #29600:   Batch Loss = 0.508002, Accuracy = 0.875\n",
      "Training iter #29608:   Batch Loss = 0.676990, Accuracy = 0.625\n",
      "Training iter #29616:   Batch Loss = 0.569752, Accuracy = 0.75\n",
      "Training iter #29624:   Batch Loss = 0.053915, Accuracy = 1.0\n",
      "Training iter #29632:   Batch Loss = 0.401869, Accuracy = 0.875\n",
      "Training iter #29640:   Batch Loss = 0.039971, Accuracy = 1.0\n",
      "Training iter #29648:   Batch Loss = 0.389429, Accuracy = 0.875\n",
      "Training iter #29656:   Batch Loss = 0.394552, Accuracy = 0.875\n",
      "Training iter #29664:   Batch Loss = 0.398838, Accuracy = 0.875\n",
      "Training iter #29672:   Batch Loss = 0.072985, Accuracy = 1.0\n",
      "Training iter #29680:   Batch Loss = 0.389419, Accuracy = 0.875\n",
      "Training iter #29688:   Batch Loss = 0.574268, Accuracy = 0.75\n",
      "Training iter #29696:   Batch Loss = 0.059125, Accuracy = 1.0\n",
      "Training iter #29704:   Batch Loss = 0.008576, Accuracy = 1.0\n",
      "Training iter #29712:   Batch Loss = 0.992389, Accuracy = 0.625\n",
      "Training iter #29720:   Batch Loss = 0.569363, Accuracy = 0.75\n",
      "Training iter #29728:   Batch Loss = 0.719437, Accuracy = 0.5\n",
      "Training iter #29736:   Batch Loss = 0.493378, Accuracy = 0.875\n",
      "Training iter #29744:   Batch Loss = 0.010704, Accuracy = 1.0\n",
      "Training iter #29752:   Batch Loss = 0.385088, Accuracy = 0.875\n",
      "Training iter #29760:   Batch Loss = 0.386686, Accuracy = 0.875\n",
      "Training iter #29768:   Batch Loss = 0.569963, Accuracy = 0.75\n",
      "Training iter #29776:   Batch Loss = 0.569217, Accuracy = 0.75\n",
      "Training iter #29784:   Batch Loss = 0.569194, Accuracy = 0.75\n",
      "Training iter #29792:   Batch Loss = 0.396506, Accuracy = 0.875\n",
      "Training iter #29800:   Batch Loss = 0.670214, Accuracy = 0.625\n",
      "Training iter #29808:   Batch Loss = 0.668961, Accuracy = 0.625\n",
      "Training iter #29816:   Batch Loss = 0.615072, Accuracy = 0.75\n",
      "Training iter #29824:   Batch Loss = 0.395793, Accuracy = 0.875\n",
      "Training iter #29832:   Batch Loss = 0.569264, Accuracy = 0.75\n",
      "Training iter #29840:   Batch Loss = 0.384088, Accuracy = 0.875\n",
      "Training iter #29848:   Batch Loss = 0.009573, Accuracy = 1.0\n",
      "Training iter #29856:   Batch Loss = 0.008825, Accuracy = 1.0\n",
      "Training iter #29864:   Batch Loss = 0.008355, Accuracy = 1.0\n",
      "Training iter #29872:   Batch Loss = 1.043994, Accuracy = 0.625\n",
      "Training iter #29880:   Batch Loss = 0.184300, Accuracy = 1.0\n",
      "Training iter #29888:   Batch Loss = 0.570256, Accuracy = 0.75\n",
      "Training iter #29896:   Batch Loss = 0.671524, Accuracy = 0.625\n",
      "Training iter #29904:   Batch Loss = 0.667754, Accuracy = 0.625\n",
      "Training iter #29912:   Batch Loss = 0.571234, Accuracy = 0.75\n",
      "Training iter #29920:   Batch Loss = 0.387806, Accuracy = 0.875\n",
      "Training iter #29928:   Batch Loss = 0.569145, Accuracy = 0.75\n",
      "Training iter #29936:   Batch Loss = 0.568799, Accuracy = 0.75\n",
      "Training iter #29944:   Batch Loss = 0.062602, Accuracy = 1.0\n",
      "Training iter #29952:   Batch Loss = 1.513018, Accuracy = 0.375\n",
      "Training iter #29960:   Batch Loss = 0.415854, Accuracy = 0.875\n",
      "Training iter #29968:   Batch Loss = 0.384569, Accuracy = 0.875\n",
      "Training iter #29976:   Batch Loss = 0.409697, Accuracy = 0.875\n",
      "Training iter #29984:   Batch Loss = 0.568982, Accuracy = 0.75\n",
      "Training iter #29992:   Batch Loss = 0.074197, Accuracy = 1.0\n",
      "Training iter #30000:   Batch Loss = 0.008417, Accuracy = 1.0\n",
      "Optimization Finished!\n",
      "FINAL RESULT: Batch Loss = 0.5959622263908386, Accuracy = 0.7797619104385376\n"
     ]
    }
   ],
   "source": [
    "# To keep track of training's performance\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "# validation_losses=[]\n",
    "# validation _accuracies=[]\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "train_writer = tf.summary.FileWriter('/home/somsukla/Documents/log_dir/deep8',\n",
    "                                     sess.graph)\n",
    "# train_writer = tf.summary.FileWriter('E:\\\\SMC\\\\log_dir\\\\newdata\\\\tr8',\n",
    "#                                      sess.graph)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "# Perform Training steps with \"batch_size\" amount of example data at each loop\n",
    "step = 1\n",
    "while step * batch_size <= training_iters:\n",
    "    batch_xs =         extract_batch_size(X_train,step, batch_size)\n",
    "    batch_ys = extract_batch_size(Y_train,step, batch_size)\n",
    "    \n",
    "    summary,_ = sess.run(\n",
    "        [merged,optimizer],\n",
    "        feed_dict={x:batch_xs,\n",
    "                   y:batch_ys}\n",
    "    )\n",
    "    \n",
    "#     train_writer.add_summary(summary,i)\n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs, \n",
    "            y: batch_ys\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    train_writer.add_summary(summary,step)\n",
    "    print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "    \n",
    "    \n",
    "#     summary,_ = sess.run(\n",
    "#         [merged,optimizer],\n",
    "#         feed_dict={x:X_test,\n",
    "#                    y:Y_test}\n",
    "#     )\n",
    "#     one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "#         [pred, accuracy, cost],\n",
    "#         feed_dict={\n",
    "#             x: X_,\n",
    "#             y: Y_\n",
    "#         }\n",
    "#     )\n",
    "#     test_losses.append(final_loss)\n",
    "#     test_accuracies.append(accuracy)\n",
    "#     train_writer.add_summary(summary,step)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Evaluate network only at some steps for faster training: \n",
    "#     if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "#         # To not spam console, show training accuracy/loss in this \"if\"\n",
    "#         print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "#               \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "#               \", Accuracy = {}\".format(acc))\n",
    "        \n",
    "#         # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "#         loss, acc = sess.run(\n",
    "#             [cost, accuracy], \n",
    "#             feed_dict={\n",
    "#                 x: X_test,\n",
    "#                 y: Y_test\n",
    "#             }\n",
    "#         )\n",
    "#         test_losses.append(loss)\n",
    "#         test_accuracies.append(acc)\n",
    "#         train_writer.add_summary(summary,step)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    step += 1\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Accuracy for test data\n",
    "summary,_ = sess.run(\n",
    "    [merged,optimizer],\n",
    "    feed_dict={x:X_test,\n",
    "               y:Y_test}\n",
    ")\n",
    "one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "    [pred, accuracy, cost],\n",
    "    feed_dict={\n",
    "        x: X_test,\n",
    "        y: Y_test\n",
    "    }\n",
    ")\n",
    "\n",
    "test_losses.append(final_loss)\n",
    "test_accuracies.append(accuracy)\n",
    "train_writer.add_summary(summary,step)\n",
    "\n",
    "print(\"FINAL RESULT: \" + \\\n",
    "      \"Batch Loss = {}\".format(final_loss) + \\\n",
    "      \", Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 77.97619104385376%\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "\n",
    "predictions = one_hot_predictions.argmax(1)\n",
    "\n",
    "print(\"Testing Accuracy: {}%\".format(100*accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
